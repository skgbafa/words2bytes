{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import enum\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from torchtext.data import Field, BPTTIterator\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "def extract_config(config, *argv):\n",
    "    assert len(argv) > 0, \"No keys to extract\"\n",
    "    config_values = []\n",
    "    for key in argv:\n",
    "        assert key in config, f\"Key '{key}' not in config\"\n",
    "        config_values.append(config[key])\n",
    "    \n",
    "    return tuple(config_values) if len(argv) > 1 else config_values[0]\n",
    "\n",
    "def validate_config(config):\n",
    "    embedding_dimension, n_attention_heads = extract_config(config, \"embedding_dimension\", \"n_attention_heads\")\n",
    "    \n",
    "    # embedding dimension must be divisible by n_attention_heads\n",
    "    assert embedding_dimension %  n_attention_heads == 0, f\"Embedding dimension ({embedding_dimension}) must be divisible by n_attention_heads ({n_attention_heads})\"\n",
    "\n",
    "def emb_to_string(emb, vocab):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants/enums\n",
    "class Dataset(enum.Enum):\n",
    "    PennTreebank = 0,\n",
    "    WikiText2 = 1,\n",
    "    WikiText103 = 2\n",
    "\n",
    "class LanguageTask(enum.Enum):\n",
    "    CausalLanuageModeling = 0,\n",
    "    MaskedLanuageModeling = 1\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BPE = 3\n",
    "    BBPE = 4\n",
    "    BYTE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character tokenizer\n",
    "# ## UTF-8 Encoder\n",
    "# def char_tokenizer(string):\n",
    "#     return [x + 2 for x in str.encode(string)]\n",
    "# def char_decoder(tokens):\n",
    "#     return \"\".join([chr(x - 2) if x > 1 else \"\" for x in tokens])\n",
    "\n",
    "def char_tokenizer(string):\n",
    "    return [x for x in string]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([x for x in tokens])\n",
    "\n",
    "# batch functions\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "# load training data\n",
    "def load_data(config):\n",
    "    segmentation = extract_config(config, \"segmentation\")\n",
    "\n",
    "    if segmentation == Segmentation.Word.name:\n",
    "        return load_data_word(config)\n",
    "    if segmentation == Segmentation.Subword.name:\n",
    "        return load_data_subword(config)\n",
    "    if segmentation == Segmentation.Character.name:\n",
    "        return load_data_character(config)\n",
    "    else:\n",
    "        raise ValueError(f'Segementation {segmentation} not supported.')\n",
    "    \n",
    "# load word based training data\n",
    "def load_data_word(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset) \n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "def load_data_subword(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset) \n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    field_processor = Field(tokenize=tokenizer.encode)\n",
    "\n",
    "\n",
    "    # tokenizer = get_tokenizer('subword')\n",
    "    # field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    print(train_dataset)\n",
    "    # get vocabulary\n",
    "    # field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    # vocab = field_processor.vocab\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "\n",
    "\n",
    "def load_data_character(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset) \n",
    "    # tokenizer = get_tokenizer('basic_english')\n",
    "    tokenizer = char_tokenizer\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    print(train_dataset[0:10])\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightning stuff\n",
    "def load_data_pl(config): \n",
    "    # get dataset\n",
    "    dataset = extract_config(config, \"dataset\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    \n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, field_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateExperiements():\n",
    "    WANDB_ENTITY = \"skgbafa\"\n",
    "    # WANDB_ENTITY = \"openai-scholars\"\n",
    "    WANDB_PROJECT = \"\"\n",
    "\n",
    "    # experiment_datasets = [ Dataset.PennTreebank.name, Dataset.WikiText2.name, Dataset.WikiText103.name ]\n",
    "    experiment_datasets = [ Dataset.WikiText2.name ]\n",
    "    experiment_segmentation = [Segmentation.Word.name, Segmentation.Character.name]\n",
    "    # for each dataset\n",
    "        # \n",
    "    sweep_parameters = {\n",
    "        \"n_attention_heads\": {\n",
    "            \"values\": [2, 3, ]\n",
    "        },\n",
    "        \"n_decoder_layers\": {\n",
    "            \"values\": [2, 4, 6]\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"values\": experiment_datasets\n",
    "        },\n",
    "        \"n_epochs\": {\n",
    "            \"values\": [3]\n",
    "        },\n",
    "        \"segmentation\": {\n",
    "            \"values\": experiment_segmentation\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_config = {\n",
    "        \"name\": \"Experamental Sweeps\",\n",
    "        \"method\": \"grid\",\n",
    "        \"parameters\": sweep_parameters\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY)\n",
    "\n",
    "    return sweep_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate/visualize artifacts\n",
    "def initalize_artifacts(config, train_data_batches, val_data_batches):\n",
    "        n_epochs, max_seq_len = extract_config(config, \"n_epochs\", \"max_seq_len\")\n",
    "        training_cel = torch.ones(n_epochs, math.ceil(len(train_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        validation_cel = torch.ones(n_epochs, math.ceil(len(val_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        artifacts = {\n",
    "            \"training\": {\n",
    "                \"CrossEntropyLoss\": training_cel\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"CrossEntropyLoss\": validation_cel\n",
    "            }\n",
    "        }\n",
    "        return artifacts\n",
    "\n",
    "def update_artifact_loss(artifacts, training_stage, metric, epoch, batch, value):\n",
    "    try:\n",
    "        artifacts[training_stage][metric][epoch - 1][batch] = value\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "        print(\"epoch\", epoch)\n",
    "        print(\"batch\", batch)\n",
    "        print(artifacts)\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "    flat_loss = artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "    count = flat_loss.size(0)\n",
    "    batch_number = np.arange(0, flat_loss.size(0))\n",
    "    plt.plot(batch_number, flat_loss)\n",
    "    plt.legend(\"CrossEntropyLoss\")\n",
    "    None\n",
    "\n",
    "# artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 1, 0.5)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 2, 3)\n",
    "# # artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "# visualize_artifacts(artifacts)\n",
    "# # visualize_artifacts(artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only transformer implementation\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder, LayerNorm\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Decoder Only implmentation without memory for encoder\n",
    "# Adapted from pytorch implmentation @ https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model) # skip\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout) # skip\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(CustomTransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "        #                            key_padding_mask=memory_key_padding_mask)[0]\n",
    "        # tgt = tgt + self.dropout2(tgt2)\n",
    "        # tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "# decoder only implmentation\n",
    "# pytorch implmentation for torch ligthning\n",
    "# class Transformer(pl.LightningModule):\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, ntokens, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", custom_encoder=None, custom_decoder=None):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        # model vars\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # decoder setup \n",
    "        decoder_layer = CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # embedding setup\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.to_embedding = nn.Embedding(ntokens, d_model)\n",
    "\n",
    "        # output setup\n",
    "        self.linear = nn.Linear(d_model, ntokens)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "   \n",
    "        # convert input/targets to embeddings\n",
    "        tgt = self.to_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # add positional encodings\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # pytorch checks\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
    "        if  tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of tgt must be equal to d_model\")\n",
    "        \n",
    "        # decoder pass\n",
    "        output = self.decoder(tgt, memory=None, tgt_mask=tgt_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # return after linear layer\n",
    "        return self.linear(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"words2btyes\")\n",
    "# config = wandb.config\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f40be2c3165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.0\u001b[0m \u001b[0;31m# learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# lr = 5.0 # learning rate\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, config, epoch, artifacts):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != max_seq_len:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        # print(data.dtype)\n",
    "        # output = model(data, targets)\n",
    "        reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "        targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "        output = model(data, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        print(\"output_flat\", output_flat.shape)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "        wandb.log({\n",
    "            # \"elapsed_time\": start_time - time.time(),\n",
    "            \"epoch\": epoch,\n",
    "            \"batch\": batch,\n",
    "            \"batch_loss\": loss.item(),\n",
    "            # \"current_loss\": cur_loss,\n",
    "            \"ppl\": math.exp(loss.item()),\n",
    "            \"learning_rate\": scheduler.get_lr()[0],\n",
    "        })\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        cur_loss = total_loss / log_interval\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data_batches) // max_seq_len, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_source, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(max_seq_len, data_source, i)\n",
    "            \n",
    "            # print(data)\n",
    "            # print(targets)\n",
    "            if data.size(0) != max_seq_len:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            # output = model(data, targets)\n",
    "            reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "            targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "            output = model(data, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            loss = criterion(output_flat, targets)\n",
    "            # update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "            total_loss += len(data) * loss.item()\n",
    "\n",
    "            wandb.log({\n",
    "                # \"elapsed_time\": start_time - time.time(),\n",
    "                # \"epoch\": epoch,\n",
    "                \"batch\": i,\n",
    "                \"batch_loss\": loss.item(),\n",
    "                # \"current_loss\": cur_loss,\n",
    "                \"ppl\": math.exp(loss.item()),\n",
    "            })\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "    run = wandb.init(config=default_config)\n",
    "    config = run.config\n",
    "    print(config)\n",
    "\n",
    "    # setup data\n",
    "    # extract config vars\n",
    "    embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size, learning_rate = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\", \"learning_rate\")\n",
    "\n",
    "\n",
    "    # configure device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    # load training data\n",
    "    train_data, val_data, test_data, vocab = load_data(config)\n",
    "    ntokens = len(vocab.stoi)\n",
    "\n",
    "    # batch data\n",
    "    train_data_batches = batchify(train_data, batch_size, device)\n",
    "    val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "    test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "    # instantiate model\n",
    "    model = DecoderOnlyTransformer(ntokens, d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "    \n",
    "\n",
    "    # hyperparams\n",
    "    # lr = 5.0 # learning rate\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    \n",
    "    # train loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs = 3 # The number of epochs\n",
    "    best_model = None\n",
    "    artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        train(model, config, epoch, artifacts)\n",
    "        val_loss = evaluate(model, val_data_batches, config)\n",
    "        wandb.log({\"val_loss\": val_loss, \"val_ppl\": math.exp(val_loss), \"epoch\": epoch})\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                        val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    visualize_artifacts(artifacts)\n",
    "\n",
    "    # test model\n",
    "    test_loss = evaluate(best_model, test_data_batches, config)\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_ppl\": math.exp(test_loss)})\n",
    "\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "default_config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Character.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:17ypdn64) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 52789<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9199959cbb147e2aa22195d11d8f0b0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210211_224033-17ypdn64/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210211_224033-17ypdn64/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">wobbly-planet-12</strong>: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/17ypdn64\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/17ypdn64</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:17ypdn64). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.18 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">devout-tree-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/openai-scholars/words2bytes\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes</a><br/>\n                Run page: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/10x2lrdn\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/10x2lrdn</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210211_224234-10x2lrdn</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_dimension': 200, 'ff_dimension': 200, 'n_attention_heads': 2, 'n_encoder_layers': 0, 'n_decoder_layers': 2, 'dataset': 'PennTreebank', 'segmentation': 'Character', 'max_seq_len': 35, 'batch_size': 20, 'eval_batch_size': 10, 'dropout': 0.2, 'n_epochs': 3, 'learning_rate': 0.5, 'loss_criterion': 'CrossEntropyLoss'}\n",
      "[Start Load Data]\n",
      "Fetched Data (0.000037s)\n",
      "Split Data (0.263195s)\n",
      "[<torchtext.data.example.Example object at 0x7f9503060070>]\n",
      "Build Vocab (0.571970s)\n",
      "[End Load Data] (52.985639s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 9924]' is invalid for input of size 36400",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6a8b402ac703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(Segmentation.Character.name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# print(Segmentation.Subword.name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# load_data(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-9d2dd4f4efc6>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifacts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_ppl\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-984e9cebb576>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, epoch, artifacts)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# output = model(data, targets_flat, src_mask, src_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 9924]' is invalid for input of size 36400"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-6740b5958db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-07c88706c66d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_data_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_data_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-07c88706c66d>\u001b[0m in \u001b[0;36mload_data_word\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# get dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetched Data ({time.time() - ts:3f}s)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# configure model\n",
    "default_config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# validate \n",
    "config = default_config\n",
    "validate_config(default_config)\n",
    "\n",
    "# extract config vars\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size, learning_rate = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\", \"learning_rate\")\n",
    "\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "# instantiate model\n",
    "model = DecoderOnlyTransformer(ntokens, d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "\n",
    "# # c\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "# set up experiments\n",
    "# sweep_id = generateExperiements()\n",
    "\n",
    "# run experiments\n",
    "# wandb.agent(sweep_id, function=train_and_eval)\n",
    "# model = Transformer(embedding_dimension).to(device)\n",
    "\n",
    "\n",
    "# training w/ lightning\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word\n"
     ]
    }
   ],
   "source": [
    "print(Segmentation.Word.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-fe67be6b2035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n\u001b[1;32m      4\u001b[0m     test_loss, math.exp(test_loss)))\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data_batches, config)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecoderOnlyTransformer(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (to_embedding): Embedding(28783, 200)\n",
       "  (linear): Linear(in_features=200, out_features=28783, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pytorch lightning experimentation\n",
    "# train_dataset, val_dataset, test_dataset, field_processor = load_data_pl(config)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling laws plots\n",
    "    # map config values to scaling laws (model size, compute, dataset size)\n",
    "\n",
    "# scaling laws goals\n",
    "    # predict test loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention in encoder and decoder layers\n",
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create sweep with ID: wwapp5vq\nSweep URL: https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq\n"
     ]
    }
   ],
   "source": [
    "# # wandb sweep\n",
    "# # https://docs.wandb.ai/sweeps/python-api\n",
    "\n",
    "# WANDB_ENTITY = \"skgbafa\"\n",
    "# WANDB_PROJECT = \"\"\n",
    "\n",
    "# sweep_config = {\n",
    "#   \"name\": \"My Sweep\",\n",
    "#   \"method\": \"grid\",\n",
    "#   \"parameters\": {\n",
    "#         \"n_epochs\": {\n",
    "#             \"values\": [1, 2, 3]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # wandb.init(entity=WANDB_ENTITY)\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p7f0vb5l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">laced-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/uncategorized\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq</a><br/>\nRun page: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/p7f0vb5l\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/p7f0vb5l</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210210_203418-p7f0vb5l</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "config: {'n_epochs': 1, 'embedding_dimension': 200, 'ff_dimension': 200, 'n_attention_heads': 2, 'n_encoder_layers': 0, 'n_decoder_layers': 2, 'dataset': 'Dataset.PennTreebank', 'segmentation': 'Segmentation.Word', 'max_seq_len': 35, 'batch_size': 20, 'eval_batch_size': 10, 'dropout': 0.2, 'loss_criterion': 'CrossEntropyLoss'}\n",
      "running 0\n",
      "running 1\n",
      "running 2\n",
      "running 3\n",
      "running 4\n",
      "running 5\n",
      "running 6\n",
      "running 7\n",
      "running 8\n",
      "running 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 60810<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcc8f119c33046339327a796273c174f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203418-p7f0vb5l/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203418-p7f0vb5l/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>1</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>_step</td><td>9</td></tr><tr><td>_runtime</td><td>10</td></tr><tr><td>_timestamp</td><td>1612989268</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_step</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_runtime</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_timestamp</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">laced-sweep-1</strong>: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/p7f0vb5l\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/p7f0vb5l</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lc7mm5w8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">sunny-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/uncategorized\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq</a><br/>\nRun page: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/lc7mm5w8\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/lc7mm5w8</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210210_203432-lc7mm5w8</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "config: {'n_epochs': 2, 'embedding_dimension': 200, 'ff_dimension': 200, 'n_attention_heads': 2, 'n_encoder_layers': 0, 'n_decoder_layers': 2, 'dataset': 'Dataset.PennTreebank', 'segmentation': 'Segmentation.Word', 'max_seq_len': 35, 'batch_size': 20, 'eval_batch_size': 10, 'dropout': 0.2, 'loss_criterion': 'CrossEntropyLoss'}\n",
      "running 0\n",
      "running 1\n",
      "running 2\n",
      "running 3\n",
      "running 4\n",
      "running 5\n",
      "running 6\n",
      "running 7\n",
      "running 8\n",
      "running 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 60861<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45bfabbb424e43698e2aef4f1a8af9f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203432-lc7mm5w8/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203432-lc7mm5w8/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>2</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>_step</td><td>9</td></tr><tr><td>_runtime</td><td>10</td></tr><tr><td>_timestamp</td><td>1612989282</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_step</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_runtime</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_timestamp</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">sunny-sweep-2</strong>: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/lc7mm5w8\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/lc7mm5w8</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: in788h8n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">gentle-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/uncategorized\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/sweeps/wwapp5vq</a><br/>\nRun page: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/in788h8n\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/in788h8n</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210210_203449-in788h8n</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "config: {'n_epochs': 3, 'embedding_dimension': 200, 'ff_dimension': 200, 'n_attention_heads': 2, 'n_encoder_layers': 0, 'n_decoder_layers': 2, 'dataset': 'Dataset.PennTreebank', 'segmentation': 'Segmentation.Word', 'max_seq_len': 35, 'batch_size': 20, 'eval_batch_size': 10, 'dropout': 0.2, 'loss_criterion': 'CrossEntropyLoss'}\n",
      "running 0\n",
      "running 1\n",
      "running 2\n",
      "running 3\n",
      "running 4\n",
      "running 5\n",
      "running 6\n",
      "running 7\n",
      "running 8\n",
      "running 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 60923<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5f883b6b74f4b95b653720f15dca1e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203449-in788h8n/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210210_203449-in788h8n/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>3</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>_step</td><td>9</td></tr><tr><td>_runtime</td><td>10</td></tr><tr><td>_timestamp</td><td>1612989299</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>metric</td><td>â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_step</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>_runtime</td><td>â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆ</td></tr><tr><td>_timestamp</td><td>â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆ</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">gentle-sweep-3</strong>: <a href=\"https://wandb.ai/skgbafa/uncategorized/runs/in788h8n\" target=\"_blank\">https://wandb.ai/skgbafa/uncategorized/runs/in788h8n</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def train():\n",
    "#     run = wandb.init(config=default_config)\n",
    "#     print(\"config:\", dict(run.config))\n",
    "#     for epoch in range(10):\n",
    "#         print(\"running\", epoch)\n",
    "#         wandb.log({\"metric\": run.config.n_epochs, \"epoch\": epoch})\n",
    "#         time.sleep(1)\n",
    "#     return \"test\"\n",
    "\n",
    "# result = wandb.agent(sweep_id, function=train)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}