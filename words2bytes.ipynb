{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import enum\n",
    "import io\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from torchtext.data import Field\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch functions\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data#.to(device)\n",
    "\n",
    "def batchify_pad(data, batch_size, embedding_dimension, device):\n",
    "    ## Method 1: Add padding to embedding and batch (and add mask?)\n",
    "    num_batched_units = batch_size * embedding_dimension\n",
    "\n",
    "    # Append padded data\n",
    "    PAD_VALUE = 0 # identify correct pad value (pad token in vocab?)\n",
    "    padcount = num_batched_units - (data.size(0) % num_batched_units)\n",
    "    pad = torch.ones(padcount, dtype=data.dtype) * PAD_VALUE\n",
    "    data = torch.cat((data, pad))\n",
    "\n",
    "    # Divide the dataset into batch_size parts.\n",
    "    nbatch = data.size(0) // num_batched_units\n",
    "\n",
    "    # Evenly divide the data across the batches.\n",
    "    data = data.view((nbatch, batch_size, embedding_dimension)).contiguous()\n",
    "    data = data.to(torch.long) # convert toz float (why?)\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def batchify_trim(data, batch_size, embedding_dimension, device):\n",
    "    ## Method 2: Trim and batch (no mask?)\n",
    "    num_batched_units = batch_size * embedding_dimension\n",
    "\n",
    "    # Divide the dataset into batch_size parts.\n",
    "    nbatch = data.size(0) // num_batched_units\n",
    "\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * num_batched_units)\n",
    "\n",
    "    # Evenly divide the data across the batches.\n",
    "    data = data.view((nbatch, batch_size, embedding_dimension)).contiguous()\n",
    "    data = data.to(torch.float) # convert to float (why?)\n",
    "    return data.to(device)\n",
    "\n",
    "# default to trim\n",
    "# batchify = batchify_trim\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "# utils \n",
    "def extract_config(config, *argv, ):\n",
    "    assert len(argv) > 0, \"No keys to extract\"\n",
    "    config_values = []\n",
    "    for key in argv:\n",
    "        assert key in config, f\"Key '{key}' not in config\"\n",
    "        config_values.append(config[key])\n",
    "    \n",
    "    return tuple(config_values) if len(argv) > 1 else config_values[0]\n",
    "\n",
    "def validate_config(config):\n",
    "    embedding_dimension, n_attention_heads = extract_config(config, \"embedding_dimension\", \"n_attention_heads\")\n",
    "    \n",
    "    # embedding dimension must be divisible by n_attention_heads\n",
    "    assert embedding_dimension %  n_attention_heads == 0, f\"Embedding dimension ({embedding_dimension}) must be divisible by n_attention_heads ({n_attention_heads})\"\n",
    "\n",
    "def emb_to_string(emb, vocab):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "# TODO: change to use config\n",
    "def load_data():\n",
    "    url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "    test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                        iter(io.open(train_filepath,\n",
    "                                                    encoding=\"utf8\")))) # should build from all text\n",
    "    def data_process(raw_text_iter):\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "    val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "    test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
    "\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "def load_data_pl(config): \n",
    "    # get dataset\n",
    "    dataset = extract_config(config, \"dataset\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    \n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, field_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data, vocab = load_data()\n",
    "# batch_size, embedding_dimension = extract_config(config, \"batch_size\", \"embedding_dimension\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batchify2(data, batch_size, embedding_dimension):\n",
    "#     # Divide the dataset into batch_size parts.\n",
    "#     nbatch = data.size(0) // batch_size\n",
    "#     print(nbatch)\n",
    "#     # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#     data = data.narrow(0, 0, nbatch * batch_size)\n",
    "#     # Evenly divide the data across the batch_size batches.\n",
    "#     data = data.view(batch_size, -1).t().contiguous()\n",
    "#     return data.to(device)\n",
    "# train_data_batched = batchify2(train_data, batch_size, embedding_dimension)\n",
    "# print(train_data_batched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4\n",
    "# embedding_dimension = 5\n",
    "# src = torch.rand((3, batch_size, embedding_dimension))\n",
    "# # src = torch.rand((10, 32, 512))\n",
    "# src.shape\n",
    "# print(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# batch_size = 4\n",
    "# embedding_dimension = 5\n",
    "# batched_units = batch_size* embedding_dimension\n",
    "# count = (random.randint(1,5) * batched_units) + random.randint(1, embedding_dimension)\n",
    "# print(count)\n",
    "# print(count % batched_units)\n",
    "# print(batched_units - (count % batched_units))\n",
    "# test = torch.rand(count)\n",
    "\n",
    "# PAD_VALUE = 0\n",
    "# padcount = batched_units - (count % batched_units)\n",
    "# pad = torch.ones(padcount) * PAD_VALUE\n",
    "\n",
    "# data = torch.cat((test, pad))\n",
    "# s_len = int(len(data) / batched_units)\n",
    "# print(s_len)\n",
    "# data.view((s_len, batch_size, embedding_dimension)).contiguous()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_data_batched = batchify(train_data, batch_size, embedding_dimension)\n",
    "# print(train_data_batched.shape)\n",
    "# print(train_data_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 5])\ntorch.Size([3])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.7819, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(input.shape)\n",
    "print(target.shape)\n",
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder, LayerNorm\n",
    "\n",
    "\n",
    "\n",
    "# pytorch implmentation for torch ligthning\n",
    "# class Transformer(pl.LightningModule):\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, ntokens, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", custom_encoder=None, custom_decoder=None):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            encoder_norm = LayerNorm(d_model)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            decoder_norm = LayerNorm(d_model)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.linear = nn.Linear(d_model, ntokens)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                memory_mask=None, src_key_padding_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
    "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.linear(output)\n",
    "        # return output\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "36718lines [00:01, 25222.63lines/s]\n",
      "torch.Size([2049990])\n",
      "tensor([  10, 3850, 3870,  ..., 2443, 4811,    4])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# constants/enums\n",
    "class Dataset(enum.Enum):\n",
    "    PennTreebank = 0,\n",
    "    WikiText2 = 1,\n",
    "    WikiText103 = 2\n",
    "\n",
    "class LanguageTask(enum.Enum):\n",
    "    CausalLanuageModeling = 0,\n",
    "    MaskedLanuageModeling = 1\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BPE = 3\n",
    "    BBPE = 4\n",
    "    BYTE = 5\n",
    "\n",
    "# configure model\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "# validate \n",
    "validate_config(config)\n",
    "\n",
    "# extract config vars\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\")\n",
    "\n",
    "\n",
    "# configure device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_data)\n",
    "print(device)\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = Transformer(d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout)#.to(device)\n",
    "\n",
    "# model = Transformer(embedding_dimension).to(device)\n",
    "\n",
    "\n",
    "# training w/ lightning\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.shape\n",
    "# print(src.shape)\n",
    "# # print(src[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ntokens 28783\ntorch.Size([102499, 20])\ndata torch.Size([35, 20])\ntorch.Size([700])\nsrc_embedding torch.Size([35, 20, 200])\ntorch.Size([35, 20, 200])\ntorch.Size([35, 20, 200])\ntensor([[[ 1.5619e-01,  6.7937e-02,  1.2150e+00,  ...,  1.2139e+00,\n          -6.7299e-01,  1.3177e+00],\n         [-6.4642e-01,  2.1482e+00,  1.1107e+00,  ...,  1.3948e+00,\n           3.7782e-02, -1.1421e+00],\n         [-3.4951e-01,  1.9895e+00, -6.2264e-01,  ..., -4.8888e-01,\n           1.3969e-01, -1.0725e+00],\n         ...,\n         [ 3.7883e-02,  3.3105e-01,  9.1547e-01,  ...,  6.9415e-01,\n           1.4561e+00, -1.0438e-01],\n         [-1.1647e-01, -7.4390e-01,  7.6275e-01,  ..., -6.0463e-01,\n          -2.4515e-01, -2.0075e-01],\n         [-2.5119e-01, -1.4853e-03,  8.3799e-01,  ...,  1.2962e+00,\n          -2.6893e-01, -1.1167e+00]],\n\n        [[ 8.1311e-02,  1.2306e-03,  1.5324e+00,  ..., -8.3095e-01,\n           1.6084e+00,  3.2075e-01],\n         [ 4.9581e-01, -1.2281e+00,  1.8826e-01,  ..., -1.0401e-01,\n          -7.1399e-01, -1.0622e+00],\n         [-9.7688e-01, -1.1240e-01,  5.1407e-01,  ..., -1.5332e-01,\n          -1.6071e-01,  1.4408e+00],\n         ...,\n         [ 1.6033e-02,  5.2750e-01,  1.6880e+00,  ...,  1.0030e+00,\n           2.2469e-01, -7.3472e-01],\n         [ 1.2809e+00, -6.7292e-01, -1.4762e-01,  ...,  5.7874e-01,\n           1.8342e+00, -1.3216e+00],\n         [-5.4603e-01, -1.1296e+00, -6.7907e-01,  ...,  3.6561e-01,\n           5.8019e-01, -1.5087e+00]],\n\n        [[-5.1674e-01,  4.1369e-01,  7.5828e-01,  ...,  3.0872e-01,\n          -1.5094e-01,  1.7986e+00],\n         [-1.0869e-01,  1.2964e+00,  8.6813e-01,  ..., -1.4980e-01,\n          -2.7232e-01, -1.8628e-01],\n         [ 1.2087e+00, -6.3154e-01,  8.1330e-01,  ..., -4.1808e-01,\n           1.0023e+00, -4.1687e-01],\n         ...,\n         [ 4.4723e-01,  1.3435e+00, -2.3857e-01,  ..., -2.0498e-01,\n           2.2193e-01, -9.6264e-01],\n         [-6.0806e-01,  6.6376e-01,  6.9563e-01,  ...,  3.4557e-01,\n          -1.2209e+00, -1.6207e+00],\n         [-3.3842e-01, -1.6828e-01,  2.2046e-01,  ...,  6.7042e-01,\n          -1.3058e+00, -6.3468e-01]],\n\n        ...,\n\n        [[-1.3609e+00, -7.5734e-01,  6.6323e-01,  ...,  1.0362e+00,\n           2.3890e-01,  7.7984e-01],\n         [ 1.4306e-01,  6.7856e-01,  9.9307e-01,  ..., -1.8723e-01,\n           6.3755e-01, -7.4397e-01],\n         [ 2.4347e-01, -9.3076e-01,  2.3634e+00,  ...,  5.4499e-01,\n          -1.1236e-01, -6.1470e-01],\n         ...,\n         [-1.0293e+00,  2.0527e-01, -8.7539e-01,  ..., -4.1713e-01,\n           1.6233e+00, -3.3021e-02],\n         [-1.1084e+00,  3.9243e-01,  7.8702e-01,  ...,  1.9552e-01,\n           2.3027e+00,  4.2993e-01],\n         [-1.4245e+00, -1.5785e+00,  1.0298e+00,  ..., -1.4011e+00,\n           7.1080e-01, -1.1057e+00]],\n\n        [[-5.3965e-01, -1.4801e+00,  6.5455e-01,  ...,  2.0049e+00,\n           7.5298e-01,  9.4235e-01],\n         [-5.7376e-01, -2.7686e-01, -2.5071e-01,  ..., -8.2784e-01,\n          -1.3999e-01, -1.7502e-01],\n         [-1.0843e-02, -3.5933e-01,  9.1993e-01,  ..., -4.8460e-01,\n          -1.8311e-01, -7.3514e-02],\n         ...,\n         [-1.0817e+00,  1.8824e-01,  7.9012e-01,  ...,  1.1619e+00,\n          -4.7584e-01, -2.5001e-01],\n         [-1.4455e+00,  1.2272e+00,  9.2445e-01,  ...,  2.0784e-01,\n           1.2676e+00, -6.5571e-01],\n         [ 7.3316e-01,  8.7387e-01,  1.4395e+00,  ..., -4.5334e-01,\n           1.2357e-01,  2.5444e-01]],\n\n        [[-1.9671e-01, -9.3117e-01,  1.3256e+00,  ..., -1.8293e-01,\n          -4.0909e-01,  9.2188e-01],\n         [-9.4097e-01,  1.1133e+00, -3.4672e-01,  ...,  2.7154e+00,\n          -7.8457e-02, -1.4011e+00],\n         [-6.5734e-01, -3.2087e-01,  4.9104e-01,  ..., -1.2023e+00,\n          -4.8229e-02,  9.1336e-01],\n         ...,\n         [-4.3711e-01,  7.3508e-02,  3.8104e-01,  ..., -8.3618e-01,\n           2.2915e+00, -5.7421e-01],\n         [-5.7564e-01,  5.5765e-01,  1.0406e+00,  ...,  1.5646e+00,\n          -7.4309e-01,  9.7013e-01],\n         [-1.3949e+00, -3.1739e-01, -1.3687e-01,  ..., -1.6255e-01,\n           1.4057e+00, -1.3231e+00]]], grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 28783]' is invalid for input of size 140000",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1dcbc8c3e4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 28783]' is invalid for input of size 140000"
     ]
    }
   ],
   "source": [
    "# playground\n",
    "ntokens = len(vocab.stoi)\n",
    "print(\"ntokens\", ntokens)\n",
    "max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data#.to(device)\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "\n",
    "    return data, target\n",
    "\n",
    "# bptt = max_seq_len\n",
    "# def get_batch(source, i):\n",
    "#     seq_len = min(bptt, len(source) - 1 - i)\n",
    "#     data = source[i:i+seq_len]\n",
    "#     target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "#     return data, target\n",
    "\n",
    "\n",
    "train_data_batched = batchify(train_data, batch_size, device)\n",
    "print(train_data_batched.shape)\n",
    "\n",
    "# model = Transformer().to(device)\n",
    "model = Transformer(d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "to_embedding = nn.Embedding(ntokens, embedding_dimension)\n",
    "\n",
    "for batch, i in enumerate(range(0, train_data_batched.size(0) - 1, max_seq_len)):\n",
    "        # data, targets = get_batch(train_data_batched, i)\n",
    "        data, targets = get_batch(max_seq_len, train_data_batched, i)\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"data\", data.shape)\n",
    "print(targets.shape)\n",
    "# print(data)\n",
    "src_embedding = to_embedding(data)#.to(device)\n",
    "tgt_embedding = to_embedding(targets.reshape(max_seq_len, targets.size(0)//max_seq_len))#.to(device)\n",
    "print(\"src_embedding\", src_embedding.shape)\n",
    "print(tgt_embedding.shape)\n",
    "# print(tgt_embedding)\n",
    "output = model(src_embedding, tgt_embedding)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "print(output)\n",
    "\n",
    "\n",
    "loss = criterion(output, targets)\n",
    "\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 5.0 # learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    ntokens = len(vocab.stoi)\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len)#.to(device)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != max_seq_len:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0))#.to(device)\n",
    "        print(data.dtype)\n",
    "        output = model(data, targets)\n",
    "        output.view(-1, ntokens)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data_batches) // max_seq_len, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, data_source, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    ntokens = len(vocab.stoi)\n",
    "    \n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(max_seq_len, data_source, i)\n",
    "            print(data)\n",
    "            print(targets)\n",
    "            if data.size(0) != max_seq_len:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = model(data, targets)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, criterion, config)\n",
    "    val_loss = evaluate(model, val_data_batches, config)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_data_batches, config)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate experiment configs\n",
    "n_attention_heads_range = range(2,6)\n",
    "n_layers_range = range(2,6)\n",
    "experiment_datasets = [ Dataset.PennTreebank, Dataset.WikiText2, Dataset.WikiText103 ]\n",
    "max_seq_len_range = range()\n",
    "embedding_dimension\n",
    "# datasets\n",
    "def generateExperiements():\n",
    "    # for each dataset\n",
    "        # \n",
    "    config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 2,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "    }\n",
    "    pass\n",
    "\n",
    "# artifacts?\n",
    "    # generate/visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightnign experimentation\n",
    "train_dataset, val_dataset, test_dataset, field_processor = load_data_pl(config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling laws plots\n",
    "    # map config values to scaling laws (model size, compute, dataset size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention in encoder and decoder layers\n",
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}