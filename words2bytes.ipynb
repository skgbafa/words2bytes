{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import enum\n",
    "import io\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from torchtext.data import Field\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch functions\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]#.reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "# utils \n",
    "def extract_config(config, *argv, ):\n",
    "    assert len(argv) > 0, \"No keys to extract\"\n",
    "    config_values = []\n",
    "    for key in argv:\n",
    "        assert key in config, f\"Key '{key}' not in config\"\n",
    "        config_values.append(config[key])\n",
    "    \n",
    "    return tuple(config_values) if len(argv) > 1 else config_values[0]\n",
    "\n",
    "def emb_to_string(emb, vocab):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "# TODO: change to use config\n",
    "def load_data():\n",
    "    url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "    test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                        iter(io.open(train_filepath,\n",
    "                                                    encoding=\"utf8\")))) # should build from all text\n",
    "    def data_process(raw_text_iter):\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "    val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "    test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
    "\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "def load_data_pl(config): \n",
    "    # get dataset\n",
    "    dataset = extract_config(config, \"dataset\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    \n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, field_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torchtext.datasets.language_modeling.PennTreebank object at 0x7f3836493250>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f382a8e19d0>"
      ]
     },
     "metadata": {},
     "execution_count": 318
    }
   ],
   "source": [
    "# pytorch lightnign experimentation\n",
    "train_dataset, val_dataset, test_dataset, field_processor = load_data_pl(config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder, LayerNorm\n",
    "\n",
    "# pytorch implmentation for torch ligthning\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", custom_encoder=None, custom_decoder=None):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            encoder_norm = LayerNorm(d_model)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            decoder_norm = LayerNorm(d_model)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                memory_mask=None, src_key_padding_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        print(src.size(1))\n",
    "        print(tgt.size(1))\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        print(src.shape)\n",
    "        print(tgt.shape)\n",
    "        # print(src.size(2))\n",
    "        # print(tgt.size(2))\n",
    "        # print(self.d_model)\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
    "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "36718lines [00:01, 25677.89lines/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# constants/enums\n",
    "class Dataset(enum.Enum):\n",
    "    PennTreebank = 0,\n",
    "    WikiText2 = 1,\n",
    "    WikiText103 = 2\n",
    "\n",
    "class LanguageTask(enum.Enum):\n",
    "    CausalLanuageModeling = 0,\n",
    "    MaskedLanuageModeling = 1\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BPE = 3\n",
    "    BBPE = 4\n",
    "    BYTE = 5\n",
    "\n",
    "# configure model\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 2,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data()\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, config[\"batch_size\"], device)\n",
    "val_data_batches = batchify(val_data, config[\"eval_batch_size\"], device)\n",
    "test_data_batches = batchify(test_data, config[\"eval_batch_size\"], device)\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\")\n",
    "\n",
    "model = Transformer(d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "\n",
    "# model = Transformer(embedding_dimension).to(device)\n",
    "\n",
    "\n",
    "# training w/ lightning\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 32, 512])\ntensor([0.2112, 0.0264, 0.9768, 0.4366, 0.3703, 0.5703, 0.7766, 0.5213, 0.9574,\n        0.7204, 0.4666, 0.5307, 0.1132, 0.3950, 0.4074, 0.4248, 0.2554, 0.0607,\n        0.9974, 0.7890, 0.3389, 0.0739, 0.0385, 0.0209, 0.9182, 0.2979, 0.1893,\n        0.3270, 0.7633, 0.3891, 0.0074, 0.3917, 0.8699, 0.9401, 0.7385, 0.0688,\n        0.6380, 0.2119, 0.1770, 0.8406, 0.5349, 0.7507, 0.1627, 0.0511, 0.7743,\n        0.1440, 0.4690, 0.2705, 0.6637, 0.9803, 0.1513, 0.2375, 0.2431, 0.3791,\n        0.9232, 0.4596, 0.7163, 0.0025, 0.1672, 0.7188, 0.6225, 0.5879, 0.8044,\n        0.4368, 0.3745, 0.2037, 0.1162, 0.1558, 0.3410, 0.9815, 0.5976, 0.2386,\n        0.2561, 0.8951, 0.4939, 0.6797, 0.7452, 0.3021, 0.4565, 0.7305, 0.2348,\n        0.1895, 0.0836, 0.5293, 0.3951, 0.1958, 0.1477, 0.1933, 0.4536, 0.5720,\n        0.0592, 0.3740, 0.9713, 0.5331, 0.9979, 0.3066, 0.4719, 0.2913, 0.5764,\n        0.9946, 0.4562, 0.4548, 0.2382, 0.5869, 0.1000, 0.4734, 0.6366, 0.8861,\n        0.5466, 0.6372, 0.8450, 0.5226, 0.3062, 0.1618, 0.2696, 0.6561, 0.5878,\n        0.4593, 0.3548, 0.7837, 0.2241, 0.2861, 0.7701, 0.3048, 0.0262, 0.0329,\n        0.7370, 0.8283, 0.9479, 0.2632, 0.0063, 0.6195, 0.3827, 0.6943, 0.4026,\n        0.5229, 0.3387, 0.9568, 0.1076, 0.4214, 0.1187, 0.2614, 0.8497, 0.5153,\n        0.4862, 0.8550, 0.3560, 0.5815, 0.8024, 0.8111, 0.2521, 0.2969, 0.5530,\n        0.3833, 0.0956, 0.7758, 0.8992, 0.8451, 0.5205, 0.4203, 0.7968, 0.5104,\n        0.0749, 0.5132, 0.8982, 0.0824, 0.1513, 0.1090, 0.4824, 0.1147, 0.1621,\n        0.9603, 0.6331, 0.9207, 0.4828, 0.3002, 0.7432, 0.4390, 0.8648, 0.5569,\n        0.3595, 0.2133, 0.2991, 0.1382, 0.6742, 0.1270, 0.4402, 0.7353, 0.3458,\n        0.6640, 0.7803, 0.2703, 0.8668, 0.6668, 0.7370, 0.4410, 0.2161, 0.1278,\n        0.1042, 0.7365, 0.9455, 0.9301, 0.8584, 0.3170, 0.5262, 0.1310, 0.8363,\n        0.6282, 0.1542, 0.1925, 0.7735, 0.0668, 0.0162, 0.6020, 0.3702, 0.6531,\n        0.2358, 0.7688, 0.0182, 0.9446, 0.9917, 0.1512, 0.6276, 0.0555, 0.3109,\n        0.8030, 0.2586, 0.1334, 0.5900, 0.6863, 0.2702, 0.5710, 0.3728, 0.8733,\n        0.6722, 0.5111, 0.4058, 0.0614, 0.4346, 0.1929, 0.4480, 0.6841, 0.8729,\n        0.9675, 0.0417, 0.1810, 0.7333, 0.9810, 0.0388, 0.4138, 0.7794, 0.1316,\n        0.0626, 0.9623, 0.4600, 0.2631, 0.6777, 0.3507, 0.6198, 0.8396, 0.8091,\n        0.6514, 0.1540, 0.3618, 0.2896, 0.6184, 0.3197, 0.2713, 0.1954, 0.0087,\n        0.8856, 0.9358, 0.4303, 0.5658, 0.7969, 0.0698, 0.9349, 0.0621, 0.4767,\n        0.4123, 0.3292, 0.6165, 0.8746, 0.6477, 0.7251, 0.5076, 0.8360, 0.2532,\n        0.7430, 0.6551, 0.1275, 0.8068, 0.5805, 0.9593, 0.9074, 0.4099, 0.0023,\n        0.0794, 0.1414, 0.0402, 0.4902, 0.8918, 0.2623, 0.0490, 0.0412, 0.3444,\n        0.4729, 0.9787, 0.4303, 0.4296, 0.4021, 0.4649, 0.1510, 0.2766, 0.4281,\n        0.3695, 0.3242, 0.3195, 0.3694, 0.7848, 0.8590, 0.6568, 0.7502, 0.8893,\n        0.7149, 0.9457, 0.0529, 0.3218, 0.6213, 0.9620, 0.4809, 0.0366, 0.9818,\n        0.0284, 0.6515, 0.6503, 0.6856, 0.8124, 0.9397, 0.0853, 0.3220, 0.3570,\n        0.0358, 0.3983, 0.9609, 0.9354, 0.9244, 0.4144, 0.7075, 0.6957, 0.5435,\n        0.4223, 0.6191, 0.1704, 0.7347, 0.9260, 0.8546, 0.1379, 0.4344, 0.4289,\n        0.1051, 0.8731, 0.1144, 0.2203, 0.6595, 0.5067, 0.5784, 0.8293, 0.4631,\n        0.8618, 0.3886, 0.6680, 0.6398, 0.5015, 0.9982, 0.4982, 0.9847, 0.3194,\n        0.3893, 0.5817, 0.6369, 0.8019, 0.2438, 0.9022, 0.0455, 0.3947, 0.2560,\n        0.5693, 0.5047, 0.6264, 0.5795, 0.6259, 0.1580, 0.8725, 0.3466, 0.0849,\n        0.2216, 0.2203, 0.1618, 0.8450, 0.0718, 0.7409, 0.2923, 0.6804, 0.7334,\n        0.0285, 0.6559, 0.8454, 0.6726, 0.5605, 0.9496, 0.4355, 0.5996, 0.0110,\n        0.3585, 0.7339, 0.7103, 0.0613, 0.3608, 0.5156, 0.4091, 0.9039, 0.6481,\n        0.5188, 0.7651, 0.1694, 0.0600, 0.2615, 0.2039, 0.7742, 0.6064, 0.0992,\n        0.5410, 0.7153, 0.5332, 0.9612, 0.9828, 0.3092, 0.5372, 0.2753, 0.3541,\n        0.5198, 0.7461, 0.0276, 0.6164, 0.1681, 0.1399, 0.5273, 0.9462, 0.5048,\n        0.3096, 0.4130, 0.1333, 0.7880, 0.4410, 0.8419, 0.8817, 0.2778, 0.0796,\n        0.7432, 0.5386, 0.0656, 0.5977, 0.1260, 0.4065, 0.3887, 0.8022, 0.2003,\n        0.1712, 0.9555, 0.7804, 0.2156, 0.3457, 0.6289, 0.5954, 0.5033, 0.8762,\n        0.0937, 0.5420, 0.4144, 0.5250, 0.7421, 0.4048, 0.6924, 0.7663, 0.9707,\n        0.3346, 0.8403, 0.7310, 0.9673, 0.8776, 0.4344, 0.7579, 0.9693, 0.4695,\n        0.0735, 0.7520, 0.4009, 0.8409, 0.5723, 0.0496, 0.7229, 0.9442, 0.1004,\n        0.0691, 0.0402, 0.1223, 0.1309, 0.5395, 0.7552, 0.2122, 0.2162])\n"
     ]
    }
   ],
   "source": [
    "src.shape\n",
    "print(src.shape)\n",
    "print(src[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([35, 20])\n35\n20\n20\n20\ntorch.Size([35, 20])\ntorch.Size([35, 20])\n200\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-282-34619e712aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# print(targets.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print(src_mask.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-119ff45cc447>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# print(tgt.size(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# playground\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target\n",
    "\n",
    "# model = Transformer().to(device)\n",
    "model = Transformer(d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "\n",
    "for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        break\n",
    "\n",
    "\n",
    "# print(train_data_batches.shape)\n",
    "print(data.shape)\n",
    "print(data.size(0))\n",
    "print(data.size(1))\n",
    "# print(targets.shape)\n",
    "# print(src_mask.shape)\n",
    "output = model(data, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 5.0 # learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len)#.to(device)\n",
    "    print(src_mask)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != max_seq_len:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0))#.to(device)\n",
    "        output = model(data, targets)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data_batches) // max_seq_len, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, data_source, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(max_seq_len, data_source, i)\n",
    "            print(data)\n",
    "            print(targets)\n",
    "            if data.size(0) != max_seq_len:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = model(data, targets)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n        [0., 0., -inf,  ..., -inf, -inf, -inf],\n        [0., 0., 0.,  ..., -inf, -inf, -inf],\n        ...,\n        [0., 0., 0.,  ..., 0., -inf, -inf],\n        [0., 0., 0.,  ..., 0., 0., -inf],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n20\n20\ntorch.Size([35, 20])\ntorch.Size([35, 20])\n200\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-d288aed99aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-d56b92a969a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, config)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-119ff45cc447>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# print(tgt.size(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, criterion, config)\n",
    "    val_loss = evaluate(model, val_data_batches, config)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_data_batches, config)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate experiment configs\n",
    "n_attention_heads_range = range(2,6)\n",
    "n_layers_range = range(2,6)\n",
    "experiment_datasets = [ Dataset.PennTreebank, Dataset.WikiText2, Dataset.WikiText103 ]\n",
    "max_seq_len_range = range()\n",
    "embedding_dimension\n",
    "# datasets\n",
    "def generateExperiements():\n",
    "    # for each dataset\n",
    "        # \n",
    "    config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 2,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "    }\n",
    "    pass\n",
    "\n",
    "# artifacts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling laws plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention in encoder and decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}