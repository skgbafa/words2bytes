{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from constants import *\n",
    "from utils import *\n",
    "\n",
    "from data import load_data\n",
    "from transformer import DecoderOnlyTransformer\n",
    "\n",
    "\n",
    "# default config\n",
    "default_config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"vocab_size\": 40000, # subword/bbpe only\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"gamma\": 0.95,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# benchmarked against https://arxiv.org/pdf/1904.09408v2.pdf\n",
    "# bert_lm_12_768_12_300_1150_wikitext2\n",
    "benchmark_config_1 = {\n",
    "    \"embedding_dimension\": 768,  # units\n",
    "    \"ff_dimension\": 3072,  # hidden_size\n",
    "    \"n_attention_heads\": 12,  # num_heads\n",
    "    \"n_encoder_layers\": 0,  # num_layers\n",
    "    \"n_decoder_layers\": 12,  # num_layers\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Subword.name,\n",
    "    \"vocab_size\": 40000,\n",
    "    \"max_seq_len\": 64,  # max_length\n",
    "    \"dropout\": 0.1,  # dropout\n",
    "    \"batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.00003125,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# experiment generation\n",
    "def generateExperiements():\n",
    "    # WANDB_ENTITY = \"skgbafa\"\n",
    "    WANDB_ENTITY = \"openai-scholars\"\n",
    "\n",
    "    experiment_datasets = [ Dataset.PennTreebank.name,Dataset.WikiText2.name, Dataset.WikiText103.name ]\n",
    "    # experiment_datasets = [ Dataset.WikiText2.name ]\n",
    "    experiment_segmentation = [ Segmentation.Word.name, Segmentation.Subword.name ]\n",
    "\n",
    "    sweep_parameters = {\n",
    "        \"dataset\": {\n",
    "            \"values\": experiment_datasets\n",
    "        },\n",
    "        \"n_epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"segmentation\": {\n",
    "            \"values\": experiment_segmentation\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_config = {\n",
    "        \"name\": \"Benchmark Sweeps\",\n",
    "        \"method\": \"grid\",\n",
    "        \"parameters\": sweep_parameters\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY)\n",
    "\n",
    "    return sweep_id\n",
    "\n",
    "# sweep function\n",
    "def train_and_eval(config=benchmark_config_1, entity=WANDB_ENTITY, num_gpus=4):\n",
    "    run = wandb.init(config=config, entity=entity)\n",
    "    config = run.config\n",
    "    n_epochs = extract_config(config, \"n_epochs\")\n",
    "\n",
    "    # load data\n",
    "    train_loader, val_loader, test_loader, vocab, tokenizer = load_data(config)\n",
    "    ntokens = len(vocab)\n",
    "    \n",
    "    # logger\n",
    "    wandb_logger = pl.loggers.WandbLogger(config=config, entity=entity)\n",
    "\n",
    "    # checkpoints\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(period=1)\n",
    "\n",
    "    # run model\n",
    "    trainer = pl.Trainer(gpus=num_gpus, accelerator=\"dp\",\n",
    "                         max_epochs=n_epochs, logger=wandb_logger,\n",
    "                         checkpoint_callback=checkpoint_callback)\n",
    "    model = DecoderOnlyTransformer(config, ntokens, trainer)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:2c91z05w) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 60362<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ac5724ada8a4bc48207022d2bd87823"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/data/github/words2bytes/wandb/run-20210322_221000-2c91z05w/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/data/github/words2bytes/wandb/run-20210322_221000-2c91z05w/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">gallant-bush-163</strong>: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/2c91z05w\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/2c91z05w</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:2c91z05w). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.23 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">summer-spaceship-164</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/openai-scholars/words2bytes\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes</a><br/>\n                Run page: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/183qqdq1\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/183qqdq1</a><br/>\n                Run data is saved locally in <code>/data/github/words2bytes/wandb/run-20210322_221138-183qqdq1</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000015s)\n",
      "Tokenized and Split Data (0.019804s)\n",
      "[End Load Data] (4.235560s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'every_n_val_epochs'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e06a98e4c5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ded44ddcb838>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(config, entity, num_gpus)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevery_n_val_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'every_n_val_epochs'"
     ]
    }
   ],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}