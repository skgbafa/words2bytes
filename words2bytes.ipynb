{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import enum\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from torchtext.data import Field, BPTTIterator\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "def extract_config(config, *argv):\n",
    "    assert len(argv) > 0, \"No keys to extract\"\n",
    "    config_values = []\n",
    "    for key in argv:\n",
    "        assert key in config, f\"Key '{key}' not in config\"\n",
    "        config_values.append(config[key])\n",
    "    \n",
    "    return tuple(config_values) if len(argv) > 1 else config_values[0]\n",
    "\n",
    "def validate_config(config):\n",
    "    embedding_dimension, n_attention_heads = extract_config(config, \"embedding_dimension\", \"n_attention_heads\")\n",
    "    \n",
    "    # embedding dimension must be divisible by n_attention_heads\n",
    "    assert embedding_dimension %  n_attention_heads == 0, f\"Embedding dimension ({embedding_dimension}) must be divisible by n_attention_heads ({n_attention_heads})\"\n",
    "\n",
    "def emb_to_string(emb, vocab):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch functions\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "# load training data\n",
    "def load_data(config):\n",
    "    return load_data_word(config)\n",
    "    \n",
    "# load word based training data\n",
    "def load_data_word(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "# def load_data_subword(config):\n",
    "#     # load word based training data\n",
    "#     print(\"[Start Load Data]\")\n",
    "#     ts = time.time()\n",
    "\n",
    "#     # get dataset\n",
    "#     dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "#     dataset = getattr(datasets, dataset.name) \n",
    "\n",
    "#     tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#     field_processor = Field(tokenize=tokenizer)\n",
    "#     print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "#     # split dataset\n",
    "#     train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "#     print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "#     # get vocabulary\n",
    "#     field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "#     vocab = field_processor.vocab\n",
    "#     print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "#     def data_process(tt_dataset_split):\n",
    "#         raw_text_iter = tt_dataset_split[0].text\n",
    "#         data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "#                             dtype=torch.long) for item in raw_text_iter]\n",
    "#         return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "#     train_data = data_process(train_dataset)\n",
    "#     val_data = data_process(val_dataset)\n",
    "#     test_data = data_process(test_dataset)\n",
    "\n",
    "#     print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "#     return train_data, val_data, test_data, vocab\n",
    "\n",
    "# character tokenizer\n",
    "def char_tokenizer(string):\n",
    "    return [x + 2 for x in str.encode(string)]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([chr(x - 2) if x > 1 else \"\" for x in tokens])\n",
    "\n",
    "def load_data_character(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    # tokenizer = get_tokenizer('basic_english')\n",
    "    tokenizer = get_tokenizer('subword')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ff0f43187f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is a test string to test the tokenization process.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subword'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# train_data, val_data, test_data, vocab = load_data(config)\n",
    "\n",
    "test_string = \"This is a test string to test the tokenization process.\"\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "tokenizer = get_tokenizer('subword')\n",
    "tokenizer(test_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a7078ea6186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"n_encoder_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"n_decoder_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPennTreebank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BYTE = 3\n",
    "    BBPE = 4\n",
    "\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"segmentation\": Segmentation.Word,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# character tokenizer\n",
    "def char_tokenizer(string):\n",
    "    return [x + 2 for x in str.encode(string)]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([chr(x - 2) if x > 1 else \"\" for x in tokens])\n",
    "\n",
    "# switch \n",
    "def get_tokenizer_config(segmentation): \n",
    "    switcher = { \n",
    "        Segmentation.Word: get_tokenizer('basic_english'),\n",
    "        # Segmentation.Subword: XLNetTokenizer.from_pretrained('xlnet-base-cased'), \n",
    "        Segmentation.Subword: BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "        Segmentation.Character: char_tokenizer, \n",
    "        Segmentation.BYTE: \"one\", \n",
    "        Segmentation.BBPE: \"one\", \n",
    "    } \n",
    "    # default to word\n",
    "    return switcher.get(segmentation, get_tokenizer('basic_english'))\n",
    "\n",
    "# train_data, val_data, test_data, vocab = load_data(config)\n",
    "\n",
    "test_string = \"This is a preconfigured test string to test the tokenization process. Segmentation is like fragmentation.\"\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "# tokenizer = get_tokenizer('subword')\n",
    "tokenizer = get_tokenizer_config(segmentation) \n",
    "print(tokenizer)\n",
    "tokenizer(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6b94da180b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlnet-large-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlnet-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, my dog is cute\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "import torch\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 86, 106, 107, 117,  34, 107, 117,  34,  99,  34, 114, 116, 103, 101,\n         113, 112, 104, 107, 105, 119, 116, 103, 102,  34, 118, 103, 117, 118,\n          34, 117, 118, 116, 107, 112, 105,  34, 118, 113,  34, 118, 103, 117,\n         118,  34, 118, 106, 103,  34, 118, 113, 109, 103, 112, 107, 124,  99,\n         118, 107, 113, 112,  34, 114, 116, 113, 101, 103, 117, 117,  48,  34,\n          85, 103, 105, 111, 103, 112, 118,  99, 118, 107, 113, 112,  34, 107,\n         117,  34, 110, 107, 109, 103,  34, 104, 116,  99, 105, 111, 103, 112,\n         118,  99, 118, 107, 113, 112,  48]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def encode(list_of_strings, pad_token_id=0):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Decoding\n",
    "def decode(outputs_ids):\n",
    "    decoded_outputs = []\n",
    "    for output_ids in outputs_ids.tolist():\n",
    "        # transform id back to char IDs < 2 are simply transformed to \"\"\n",
    "        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n",
    "    return decoded_outputs\n",
    "\n",
    "\n",
    "input_ids, attention_masks = encode([test_string])\n",
    "print(input_ids, attention_masks)\n",
    "# decode(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[99, 100, 101, 102, 67, 68, 69, 70]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'abcdABCD'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "\n",
    "tokens = [x + 2 for x in str.encode(\"abcdABCD\")]\n",
    "print(tokens)\n",
    "char_decoder(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0055354691c34c4b99dd9b8d79c86d88"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87df526897634511bca749cfc1478254"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightning stuff\n",
    "def load_data_pl(config): \n",
    "    # get dataset\n",
    "    dataset = extract_config(config, \"dataset\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    \n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, field_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate/visualize artifacts\n",
    "def initalize_artifacts(config, train_data_batches, val_data_batches):\n",
    "        n_epochs, max_seq_len = extract_config(config, \"n_epochs\", \"max_seq_len\")\n",
    "        training_cel = torch.ones(n_epochs, math.ceil(len(train_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        validation_cel = torch.ones(n_epochs, math.ceil(len(val_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        artifacts = {\n",
    "            \"training\": {\n",
    "                \"CrossEntropyLoss\": training_cel\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"CrossEntropyLoss\": validation_cel\n",
    "            }\n",
    "        }\n",
    "        return artifacts\n",
    "\n",
    "def update_artifact_loss(artifacts, training_stage, metric, epoch, batch, value):\n",
    "    try:\n",
    "        artifacts[training_stage][metric][epoch - 1][batch] = value\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "        print(\"epoch\", epoch)\n",
    "        print(\"batch\", batch)\n",
    "        print(artifacts)\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "    flat_loss = artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "    count = flat_loss.size(0)\n",
    "    batch_number = np.arange(0, flat_loss.size(0))\n",
    "    plt.plot(batch_number, flat_loss)\n",
    "    plt.legend(\"CrossEntropyLoss\")\n",
    "    None\n",
    "\n",
    "# artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 1, 0.5)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 2, 3)\n",
    "# # artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "# visualize_artifacts(artifacts)\n",
    "# # visualize_artifacts(artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only transformer implementation\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder, LayerNorm\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Decoder Only implmentation without memory for encoder\n",
    "# Adapted from pytorch implmentation @ https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model) # skip\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout) # skip\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(CustomTransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "        #                            key_padding_mask=memory_key_padding_mask)[0]\n",
    "        # tgt = tgt + self.dropout2(tgt2)\n",
    "        # tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "# decoder only implmentation\n",
    "# pytorch implmentation for torch ligthning\n",
    "# class Transformer(pl.LightningModule):\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, ntokens, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", custom_encoder=None, custom_decoder=None):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        # model vars\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # decoder setup \n",
    "        decoder_layer = CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # embedding setup\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.to_embedding = nn.Embedding(ntokens, d_model)\n",
    "\n",
    "        # output setup\n",
    "        self.linear = nn.Linear(d_model, ntokens)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "   \n",
    "        # convert input/targets to embeddings\n",
    "        tgt = self.to_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # add positional encodings\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # pytorch checks\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
    "        if  tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of tgt must be equal to d_model\")\n",
    "        \n",
    "        # decoder pass\n",
    "        output = self.decoder(tgt, memory=None, tgt_mask=tgt_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # return after linear layer\n",
    "        return self.linear(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\nFetched Data (0.000025s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "string index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-27740f0a4e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d21e30db0b0f>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Split Data ({time.time() - ts:3f}s)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/datasets/language_modeling.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, text_field, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ptb.test.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m         return super(PennTreebank, cls).splits(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             text_field=text_field, **kwargs)\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/datasets/language_modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, text_field, newline_eos, encoding, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnewline_eos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'<eos>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/revtok/tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(s, decap, split_punctuation)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtoks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mHALF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecapitalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/revtok/tokenizer.py\u001b[0m in \u001b[0;36mdecapitalize\u001b[0;34m(tok)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHALF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mHALF\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# constants/enums\n",
    "class Dataset(enum.Enum):\n",
    "    PennTreebank = 0,\n",
    "    WikiText2 = 1,\n",
    "    WikiText103 = 2\n",
    "\n",
    "class LanguageTask(enum.Enum):\n",
    "    CausalLanuageModeling = 0,\n",
    "    MaskedLanuageModeling = 1\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BPE = 3\n",
    "    BBPE = 4\n",
    "    BYTE = 5\n",
    "\n",
    "# configure model\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"segmentation\": Segmentation.Word,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "# validate \n",
    "validate_config(config)\n",
    "\n",
    "# extract config vars\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\")\n",
    "\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = DecoderOnlyTransformer(ntokens, d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "# model = Transformer(embedding_dimension).to(device)\n",
    "\n",
    "\n",
    "# training w/ lightning\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:27y73vde) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48049<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ebf6af2b10b42559b90fdd078480f5d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210128_233803-27y73vde/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210128_233803-27y73vde/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">fluent-donkey-1</strong>: <a href=\"https://wandb.ai/skgbafa/words2btyes/runs/27y73vde\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes/runs/27y73vde</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:27y73vde). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">solar-waterfall-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/words2btyes\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes</a><br/>\n                Run page: <a href=\"https://wandb.ai/skgbafa/words2btyes/runs/2oosbszm\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes/runs/2oosbszm</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210128_234325-2oosbszm</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"words2btyes\")\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 5.0 # learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, config, epoch, artifacts):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != max_seq_len:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        # print(data.dtype)\n",
    "        # output = model(data, targets)\n",
    "        reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "        targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "        output = model(data, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "        output.view(-1, ntokens)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data_batches) // max_seq_len, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_source, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(max_seq_len, data_source, i)\n",
    "            \n",
    "            # print(data)\n",
    "            # print(targets)\n",
    "            if data.size(0) != max_seq_len:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            # output = model(data, targets)\n",
    "            reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "            targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "            output = model(data, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            loss = criterion(output_flat, targets)\n",
    "            # update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "            total_loss += len(data) * loss.item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Key 'n_epochs' not in config",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1aea59027ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m# The number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitalize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e9ac728853ac>\u001b[0m in \u001b[0;36minitalize_artifacts\u001b[0;34m(config, train_data_batches, val_data_batches)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate/visualize artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitalize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtraining_cel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_cel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8d116520ffe>\u001b[0m in \u001b[0;36mextract_config\u001b[0;34m(config, *argv)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfig_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Key '{key}' not in config\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mconfig_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key 'n_epochs' not in config"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, criterion, config, epoch, artifacts)\n",
    "    val_loss = evaluate(model, val_data_batches, config)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "visualize_artifacts(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'visualize_artifacts' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f6706569f4d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_artifacts' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_artifacts(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  5.51 | test ppl   246.15\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data_batches, config)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecoderOnlyTransformer(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (to_embedding): Embedding(28783, 200)\n",
       "  (linear): Linear(in_features=200, out_features=28783, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate experiment configs\n",
    "n_attention_heads_range = range(2,6)\n",
    "n_layers_range = range(2,6)\n",
    "experiment_datasets = [ Dataset.PennTreebank, Dataset.WikiText2, Dataset.WikiText103 ]\n",
    "max_seq_len_range = range()\n",
    "embedding_dimension\n",
    "# datasets\n",
    "def generateExperiements():\n",
    "    # for each dataset\n",
    "        # \n",
    "    config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 2,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"n_epochs\": 3\n",
    "    }\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightning experimentation\n",
    "train_dataset, val_dataset, test_dataset, field_processor = load_data_pl(config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling laws plots\n",
    "    # map config values to scaling laws (model size, compute, dataset size)\n",
    "\n",
    "# scaling laws goals\n",
    "    # predict test loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention in encoder and decoder layers\n",
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb sweep\n",
    "# https://docs.wandb.ai/sweeps/python-api\n",
    "\n",
    "WANDB_ENTITY = \"\"\n",
    "WANDB_PROJECT = \"\"\n",
    "\n",
    "sweep_config = {\n",
    "  \"name\": \"My Sweep\",\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"param1\": {\n",
    "            \"values\": [1, 2, 3]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run = wandb.init()\n",
    "    print(\"config:\", dict(run.config))\n",
    "    for epoch in range(35):\n",
    "        print(\"running\", epoch)\n",
    "        wandb.log({\"metric\": run.config.param1, \"epoch\": epoch})\n",
    "        time.sleep(1)\n",
    "\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ]
}