{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import enum\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from torchtext.data import Field, BPTTIterator\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "def extract_config(config, *argv):\n",
    "    assert len(argv) > 0, \"No keys to extract\"\n",
    "    config_values = []\n",
    "    for key in argv:\n",
    "        assert key in config, f\"Key '{key}' not in config\"\n",
    "        config_values.append(config[key])\n",
    "    \n",
    "    return tuple(config_values) if len(argv) > 1 else config_values[0]\n",
    "\n",
    "def validate_config(config):\n",
    "    embedding_dimension, n_attention_heads = extract_config(config, \"embedding_dimension\", \"n_attention_heads\")\n",
    "    \n",
    "    # embedding dimension must be divisible by n_attention_heads\n",
    "    assert embedding_dimension %  n_attention_heads == 0, f\"Embedding dimension ({embedding_dimension}) must be divisible by n_attention_heads ({n_attention_heads})\"\n",
    "\n",
    "def emb_to_string(emb, vocab):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character tokenizer\n",
    "# ## UTF-8 Encoder\n",
    "# def char_tokenizer(string):\n",
    "#     return [x + 2 for x in str.encode(string)]\n",
    "# def char_decoder(tokens):\n",
    "#     return \"\".join([chr(x - 2) if x > 1 else \"\" for x in tokens])\n",
    "\n",
    "def char_tokenizer(string):\n",
    "    return [x for x in string]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([x for x in tokens])\n",
    "\n",
    "# batch functions\n",
    "def batchify(data, bsz, device):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(max_seq_len, source, i):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "# load training data\n",
    "def load_data(config):\n",
    "    return load_data_word(config)\n",
    "    \n",
    "# load word based training data\n",
    "def load_data_word(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "    print(train_dataset[0:10])\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "def load_data_subword(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    field_processor = Field(tokenize=tokenizer.encode)\n",
    "\n",
    "\n",
    "    # tokenizer = get_tokenizer('subword')\n",
    "    # field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    print(train_dataset)\n",
    "    # get vocabulary\n",
    "    # field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    # vocab = field_processor.vocab\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "\n",
    "\n",
    "def load_data_character(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    # tokenizer = get_tokenizer('basic_english')\n",
    "    tokenizer = char_tokenizer\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    print(train_dataset[0:10])\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bc401558fec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# load word based training data\n",
    "print(\"[Start Load Data]\")\n",
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "\n",
    "tokenizer = get_tokenizer('subword')\n",
    "field_processor = Field(tokenize=tokenizer)\n",
    "print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Not found: \"spm.model\": No such file or directory Error #2",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2d72588bff60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'New York'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mInit\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, enable_sampling, nbest_size, alpha)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     def Init(self,\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"spm.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e06ddbdf8591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# train_data, val_data, test_data, vocab = load_data_word(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_subword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# train_data, val_data, test_data, vocab = load_data_character(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "def load_data_subword(config):\n",
    "    # load word based training data\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    field_processor = Field(tokenize=tokenizer.encode)\n",
    "\n",
    "\n",
    "    # tokenizer = get_tokenizer('subword')\n",
    "    # field_processor = Field(tokenize=tokenizer)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    print(train_dataset)\n",
    "    # get vocabulary\n",
    "    # field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    # vocab = field_processor.vocab\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "    def data_process(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    train_data = data_process(train_dataset)\n",
    "    val_data = data_process(val_dataset)\n",
    "    test_data = data_process(test_dataset)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_data, val_data, test_data, vocab\n",
    "    \n",
    "# train_data, val_data, test_data, vocab = load_data_word(config)\n",
    "train_data, val_data, test_data, vocab = load_data_subword(config) \n",
    "\n",
    "# train_data, val_data, test_data, vocab = load_data_character(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22, 1009, 81, 604, 916, 33, 31892, 1068, 4, 3, '<eos>', 17, 299, 98, 27, 774, 22, 1287, 18, 10431, 2028, 9861, 36, 51, 72, 2063, 25, 75, 87, 754, 18709, 1845, 248, 4, 3, '<eos>', 38, 29, 424, 1682, 132, 423, 24, 206, 0, 3411, 4, 3, '<eos>', 24, 24656, 25, 18, 1193, 28, 31892, 132, 50, 7624, 259, 99, 18, 21588, 471, 244, 119, 20, 17, 29816, 146, 9327, 5062, 4, 3, '<eos>', 18, 8373, 23, 1665, 24, 8689, 40, 7015, 70, 100, 578, 578, 20, 31892, 1353, 286, 3616, 4, 3, '<eos>', 52, 27, 48, 19488, 6529, 9327, 1727, 349, 17, 28187, 3084, 17, 20977, 9775, 118, 53, 20, 85, 617, 672, 17, 529, 13439, 17030, 577, 4, 3, '<eos>', 31892, 17, 26, 23, 17, 15916, 6040, 1449, 19473, 55, 76, 578, 3883, 25, 24, 151, 344, 3495, 38, 578, 578, 4, 3, '<eos>', 31892, 17, 26, 23, 24, 7841, 23, 144, 18, 226, 65, 20, 18, 127, 2624, 355, 9114, 5975, 2382, 25, 18, 17, 660, 9, 23, 9, 33, 70, 100, 0, 20, 81, 1353, 2431, 105, 4, 3, '<eos>', 31892, 11000, 18, 0, 649, 28, 18, 17, 660, 9, 23, 9, 95, 13, 305, 13, 17607, 344, 17, 3504, 765, 4, 3, '<eos>', 21, 31, 17, 28187, 17, 26, 23, 1002, 1725, 17, 3504, 765, 578, 173, 1353, 55, 4917, 420, 785, 18, 3586, 2961, 4, 3, '<eos>', 28, 66, 17, 26, 23, 763, 9105, 16585, 12141, 28, 178, 20, 17, 3504, 765, 17, 26, 23, 1567, 1700, 4, 3, '<eos>', 129, 260, 142, 31892, 6376, 288, 58, 356, 21, 1662, 4, 3, '<eos>', 4528, 29, 28, 66, 17, 26, 23, 0, 327, 39, 11967, 331, 31892, 1353, 91, 6228, 99, 17, 299, 98, 2207, 81, 3667, 2464, 33, 31892, 4, 3, '<eos>', 57, 3883, 17, 26, 23, 3604, 6850, 29, 28, 66, 51, 17, 180, 26, 46, 581, 630, 4, 3, '<eos>', 207, 85, 617, 672, 17, 26, 23, 0, 53, 21, 1101, 6838, 17, 88, 669, 249, 17, 30707, 1965, 3084, 38, 18, 17, 28187, 12250, 1338, 0, 1123, 3767, 1050, 9568, 58, 31892, 5955, 22, 859, 40, 971, 4, 3, '<eos>', 2558, 3330, 135, 18, 834, 22309, 17, 98, 213, 9, 17, 88, 669, 249, 4581, 464, 43, 19700, 23, 29, 142, 316, 1682, 53, 278, 31892, 17, 26, 23, 0, 269, 500, 22, 0, 4, 3, '<eos>', 57, 24, 4366, 424, 27, 160, 28, 66, 0, 90, 17, 299, 98, 18, 185, 17, 26, 23, 942, 1965, 5843, 3647, 76, 81, 1782, 629, 33, 31892, 4, 3, '<eos>', 38, 18, 1070, 28, 66, 4480, 53, 248, 293, 1756, 0, 58, 3306, 22, 2153, 28, 66, 17, 26, 23, 31892, 3979, 22, 75, 578, 578, 4, 3, '<eos>', 17, 299, 98, 27, 558, 22, 0, 5158, 17, 7967, 578, 173, 6490, 578, 173, 37, 14312, 106, 31892, 1353, 21, 137, 521, 31892, 988, 17, 26, 23, 3691, 20, 48, 11355, 578, 578, 3979, 4, 3, '<eos>', 2604, 676, 18, 398, 7928, 77, 53, 850, 1935, 17574, 22, 1627, 109, 1317, 2626, 21741, 31892, 17, 26, 23, 16441, 3388, 20, 578, 1835, 4, 3, '<eos>', 31892, 6376, 74, 47, 22, 0, 148, 24, 420, 13, 21203, 4434, 4, 3, '<eos>', 28, 66, 327, 2134, 18, 2393, 37, 2128, 24, 410, 2340, 108, 13128, 21, 18, 17, 660, 9, 267, 9, 146, 1178, 22, 1907, 18, 932, 13, 6082, 2249, 8943, 319, 4, 3, '<eos>', 17, 150, 232, 28, 66, 27, 223, 22, 280, 78, 33, 410, 4389, 0, 17, 98, 213, 9, 17, 88, 669, 249, 349, 4, 3, '<eos>', 28, 66, 1578, 31892, 172, 178, 4, 3, '<eos>', 17, 660, 9, 23, 9, 9327, 13, 7940, 30045, 61, 132, 224, 161, 578, 578, 21, 578, 578, 20, 31892, 121, 371, 28, 66, 456, 2873, 22, 2422, 18, 17, 299, 98, 629, 4, 3, '<eos>', 17, 299, 98, 327, 0, 4, 3, '<eos>', 137, 0, 53, 349, 44, 133, 24, 12659, 375, 161, 87, 172, 2628, 172, 2593, 440, 463, 4, 3, '<eos>', 43, 3764, 31892, 17, 26, 23, 763, 728, 121, 0, 22, 161, 17, 7967, 578, 21, 17, 7967, 578, 6490, 578, 22, 6490, 578, 4, 3, '<eos>', 105, 17, 26, 23, 1206, 24, 1081, 20, 991, 263, 25, 18, 31892, 1353, 193, 176, 464, 63, 47, 446, 76, 11388, 349, 112, 6381, 17, 22116, 672, 24, 1974, 1416, 28, 0, 0, 7472, 988, 4, 3, '<eos>', 38, 18, 1070, 43, 13199, 22, 435, 18, 1338, 17, 26, 23, 578, 31892, 1353, 4, 3, '<eos>', 18, 1046, 27, 29, 31892, 17, 26, 23, 763, 728, 121, 11181, 108, 17, 299, 98, 17, 26, 23, 980, 33, 31892, 3810, 0, 78, 81, 17, 660, 9, 23, 9, 3262, 4, 3, '<eos>', 28, 66, 17, 26, 23, 13822, 22, 762, 31892, 121, 6430, 0, 95, 92, 1446, 108, 17, 1512, 2460, 27, 24, 12666, 5320, 15952, 349, 17, 22116, 431, 672, 48, 1965, 3084, 38, 17, 28187, 17, 26, 23, 116, 14498, 557, 7212, 4, 3, '<eos>', 43, 7836, 17, 1512, 2460, 13, 23, 2392, 780, 17, 2460, 31, 17, 5994, 765, 53, 6460, 18, 1634, 20, 578, 578, 20, 81, 398, 2069, 22, 28, 66, 18, 463, 47, 72, 8249, 2492, 2218, 28, 399, 4, 3, '<eos>', 11651, 218, 66, 0, 379, 21, 735, 1257, 1674, 20, 0, 688, 0, 25, 369, 9, 87, 754, 526, 1245, 45, 17, 6650, 6650, 597, 1680, 1338, 17, 26, 23, 578, 31892, 24, 7841, 23, 38, 75, 578, 231, 441, 24, 0, 2336, 31, 24, 1757, 3333, 38, 578, 578, 25, 319, 132, 4, 3, '<eos>', 17, 150, 449, 18, 0, 20, 24, 12659, 375, 4456, 55, 486, 43, 349, 4, 3, '<eos>', 20, 477, 29, 30, 134, 28, 66, 17, 26, 23, 1290, 579, 4, 3, '<eos>', 31892, 17, 3405, 369, 5251, 0, 4, 3, '<eos>', 264, 5496, 1835, 4, 3, '<eos>', 119, 1348, 17, 9538, 9, 578, 578, 4, 3, '<eos>', 2839, 6490, 578, 337, 4, 3, '<eos>', 1548, 1378, 6490, 578, 173, 49, 578, 2305, 24, 763, 4, 3, '<eos>', 89, 455, 1348, 17, 10003, 93, 578, 578, 4, 3, '<eos>', 1548, 1059, 6490, 578, 173, 3371, 9, 1548, 1378, 6490, 578, 173, 49, 578, 2305, 24, 763, 4, 3, '<eos>', 0, 1362, 1700, 2961, 5595, 1353, 4840, 578, 173, 4, 3, '<eos>', 2039, 71, 2068, 41, 8503, 91, 17, 660, 9, 23, 9, 517, 515, 31, 604, 1725, 1210, 4, 3, '<eos>', 0, 17, 23, 17978, 578, 123, 532, 709, 29, 43, 53, 8872, 244, 24, 6295, 368, 34, 1770, 21, 735, 1317, 1674, 20, 52, 0, 626, 21, 0, 678, 5843, 4, 3, '<eos>', 116, 4200, 30, 1262, 812, 4, 3, '<eos>', 17, 98, 213, 9, 17, 23, 17978, 851, 22, 1313, 31, 18, 1036, 259, 45, 604, 1141, 18392, 25, 24, 6295, 368, 578, 24, 0, 908, 42, 4, 3, '<eos>', 109, 1953, 8314, 25, 369, 9, 20, 17, 23, 10716, 17, 1751, 17, 3800, 1963, 9, 42, 17, 10048, 577, 17, 508, 9, 11028, 12841, 61, 3028, 24, 578, 578, 3979, 4491, 24, 632, 6376, 17, 26, 492, 244, 80, 66, 4048, 765, 22, 2709, 237, 604, 6246, 21, 0, 48, 2602, 17774, 4, 3, '<eos>', 17, 98, 213, 9, 11028, 12841, 17, 26, 23, 227, 913, 1757, 830, 9, 20, 17, 23, 7575, 2377, 2147, 180, 9, 3692, 127, 20, 81, 3979, 129, 24, 14905, 639, 25, 48, 6490, 0, 6756, 670, 28, 109, 1953, 24, 5843, 20, 0, 3259, 4, 3, '<eos>', 109, 1953, 42, 17, 98, 213, 9, 11028, 12841, 27, 2860, 6376, 22, 6618, 114, 65, 748, 10243, 202, 17, 213, 9, 0, 24, 109, 1953, 2186, 379, 4, 3, '<eos>', 18, 1036, 27, 17, 180, 26, 46, 19466, 24, 17774, 20, 81, 224, 21, 18, 86, 237, 604, 6246, 112, 17, 180, 26, 46, 210, 22, 1613, 1756, 18, 632, 492, 1157, 109, 1953, 42, 4, 3, '<eos>', 17, 98, 213, 9, 11028, 12841, 27, 18, 109, 2085, 21, 1578, 22, 2668, 475, 42, 0, 17, 508, 9, 260, 202, 109, 1953, 17, 26, 23, 1770, 4, 3, '<eos>', 0, 24, 299, 24, 383, 17, 23, 3164, 23, 23, 3456, 21, 9563, 256, 42, 29, 81, 256, 820, 1004, 578, 578, 22, 578, 337, 9159, 6490, 578, 337, 25, 18, 89, 1349, 399, 20, 52, 119, 33, 737, 4219, 25, 71, 7898, 4, 3, '<eos>', 24, 119, 743, 820, 15361, 578, 337, 9159, 4, 3, '<eos>', 1654, 3031, 1210, 21, 737, 820, 849, 687, 22, 24, 5138, 1735, 25, 17263, 2336, 25, 18, 643, 1082, 18, 226, 190, 17, 180, 26, 46, 444, 2068, 34, 27, 0, 33, 17, 23, 3164, 23, 23, 463, 4, 3, '<eos>', 0, 42, 36, 5883, 24, 5138, 771, 25, 17263, 2336, 28, 18, 410, 119, 24530, 383, 3031, 724, 1084, 4, 3, '<eos>', 0, 30910, 24, 17, 29816, 5843, 20, 920, 5476, 21, 3964, 1476, 1926, 24, 578, 578, 15323, 25, 29636, 2336, 28, 18, 1290, 119, 4, 3, '<eos>', 18, 17, 7967, 578, 173, 6490, 578, 173, 25, 29636, 2336, 28, 18, 578, 399, 22, 17, 10003, 93, 578, 30, 151, 40, 17, 7967, 578, 173, 6490, 578, 173, 24, 119, 743, 21, 868, 344, 3880, 20, 17, 7967, 578, 173, 21, 17, 7967, 578, 173, 4, 3, '<eos>', 18, 11181, 25, 2336, 59, 301, 1433, 5338, 820, 30, 7434, 22, 1283, 1291, 28, 1322, 21, 708, 33, 1342, 0, 4, 3, '<eos>', 0, 17, 26, 23, 2336, 99, 2905, 865, 24, 12395, 7215, 578, 578, 22, 17, 7967, 578, 173, 40, 17, 7967, 578, 173, 24, 119, 743, 4, 3, '<eos>', 820, 12618, 76, 10258, 1279, 22, 17, 7967, 578, 173, 40, 17, 7967, 578, 173, 24, 119, 743, 4, 3, '<eos>', 4012, 10721, 17, 13325, 9, 17, 26, 23, 3042, 849, 952, 22, 0, 29, 20, 127, 20, 81, 7302, 21, 1391, 25, 18, 739, 13, 13514, 624, 34, 36, 550, 24, 578, 578, 3380, 25, 4186, 89, 13, 4699, 3042, 31, 24, 578, 578, 2839, 2127, 4, 3, '<eos>', 18, 1170, 7778, 8940, 9, 226, 24, 10520, 1603, 5073, 4600, 20, 2028, 1135, 21, 1058, 28, 739, 13, 13514, 7928, 21, 1623, 550, 1548, 1378, 28, 18, 1201, 1348, 28815, 9, 578, 20, 6490, 578, 173, 49, 578, 2305, 24, 763, 76, 40, 6490, 578, 173, 49, 578, 2305, 24, 763, 25, 18, 119, 13, 8218, 643, 4, 3, '<eos>', 2839, 1004, 22, 6490, 578, 173, 40, 6490, 578, 173, 4, 3, '<eos>', 4012, 10721, 2026, 3624, 36, 74, 47, 24, 737, 1201, 37, 23726, 81, 2839, 2127, 31, 17, 3374, 46, 9, 578, 3664, 24, 6490, 578, 24, 763, 3380, 25, 81, 1002, 4, 3, '<eos>', 57, 81, 1002, 4651, 292, 3883, 34, 36, 10035, 10653, 737, 13510, 31, 186, 820, 4, 3, '<eos>', 4012, 10721, 17, 26, 23, 1002, 1004, 6490, 578, 24, 763, 25, 439, 95, 13, 305, 13, 17607, 1700, 22, 6490, 578, 4, 3, '<eos>', 18, 1002, 54, 645, 24, 227, 20, 6490, 578, 24, 763, 319, 129, 260, 57, 6704, 22, 6490, 578, 25, 18, 17, 5994, 765, 1002, 15323, 4, 3, '<eos>', 18, 226, 54, 72, 10682, 17957, 56, 13510, 149, 20, 1283, 820, 20, 1058, 2079, 59, 47, 903, 13510, 100, 112, 2028, 1135, 4, 3, '<eos>', 57, 18, 226, 42, 29, 4245, 30, 8990, 25, 18, 89, 1201, 37, 352, 7690, 20, 3389, 21, 0, 25, 3856, 4, 3, '<eos>', 34, 24, 726, 4012, 10721, 17, 26, 23, 720, 20, 3195, 34, 24, 3289, 20, 820, 865, 578, 578, 40, 18, 119, 13, 8218, 1201, 21, 578, 578, 40, 18, 1085, 643, 4, 3, '<eos>', 18, 4245, 4105, 76, 18, 0, 6176, 1548, 1378, 34, 24, 3289, 20, 6622, 22, 578, 578, 25, 18, 1201, 1596, 33, 578, 578, 24, 119, 743, 4, 3, '<eos>', 4012, 10721, 361, 42, 18, 737, 825, 77, 6876, 3304, 227, 1480, 28, 81, 1058, 2079, 21, 2028, 1135, 4, 3, '<eos>', 171, 36, 51, 5527, 29, 1729, 849, 25, 1591, 820, 20, 739, 3668, 27, 12278, 22, 75, 24, 578, 578, 16441, 724, 81, 224, 678, 41, 2419, 38, 24, 178, 3477, 724, 149, 142, 41, 20987, 22, 18, 0, 239, 20, 18, 344, 4, 3, '<eos>', 29, 7295, 2455, 22, 701, 737, 0, 4219, 171, 18, 0, 49, 13642, 7295, 20, 18, 624, 27, 10682, 18046, 849, 49, 176, 820, 16095, 4, 3, '<eos>', 1596, 33, 81, 1085, 1201, 18, 484, 643, 20, 81, 578, 4186, 119, 1548, 1004, 578, 578, 21, 820, 1004, 578, 578, 4, 3, '<eos>', 475, 527, 17, 13325, 9, 17, 1841, 21026, 5144, 23, 1145, 24, 1935, 503, 980, 33, 12430, 3716, 920, 1135, 25, 369, 9, 22, 0, 48, 5714, 9848, 4797, 25, 623, 3924, 4, 3, '<eos>', 12430, 3716, 27, 24, 691, 25, 113, 27, 318, 34, 0, 257, 9848, 49, 17, 4156, 369, 24, 913, 13783, 29813, 23, 21, 7900, 1058, 4, 3, '<eos>', 25, 4391, 12430, 3716, 475, 527, 3655, 294, 7302, 25, 0, 17, 4156, 369, 34, 24, 109, 811, 1552, 4, 3, '<eos>', 2247, 1476, 17, 13325, 9, 24250, 3668, 25, 369, 9, 17, 23368, 17, 13325, 9, 21, 256, 6446, 447, 538, 47, 930, 6656, 33, 12430, 3716, 515, 25, 14540, 15767, 17, 3800, 1963, 4, 3, '<eos>', 475, 527, 42, 36, 5883, 81, 89, 0, 587, 10535, 2138, 22, 39, 2090, 244, 119, 4, 3, '<eos>', 18, 4434, 33, 12430, 3716, 1624, 28, 475, 527, 22, 763, 81, 5874, 25, 527, 3386, 18, 463, 42, 4, 3, '<eos>', 475, 527, 77, 42, 36, 27, 2010, 113, 36, 271, 24, 0, 920, 18, 0, 578, 2238, 28, 3582, 7652, 21, 86, 1623, 20, 0, 0, 3668, 4, 3, '<eos>', 0, 1002, 31109, 48, 1268, 6490, 578, 22, 6490, 578, 34, 17, 29816, 562, 8179, 3624, 36, 132, 0, 38, 124, 23888, 7595, 1040, 20, 18, 24, 7157, 1415, 6490, 578, 337, 971, 13, 1281, 20, 9114, 562, 17, 26, 23, 4090, 4, 3, '<eos>', 17, 5908, 51, 4700, 6490, 578, 49, 578, 578, 25, 18, 139, 1700, 307, 196, 8921, 20, 18, 971, 13, 1281, 17, 26, 23, 4724, 19915, 68, 18, 1002, 344, 4, 3, '<eos>', 6364, 8146, 17, 1880, 3454, 8075, 4856, 42, 43, 1484, 1957, 25, 17, 5908, 57, 43, 1695, 45, 743, 6490, 3213, 13, 101, 13, 8180, 0, 2340, 4, 3, '<eos>', 1002, 843, 865, 16026, 25, 1567, 1700, 5954, 37, 8892, 13, 3361, 367, 2419, 21, 608, 16095, 37, 17, 5908, 21, 86, 4413, 2382, 4, 3, '<eos>', 18, 17, 9493, 17, 16884, 202, 2307, 23, 946, 177, 578, 397, 38, 578, 99, 23794, 95, 578, 397, 25, 18, 907, 4, 3, '<eos>', 3020, 843, 1348, 903, 99, 48, 319, 3789, 171, 18, 1691, 30, 3244, 4, 3, '<eos>', 18, 17, 660, 9, 23, 9, 750, 4114, 28188, 22, 6490, 578, 337, 25, 24, 14905, 639, 12717, 9135, 29, 18, 1051, 17, 26, 23, 4416, 1323, 54, 12999, 4, 3, '<eos>', 4213, 2574, 28, 18, 205, 432, 25, 24, 3559, 171, 6670, 1004, 22, 24, 598, 4, 3, '<eos>', 48, 3084, 271, 36, 65, 20, 18, 2598, 750, 1107, 196, 18, 1691, 0, 78, 25, 4, 3, '<eos>', 2307, 3388, 865, 578, 578, 25, 28815, 22216, 18, 1290, 1236, 3856, 27, 12278, 4, 3, '<eos>', 48, 3084, 4670, 11977, 688, 1776, 21, 4213, 4, 3, '<eos>', 15868, 2269, 462, 6490, 578, 337, 22, 5577, 28, 393, 185, 1728, 18, 1290, 534, 1013, 22, 182, 148, 24, 1101, 4, 3, '<eos>', 36, 5883, 24, 6490, 578, 337, 11963, 1059, 4, 3, '<eos>', 2349, 6225, 13325, 1926, 24, 578, 578, 1907, 25, 11963, 2336, 4, 3, '<eos>', 4876, 17, 2643, 2249, 54, 24, 1059, 542, 22, 24, 534, 4910, 864, 4, 3, '<eos>', 1013, 20, 109, 17, 5618, 729, 851, 22, 1523, 106, 1354, 21, 2888, 177, 578, 578, 20, 81, 154, 779, 99, 24, 119, 20, 3570, 3042, 21, 10087, 1361, 708, 4, 3, '<eos>', 2426, 7964, 17, 26, 13982, 47, 4117, 10261, 2602, 6417, 22, 24, 4893, 578, 21125, 149, 63, 41, 12729, 33, 18, 5804, 17, 26, 23, 1290, 2393, 4, 3, '<eos>', 7406, 13, 25995, 23, 1178, 22, 971, 455, 20, 351, 706, 3986, 40, 1647, 756, 28, 6490, 578, 173, 4, 3, '<eos>', 18, 579, 121, 10077, 24, 109, 2134, 22, 92, 375, 1357, 17, 26, 23, 192, 1809, 495, 4, 3, '<eos>', 18, 2349, 2378, 369, 851, 22, 8373, 7600, 1700, 31, 14426, 1658, 25, 59, 6786, 971, 21, 1523, 207, 28, 58, 224, 1122, 21, 28, 2260, 4, 3, '<eos>', 18, 579, 27, 774, 22, 4622, 6786, 4, 3, '<eos>', 2830, 5958, 1770, 17, 1022, 1500, 249, 42, 29, 17, 31314, 4552, 1123, 1361, 20, 17618, 780, 170, 47, 72, 4665, 25, 578, 22, 3104, 2931, 43, 2108, 53, 720, 11515, 34, 178, 34, 6490, 578, 337, 4, 3, '<eos>', 24, 6490, 578, 337, 1776, 1325, 30, 1951, 37, 480, 13, 2360, 1167, 22088, 10119, 29, 1049, 383, 6025, 9171, 18, 819, 2556, 344, 4, 3, '<eos>', 6787, 118, 17, 26, 23, 17, 660, 9, 23, 9, 1591, 27, 558, 22, 2125, 25, 4926, 52, 260, 22, 1523, 81, 110, 9030, 14200, 118, 12785, 270, 3284, 22, 988, 549, 22, 4480, 4, 3, '<eos>', 18, 4114, 13, 88, 11262, 1325, 403, 0, 95, 1074, 22, 23741, 18, 480, 1040, 20, 18, 2592, 25, 2511, 20, 24, 480, 13, 2360, 1167, 1008, 4, 3, '<eos>', 5513, 1485, 42, 916, 47, 1348, 33, 245, 1220, 8689, 20, 81, 3034, 1812, 4, 3, '<eos>', 139, 534, 1208, 7928, 1926, 9024, 393, 13, 4699, 3042, 4, 3, '<eos>', 23445, 267, 17, 26, 23, 2336, 6224, 578, 578, 375, 1357, 13, 6798, 5500, 17, 26, 23, 578, 578, 21, 17, 8252, 17, 1258, 7749, 17, 26, 23, 578, 578, 4, 3, '<eos>', 1658, 4, 3, '<eos>', 2382, 2961, 578, 1353, 4, 3, '<eos>', 17, 9493, 17, 16884, 202, 2307, 23, 578, 177, 578, 4205, 578, 177, 578, 12038, 578, 177, 578, 4, 3, '<eos>', 4388, 85, 617, 672, 17, 529, 13439, 17030, 577, 21061, 1663, 578, 177, 4, 3, '<eos>', 14426, 17, 9493, 17, 16884, 202, 8892, 1663, 578, 7159, 2080, 1663, 578, 177, 578, 4, 3, '<eos>', 1691, 578, 2293, 76, 578, 578, 4104, 177, 578, 4, 3, '<eos>', 7063, 1797, 0, 580, 2229, 20, 0, 3221, 24, 4440, 13, 21170, 1338, 515, 25, 416, 1929, 2133, 17, 3800, 1963, 9, 30, 812, 24, 748, 20, 52, 920, 226, 4, 3, '<eos>', 17, 98, 213, 9, 0, 578, 123, 532, 7655, 4069, 18, 1036, 22, 800, 340, 4, 3, '<eos>', 634, 748, 17, 6684, 3742, 0, 51, 42, 43, 17, 6362, 17, 180, 26, 46, 2304, 17, 88, 13, 5347, 38, 18, 226, 17, 26, 23, 1360, 492, 244, 432, 4, 3, '<eos>', 0, 12542, 25, 369, 9, 18, 17, 5546, 11166, 202, 1257, 4164, 795, 6490, 0, 2340, 28, 0, 3856, 830, 9, 30, 11000, 1050, 37, 24, 6230, 670, 40, 24, 17, 23, 7954, 1406, 1911, 10035, 29, 36, 1245, 81, 1253, 578, 578, 0, 3979, 4, 3, '<eos>', 0, 24, 0, 25, 66, 9, 0, 5573, 54, 30236, 0, 17, 26, 23, 2393, 4, 3, '<eos>', 36, 51, 196, 442, 13128, 50, 22, 1262, 6756, 58, 1353, 168, 24, 644, 6490, 0, 49, 6490, 578, 173, 2340, 40, 17, 2460, 17, 3125, 722, 20, 17, 23, 1603, 1426, 259, 0, 6246, 47, 1570, 58, 5850, 4, 3, '<eos>', 25, 24, 6251, 21, 1725, 2520, 6610, 0, 42, 36, 1245, 18, 578, 0, 1353, 28, 6490, 578, 173, 25, 24, 804, 6686, 31, 17, 3374, 46, 9, 578, 4, 3, '<eos>', 0, 190, 17, 180, 26, 46, 3201, 18, 8689, 20, 18, 1353, 57, 18, 1157, 20, 18, 0, 1060, 37, 65, 191, 18, 17, 23, 7954, 1406, 1911, 17, 26, 23, 6756, 670, 21, 18, 3624, 728, 20, 18, 1353, 1245, 0, 17, 3125, 722, 17, 26, 23, 6490, 0, 6756, 670, 728, 4, 3, '<eos>', 24, 0, 3855, 42, 18, 226, 1245, 18, 1002, 25, 18, 433, 344, 21, 2189, 121, 17, 180, 26, 46, 3201, 18, 8689, 49, 6085, 4, 3, '<eos>', 17, 2311, 590, 0, 578, 123, 532, 51, 72, 1726, 22, 18, 1036, 20, 52, 25926, 4, 3, '<eos>', 17, 98, 213, 9, 0, 317, 379, 20, 9114, 1320, 440, 21, 18, 0, 0, 1090, 127, 1050, 0, 0, 0, 3221, 24, 0, 789, 5981, 1338, 4, 3, '<eos>', 17, 98, 213, 9, 0, 18, 89, 45, 4805, 556, 601, 22, 1613, 34, 24, 830, 218, 23, 748, 27, 48, 864, 22, 18, 1036, 2395, 81, 3449, 22, 1349, 4, 3, '<eos>', 0, 17, 23, 9, 101, 9, 24, 17, 30707, 789, 21, 5688, 256, 550, 24, 316, 1735, 25, 81, 29132, 89, 13, 5960, 256, 2336, 16834, 0, 1793, 22, 578, 173, 9159, 6490, 578, 173, 40, 578, 173, 9159, 24, 119, 743, 4, 3, '<eos>', 18, 0, 256, 42, 81, 743, 17809, 29, 256, 2336, 28, 71, 20, 578, 74, 39, 474, 22, 18, 578, 173, 9159, 1926, 28, 578, 1484, 6125, 4, 3, '<eos>', 637, 91, 1122, 555, 88, 3451, 2154, 4219, 21, 2931, 0, 17, 26, 23, 256, 1548, 1378, 28, 18, 89, 404, 399, 20, 52, 119, 15361, 578, 173, 9159, 10448, 1620, 18, 119, 13, 28639, 1586, 20, 578, 173, 9159, 4, 3, '<eos>', 2604, 42, 0, 17, 26, 23, 3042, 25, 18, 205, 455, 327, 39, 12735, 37, 24, 688, 2127, 40, 18, 1634, 20, 18, 17, 21605, 23, 3021, 20, 24, 0, 226, 29, 27, 578, 578, 2431, 37, 0, 4, 3, '<eos>', 0, 25, 369, 9, 0, 17, 98, 66, 9, 42, 36, 596, 2679, 40, 18, 17, 660, 9, 23, 9, 626, 21, 1208, 1048, 22, 344, 24, 5555, 934, 29, 53, 3356, 25, 0, 21, 1225, 20, 0, 21, 0, 1847, 4, 3, '<eos>', 18, 0, 7104, 0, 934, 27, 70, 4668, 100, 2166, 2703, 28, 0, 18, 1037, 20, 1847, 548, 36, 51, 1912, 49, 548, 105, 27, 24, 0, 405, 1225, 42, 0, 379, 1101, 6838, 808, 118, 4, 3, '<eos>', 17, 98, 213, 9, 808, 118, 42, 18, 934, 3097, 53, 39, 179, 25, 0, 33, 0, 21, 86, 2703, 57, 1707, 327, 401, 18, 5655, 28, 13760, 2185, 4, 3, '<eos>', 17, 98, 213, 9, 808, 118, 42, 18, 934, 53, 39, 9018, 25, 578, 307, 22, 5299, 21, 4494, 17228, 4, 3, '<eos>', 17, 7841, 9, 0, 17, 21664, 672, 24, 1847, 1225, 6273, 38, 18, 439, 1847, 7212, 42, 18, 934, 27, 2624, 179, 25, 557, 4422, 57, 27, 17, 180, 26, 46, 491, 24, 383, 1585, 149, 36, 27, 114, 6151, 2763, 25, 6718, 18, 127, 1676, 1225, 4, 3, '<eos>', 57, 18, 934, 132, 3392, 22, 39, 70, 4765, 25, 10252, 548, 24, 13760, 51, 1912, 49, 1061, 405, 1225, 17, 7841, 9, 17, 21664, 672, 42, 4, 3, '<eos>', 80, 112, 17, 180, 26, 46, 175, 655, 160, 2763, 36, 17, 26, 23, 223, 22, 39, 43, 42, 4, 3, '<eos>', 0, 24, 0, 8425, 20, 5555, 1020, 2703, 1526, 29, 18, 1847, 934, 53, 222, 36, 22, 701, 81, 0, 2336, 181, 18, 89, 1201, 20, 578, 17, 98, 213, 9, 808, 118, 42, 4, 3, '<eos>', 18, 226, 53, 1336, 6490, 578, 28, 24, 934, 21, 1526, 75, 6490, 578, 173, 25, 2839, 40, 18, 934, 181, 18, 89, 578, 399, 20, 1917, 43, 42, 4, 3, '<eos>', 17, 6463, 15562, 17, 13325, 9, 0, 17, 1463, 9, 42, 36, 3692, 18, 4494, 17228, 20, 3126, 355, 1063, 14810, 7248, 25, 369, 9, 25, 24, 1454, 21, 6251, 6686, 10033, 38, 6490, 578, 173, 4, 3, '<eos>', 17, 6463, 15562, 42, 81, 15368, 2431, 0, 25, 369, 9, 1591, 1373, 6490, 578, 173, 25, 1454, 915, 6490, 578, 173, 25, 3128, 21, 6490, 578, 173, 25, 5684, 1002, 22, 6762, 1063, 17, 26, 23, 20088, 25, 18, 2237, 17, 660, 9, 23, 9, 4, 3, '<eos>', 17, 6463, 15562, 59, 1176, 4494, 7248, 472, 0, 33, 1063, 515, 25, 0, 17, 3800, 1963, 9, 25, 24, 243, 20, 689, 4, 3, '<eos>', 1756, 8380, 24, 13082, 18, 3178, 170, 444, 0, 42, 17, 20278, 0, 17, 6463, 15562, 17, 26, 23, 735, 638, 1674, 4, 3, '<eos>', 36, 77, 53, 373, 17, 6463, 15562, 109, 1658, 4, 3, '<eos>', 25, 17, 5546, 11166, 202, 28, 717, 1063, 51, 54, 24, 737, 344, 740, 171, 17, 6463, 15562, 17, 26, 23, 2059, 51, 72, 486, 3788, 549, 22, 17, 98, 213, 9, 0, 4, 3, '<eos>']]\n",
      "Build Vocab (10.244807s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-02fd3fc10367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-02fd3fc10367>\u001b[0m in \u001b[0;36mdata_process\u001b[0;34m(tt_dataset_split)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n\u001b[0m\u001b[1;32m     45\u001b[0m                         dtype=torch.long) for item in raw_text_iter]\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-02fd3fc10367>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n\u001b[0m\u001b[1;32m     45\u001b[0m                         dtype=torch.long) for item in raw_text_iter]\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/transformers-4.2.2-py3.8.egg/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \"\"\"\n\u001b[1;32m   2273\u001b[0m         \u001b[0;31m# Input type checking for clearer error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         assert isinstance(text, str) or (\n\u001b[0m\u001b[1;32m   2275\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m             and (\n",
      "\u001b[0;31mAssertionError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# load word based training data\n",
    "print(\"[Start Load Data]\")\n",
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "\n",
    "\n",
    "class HuggingFaceField(Field):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenize=tokenizer.encode)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def numericalize(self, arr):\n",
    "        arr = [self.tokenizer.convert_tokens_to_ids(x) for x in arr]\n",
    "        return torch.tensor(arr)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased',  eos_token=\"<eos>\")\n",
    "field_processor = HuggingFaceField(tokenizer=tokenizer)\n",
    "        \n",
    "\n",
    "# tokenizer = get_tokenizer('subword')\n",
    "# field_processor = Field(tokenize=tokenizer)\n",
    "print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "print(f\"Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "print(train_dataset)\n",
    "print([x for x in train_dataset.text])\n",
    "# print(vocab)\n",
    "# get vocabulary\n",
    "# field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "# vocab = field_processor.vocab\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "print(f\"Build Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "\n",
    "def data_process(tt_dataset_split):\n",
    "    raw_text_iter = tt_dataset_split[0].text\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                        dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(train_dataset)\n",
    "val_data = data_process(val_dataset)\n",
    "test_data = data_process(test_dataset)\n",
    "\n",
    "print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "return train_data, val_data, test_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([6476, 6138, 7909,  ...,   10,    0,    3]) tensor([1076,   97,  362,  ...,    8,    4,    3]) tensor([101,  17,  27,  ...,  24, 512,   3])\naer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> n years old will join the board as a nonexecutive director nov . n <eos> mr . <unk> is chairman of <unk> n . v . the dutch publishing group <eos> rudolph <unk> n years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate <eos> a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer\nconsumers may want to move their telephones a little closer to the tv set <eos> <unk> <unk> watching abc ' s monday night football can now vote during <unk> for the greatest play in n years from among four or five <unk> <unk> <eos> two weeks ago viewers of several nbc <unk> consumer segments started calling a n number for advice on various <unk> issues <eos> and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day ' s show <eos> interactive telephone technology has taken a new leap in <unk> and\nno it was n ' t black monday <eos> but while the new york stock exchange did n ' t fall apart friday as the dow jones industrial average plunged n points most of it in the final hour it barely managed to stay this side of chaos <eos> some circuit breakers installed after the october n crash failed their first test traders say unable to cool the selling panic in both stocks and futures <eos> the n stock specialist firms on the big board floor the buyers and sellers of last resort who were criticized after the n crash\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "print(train_data, val_data, test_data)\n",
    "print(emb_to_string(train_data[0:100], vocab))\n",
    "print(emb_to_string(val_data[0:100], vocab))\n",
    "print(emb_to_string(test_data[0:100], vocab))\n",
    "\n",
    "vocab.__dict__.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "def char_tokenizer(string):\n",
    "    return [x for x in string]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([x for x in tokens])\n",
    "\n",
    "char_decoder(char_tokenizer(\"this is a test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\ue302 this ',\n",
       " ' is ',\n",
       " ' a ',\n",
       " ' test ',\n",
       " ' string ',\n",
       " ' to ',\n",
       " ' test ',\n",
       " ' the ',\n",
       " ' tokenization ',\n",
       " ' process ',\n",
       " '. ']"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "# train_data, val_data, test_data, vocab = load_data(config)\n",
    "\n",
    "test_string = \"This is a test string to test the tokenization process.\"\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "tokenizer = get_tokenizer('subword')\n",
    "tokenizer(test_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6eb1807fc3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"n_encoder_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"n_decoder_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPennTreebank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BYTE = 3\n",
    "    BBPE = 4\n",
    "\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"segmentation\": Segmentation.Word,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# character tokenizer\n",
    "def char_tokenizer(string):\n",
    "    return [x + 2 for x in str.encode(string)]\n",
    "def char_decoder(tokens):\n",
    "    return \"\".join([chr(x - 2) if x > 1 else \"\" for x in tokens])\n",
    "\n",
    "# switch \n",
    "def get_tokenizer_config(segmentation): \n",
    "    switcher = { \n",
    "        Segmentation.Word: get_tokenizer('basic_english'),\n",
    "        # Segmentation.Subword: XLNetTokenizer.from_pretrained('xlnet-base-cased'), \n",
    "        Segmentation.Subword: BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "        Segmentation.Character: char_tokenizer, \n",
    "        Segmentation.BYTE: \"one\", \n",
    "        Segmentation.BBPE: \"one\", \n",
    "    } \n",
    "    # default to word\n",
    "    return switcher.get(segmentation, get_tokenizer('basic_english'))\n",
    "\n",
    "# train_data, val_data, test_data, vocab = load_data(config)\n",
    "\n",
    "test_string = \"This is a preconfigured test string to test the tokenization process. Segmentation is like fragmentation.\"\n",
    "dataset, segmentation = extract_config(config, \"dataset\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset.name) \n",
    "tokenizer = get_tokenizer('subword')\n",
    "# tokenizer = get_tokenizer_config(segmentation) \n",
    "print(tokenizer)\n",
    "tokenizer(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=798011.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02c42360a4e045a58e9a3b01552dc370"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nPreTrainedTokenizer(name_or_path='xlnet-large-cased', vocab_size=32000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>', 'additional_special_tokens': ['<eop>', '<eod>']})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17, 11368, 19, 94, 2288, 27, 10920, 4, 3]\ndict_keys(['_tokenizer', 'init_inputs', 'init_kwargs', 'name_or_path', 'model_max_length', 'padding_side', 'model_input_names', 'deprecation_warnings', '_bos_token', '_eos_token', '_unk_token', '_sep_token', '_pad_token', '_cls_token', '_mask_token', '_pad_token_type_id', '_additional_special_tokens', 'verbose', 'do_lower_case', 'remove_space', 'keep_accents', 'vocab_file'])\nHello, my dog is cute<sep><cls>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'<eos>'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-44681c2841fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# last_hidden_states = outputs.last_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_id_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '<eos>'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# tokenizer = get_tokenizer('subword')\n",
    "# model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "inputs = tokenizer.encode(\"Hello, my dog is cute\")\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\")\n",
    "\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.__dict__.keys())\n",
    "# print(tokenizer._convert_id_to_token(inputs[]))\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(tokenizer.convert_tokens_to_string([tokenizer._convert_id_to_token(x) for x in inputs]))\n",
    "# emb_to_string(inputs, vocab)\n",
    "# load_vocab(tokenizer.vocab_file)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "tokenizer._convert_id_to_token(3)\n",
    "vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[99, 100, 101, 102, 67, 68, 69, 70]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'abcdABCD'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "\n",
    "tokens = [x + 2 for x in str.encode(\"abcdABCD\")]\n",
    "print(tokens)\n",
    "char_decoder(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0055354691c34c4b99dd9b8d79c86d88"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87df526897634511bca749cfc1478254"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightning stuff\n",
    "def load_data_pl(config): \n",
    "    # get dataset\n",
    "    dataset = extract_config(config, \"dataset\")\n",
    "    dataset = getattr(datasets, dataset.name) \n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(text_field=field_processor)\n",
    "    \n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, field_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate/visualize artifacts\n",
    "def initalize_artifacts(config, train_data_batches, val_data_batches):\n",
    "        n_epochs, max_seq_len = extract_config(config, \"n_epochs\", \"max_seq_len\")\n",
    "        training_cel = torch.ones(n_epochs, math.ceil(len(train_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        validation_cel = torch.ones(n_epochs, math.ceil(len(val_data_batches) / max_seq_len)) * float(\"inf\")\n",
    "        artifacts = {\n",
    "            \"training\": {\n",
    "                \"CrossEntropyLoss\": training_cel\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"CrossEntropyLoss\": validation_cel\n",
    "            }\n",
    "        }\n",
    "        return artifacts\n",
    "\n",
    "def update_artifact_loss(artifacts, training_stage, metric, epoch, batch, value):\n",
    "    try:\n",
    "        artifacts[training_stage][metric][epoch - 1][batch] = value\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "        print(\"epoch\", epoch)\n",
    "        print(\"batch\", batch)\n",
    "        print(artifacts)\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "    flat_loss = artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "    count = flat_loss.size(0)\n",
    "    batch_number = np.arange(0, flat_loss.size(0))\n",
    "    plt.plot(batch_number, flat_loss)\n",
    "    plt.legend(\"CrossEntropyLoss\")\n",
    "    None\n",
    "\n",
    "# artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 1, 0.5)\n",
    "# update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 2, 3)\n",
    "# # artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "# visualize_artifacts(artifacts)\n",
    "# # visualize_artifacts(artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only transformer implementation\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder, LayerNorm\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Decoder Only implmentation without memory for encoder\n",
    "# Adapted from pytorch implmentation @ https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model) # skip\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout) # skip\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(CustomTransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "        #                            key_padding_mask=memory_key_padding_mask)[0]\n",
    "        # tgt = tgt + self.dropout2(tgt2)\n",
    "        # tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "# decoder only implmentation\n",
    "# pytorch implmentation for torch ligthning\n",
    "# class Transformer(pl.LightningModule):\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, ntokens, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", custom_encoder=None, custom_decoder=None):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        # model vars\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # decoder setup \n",
    "        decoder_layer = CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # embedding setup\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.to_embedding = nn.Embedding(ntokens, d_model)\n",
    "\n",
    "        # output setup\n",
    "        self.linear = nn.Linear(d_model, ntokens)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "   \n",
    "        # convert input/targets to embeddings\n",
    "        tgt = self.to_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # add positional encodings\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # pytorch checks\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
    "        if  tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of tgt must be equal to d_model\")\n",
    "        \n",
    "        # decoder pass\n",
    "        output = self.decoder(tgt, memory=None, tgt_mask=tgt_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # return after linear layer\n",
    "        return self.linear(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000015s)\n",
      "Tokenized and Split Data (0.695258s)\n",
      "[<torchtext.data.example.Example object at 0x7f47e95389a0>]\n",
      "Built Vocab (0.876123s)\n",
      "[End Load Data] (14.028440s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DecoderOnlyTransformer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-27740f0a4e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderOnlyTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_decoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mff_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecoderOnlyTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# constants/enums\n",
    "class Dataset(enum.Enum):\n",
    "    PennTreebank = 0,\n",
    "    WikiText2 = 1,\n",
    "    WikiText103 = 2\n",
    "\n",
    "class LanguageTask(enum.Enum):\n",
    "    CausalLanuageModeling = 0,\n",
    "    MaskedLanuageModeling = 1\n",
    "\n",
    "class Segmentation(enum.Enum):\n",
    "    Word = 0,\n",
    "    Subword = 1\n",
    "    Character = 2\n",
    "    BPE = 3\n",
    "    BBPE = 4\n",
    "    BYTE = 5\n",
    "\n",
    "# configure model\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank,\n",
    "    \"segmentation\": Segmentation.Word,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "# validate \n",
    "validate_config(config)\n",
    "\n",
    "# extract config vars\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size = extract_config(config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\")\n",
    "\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = DecoderOnlyTransformer(ntokens, d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers, num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "# model = Transformer(embedding_dimension).to(device)\n",
    "\n",
    "\n",
    "# training w/ lightning\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:27y73vde) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48049<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ebf6af2b10b42559b90fdd078480f5d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210128_233803-27y73vde/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210128_233803-27y73vde/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">fluent-donkey-1</strong>: <a href=\"https://wandb.ai/skgbafa/words2btyes/runs/27y73vde\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes/runs/27y73vde</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:27y73vde). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">solar-waterfall-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/words2btyes\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes</a><br/>\n                Run page: <a href=\"https://wandb.ai/skgbafa/words2btyes/runs/2oosbszm\" target=\"_blank\">https://wandb.ai/skgbafa/words2btyes/runs/2oosbszm</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210128_234325-2oosbszm</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"words2btyes\")\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 5.0 # learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, config, epoch, artifacts):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(max_seq_len, train_data_batches, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != max_seq_len:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        # print(data.dtype)\n",
    "        # output = model(data, targets)\n",
    "        reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "        targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "        output = model(data, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask)\n",
    "        # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "        output.view(-1, ntokens)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data_batches) // max_seq_len, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_source, config):\n",
    "    max_seq_len = extract_config(config, \"max_seq_len\")\n",
    "    \n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(max_seq_len, data_source, i)\n",
    "            \n",
    "            # print(data)\n",
    "            # print(targets)\n",
    "            if data.size(0) != max_seq_len:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            # output = model(data, targets)\n",
    "            reshape_seq_len = min(data.size(0), max_seq_len)\n",
    "            targets_flat = targets.reshape(reshape_seq_len, targets.size(0)//reshape_seq_len)\n",
    "            output = model(data, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "            # output = model(data, targets_flat, src_mask, src_mask)\n",
    "\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            loss = criterion(output_flat, targets)\n",
    "            # update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', epoch, batch, loss.item())\n",
    "            total_loss += len(data) * loss.item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Key 'n_epochs' not in config",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1aea59027ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m# The number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitalize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e9ac728853ac>\u001b[0m in \u001b[0;36minitalize_artifacts\u001b[0;34m(config, train_data_batches, val_data_batches)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate/visualize artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitalize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtraining_cel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_cel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8d116520ffe>\u001b[0m in \u001b[0;36mextract_config\u001b[0;34m(config, *argv)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfig_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Key '{key}' not in config\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mconfig_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key 'n_epochs' not in config"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, criterion, config, epoch, artifacts)\n",
    "    val_loss = evaluate(model, val_data_batches, config)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "visualize_artifacts(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'visualize_artifacts' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f6706569f4d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_artifacts' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_artifacts(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  5.51 | test ppl   246.15\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data_batches, config)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecoderOnlyTransformer(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): CustomTransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (to_embedding): Embedding(28783, 200)\n",
       "  (linear): Linear(in_features=200, out_features=28783, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate experiment configs\n",
    "n_attention_heads_range = range(2,6)\n",
    "n_layers_range = range(2,6)\n",
    "experiment_datasets = [ Dataset.PennTreebank, Dataset.WikiText2, Dataset.WikiText103 ]\n",
    "max_seq_len_range = range()\n",
    "embedding_dimension\n",
    "# datasets\n",
    "def generateExperiements():\n",
    "    # for each dataset\n",
    "        # \n",
    "    config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 2,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"n_epochs\": 3\n",
    "    }\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch lightning experimentation\n",
    "train_dataset, val_dataset, test_dataset, field_processor = load_data_pl(config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling laws plots\n",
    "    # map config values to scaling laws (model size, compute, dataset size)\n",
    "\n",
    "# scaling laws goals\n",
    "    # predict test loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention in encoder and decoder layers\n",
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb sweep\n",
    "# https://docs.wandb.ai/sweeps/python-api\n",
    "\n",
    "WANDB_ENTITY = \"\"\n",
    "WANDB_PROJECT = \"\"\n",
    "\n",
    "sweep_config = {\n",
    "  \"name\": \"My Sweep\",\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"param1\": {\n",
    "            \"values\": [1, 2, 3]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run = wandb.init()\n",
    "    print(\"config:\", dict(run.config))\n",
    "    for epoch in range(35):\n",
    "        print(\"running\", epoch)\n",
    "        wandb.log({\"metric\": run.config.param1, \"epoch\": epoch})\n",
    "        time.sleep(1)\n",
    "\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ]
}