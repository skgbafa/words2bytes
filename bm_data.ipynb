{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "# pylint: disable=\n",
    "\"\"\"WikiText wordpiece tokenized corpora.\"\"\"\n",
    "\n",
    "__all__ = ['WikiText2WordPiece', 'WikiText103WordPiece']\n",
    "\n",
    "import os\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon.data import SimpleDataset\n",
    "from gluonnlp.data import register, corpora\n",
    "from gluonnlp.data.utils import _get_home_dir\n",
    "from gluonnlp import _constants as C\n",
    "\n",
    "\n",
    "class TransformedCorpusBatchify(object):\n",
    "    \"\"\"Batchify the transformed dataset into N independent sequences, where N is the batch size.\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The number of samples in each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"Batchify a dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : mxnet.gluon.data.Dataset\n",
    "            A flat dataset to be batchified.\n",
    "        Returns\n",
    "        -------\n",
    "        mxnet.gluon.data.Dataset\n",
    "            NDArray of shape (len(data) // N, N) where N is the batch_size\n",
    "            wrapped by a mxnet.gluon.data.SimpleDataset. Excessive tokens that\n",
    "            don't align along the batches are discarded.\n",
    "        \"\"\"\n",
    "        batch_num = len(data) // self._batch_size\n",
    "        return SimpleDataset(\n",
    "            mx.nd.array(\n",
    "                data[:batch_num * self._batch_size]).reshape(\n",
    "                    self._batch_size, -1).T)\n",
    "\n",
    "\n",
    "def int_transformed_whitespace_splitter(s):\n",
    "    \"\"\"Split a string at whitespace (space, tab, newline, return, formfeed).\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        The string to be split\n",
    "    Returns\n",
    "    --------\n",
    "    List[int]\n",
    "        List of int. Obtained by calling s.split().\n",
    "    \"\"\"\n",
    "    ids = [int(t) for t in s.split()]\n",
    "    return ids\n",
    "\n",
    "\n",
    "@register(segment=['train', 'val', 'test'])\n",
    "class WikiText2WordPiece(corpora.wikitext._WikiText):\n",
    "\n",
    "    \"\"\"WikiText-2 wordpiece tokenized dataset for language modeling.\n",
    "    WikiText2WordPiece is implemented as CorpusDataset with the default flatten=True.\n",
    "    From\n",
    "    https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "    License: Creative Commons Attribution-ShareAlike\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : {'train', 'val', 'test'}, default 'train'\n",
    "        Dataset segment.\n",
    "    flatten : bool, default True\n",
    "        Whether to return all samples as flattened tokens. If True, each sample is a token.\n",
    "    skip_empty : bool, default True\n",
    "        Whether to skip the empty samples produced from sample_splitters. If False, `bos` and `eos`\n",
    "        will be added in empty samples.\n",
    "    tokenizer : function, default str.split\n",
    "        A function that splits each sample string into list of tokens.\n",
    "    bos : str or None, default None\n",
    "        The token to add at the beginning of each sentence. If None, nothing is added.\n",
    "    eos : str or None, default '<eos>'\n",
    "        The token to add at the end of each sentence. If None, nothing is added.\n",
    "    root : str, default '$MXNET_HOME/datasets/wikitext-2'\n",
    "        Path to temp folder for storing data.\n",
    "        MXNET_HOME defaults to '~/.mxnet'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 segment='train',\n",
    "                 flatten=True,\n",
    "                 skip_empty=True,\n",
    "                 tokenizer=int_transformed_whitespace_splitter,\n",
    "                 bos=None,\n",
    "                 eos=C.EOS_TOKEN,\n",
    "                 root=os.path.join(_get_home_dir(), 'datasets', 'wikitext-2'),\n",
    "                 **kwargs):\n",
    "        self._archive_file = ('wikitext-2-wp.zip',\n",
    "                              '1cd397cbc946c5f5e94c1a4b3ed051134b82bfa7')\n",
    "        self._data_file = {\n",
    "            'train': ('wiki.train.wp.tokens',\n",
    "                      '3aa7b785ce78ac5f9506cfed0623d2c7c635aa64'),\n",
    "            'val': ('wiki.valid.wp.tokens',\n",
    "                    '281bdcda51c35d5952fee7c538b7f1502efcaad1'),\n",
    "            'test': ('wiki.test.wp.tokens',\n",
    "                     'a25072abd6a69d256a57f7af5e434f0d2d091dd5')\n",
    "        }\n",
    "        super(WikiText2WordPiece, self).__init__(\n",
    "            'wikitext-2',\n",
    "            segment=segment,\n",
    "            bos=bos,\n",
    "            eos=eos,\n",
    "            flatten=flatten,\n",
    "            skip_empty=skip_empty,\n",
    "            root=root,\n",
    "            tokenizer=tokenizer,\n",
    "            **kwargs)\n",
    "\n",
    "\n",
    "@register(segment=['train', 'val', 'test'])\n",
    "class WikiText103WordPiece(corpora.wikitext._WikiText):\n",
    "    \"\"\"WikiText-103 wordpiece tokenized dataset for language modeling.\n",
    "    WikiText103WordPiece is implemented as CorpusDataset with the default flatten=True.\n",
    "    From\n",
    "    https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "    License: Creative Commons Attribution-ShareAlike\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : {'train', 'val', 'test'}, default 'train'\n",
    "        Dataset segment.\n",
    "    flatten : bool, default True\n",
    "        Whether to return all samples as flattened tokens. If True, each sample is a token.\n",
    "    skip_empty : bool, default True\n",
    "        Whether to skip the empty samples produced from sample_splitters. If False, `bos` and `eos`\n",
    "        will be added in empty samples.\n",
    "    tokenizer : function, default str.split\n",
    "        A function that splits each sample string into list of tokens.\n",
    "    bos : str or None, default None\n",
    "        The token to add at the beginning of each sentence. If None, nothing is added.\n",
    "    eos : str or None, default '<eos>'\n",
    "        The token to add at the end of each sentence. If None, nothing is added.\n",
    "    root : str, default '$MXNET_HOME/datasets/wikitext-103'\n",
    "        Path to temp folder for storing data.\n",
    "        MXNET_HOME defaults to '~/.mxnet'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 segment='train',\n",
    "                 flatten=True,\n",
    "                 skip_empty=True,\n",
    "                 tokenizer=int_transformed_whitespace_splitter,\n",
    "                 bos=None,\n",
    "                 eos=C.EOS_TOKEN,\n",
    "                 root=os.path.join(_get_home_dir(), 'datasets',\n",
    "                                   'wikitext-103'),\n",
    "                 **kwargs):\n",
    "        self._archive_file = ('wikitext-103-wp.zip',\n",
    "                              'c9e52bca45ca88e01b84e258251eec2be707186a')\n",
    "        self._data_file = {\n",
    "            'train': ('wiki.train.wp.tokens',\n",
    "                      'a3def8aed2d6152209b73fbf21e6cbb9def23ab6'),\n",
    "            'val': ('wiki.valid.wp.tokens',\n",
    "                    '5b730ac5ef35a1c3e6490ae196a2bf597acbffda'),\n",
    "            'test': ('wiki.test.wp.tokens',\n",
    "                     'c250f55bc98ff7b30323c2b93da5f7aaa04fb4ef')\n",
    "        }\n",
    "        super(WikiText103WordPiece, self).__init__(\n",
    "            'wikitext-103',\n",
    "            segment=segment,\n",
    "            bos=bos,\n",
    "            eos=eos,\n",
    "            flatten=flatten,\n",
    "            skip_empty=skip_empty,\n",
    "            root=root,\n",
    "            tokenizer=tokenizer,\n",
    "            **kwargs)"
   ]
  }
 ]
}