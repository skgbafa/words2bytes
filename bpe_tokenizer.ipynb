{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tokenizer_en.pad_token = tokenizer_en.eos_token\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt_vocab_size = tokenizer_en.vocab_size\n",
    "ByteLevelBPE_tokenizer_pt_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['./.data/wikitext-2/wikitext-2/wiki.train.tokens', './.data/wikitext-2/wikitext-2/wiki.valid.tokens', './.data/wikitext-2/wikitext-2/wiki.test.tokens']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "ByteLevelBPE_tokenizer_pt = ByteLevelBPETokenizer()\n",
    "\n",
    "data_path = './.data/'\n",
    "wikitext2 = 'wikitext-2/wikitext-2/'\n",
    "output_location = 'tokenizer/'\n",
    "vocab_size=40000\n",
    "paths = list(map(lambda x: str(data_path+wikitext2+x), ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']))\n",
    "print(paths)\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt.train(files=paths, \n",
    "                                vocab_size=vocab_size, \n",
    "                                min_frequency=2, \n",
    "                                special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt.enable_truncation(max_length=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./.data/tokenizer/BBPE_tokenizer_40000/\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./.data/tokenizer/BBPE_tokenizer_40000/vocab.json',\n",
       " './.data/tokenizer/BBPE_tokenizer_40000/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "import os\n",
    "# isdir = os.path.isdir(path)  \n",
    "\n",
    "bpe_tokenizer_loc = 'BBPE_tokenizer_' + str(vocab_size)\n",
    "path_to_bpe_tokenizer_loc = data_path+output_location+bpe_tokenizer_loc+ '/'\n",
    "print(path_to_bpe_tokenizer_loc)\n",
    "if not os.path.isdir(path_to_bpe_tokenizer_loc):\n",
    "    os.makedirs(path_to_bpe_tokenizer_loc)\n",
    "ByteLevelBPE_tokenizer_pt.save_model(str(path_to_bpe_tokenizer_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "ByteLevelBPE_tokenizer_pt.encode(\"This is a test of the tokenizer\").ids\n",
    "# ByteLevelBPE_tokenizer_pt.get_vocab()\n",
    "ByteLevelBPE_tokenizer_pt_vocab = ByteLevelBPE_tokenizer_pt.get_vocab() \n",
    "\n",
    "ByteLevelBPE_tokenizer_pt_vocab_ls = [k for k, v in sorted(ByteLevelBPE_tokenizer_pt_vocab.items(), key=lambda item: item[1])]\n",
    "len(ByteLevelBPE_tokenizer_pt_vocab_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./.data/wikitext-2/wikitext-2/wiki.train.tokens',\n",
       " './.data/wikitext-2/wikitext-2/wiki.valid.tokens',\n",
       " './.data/wikitext-2/wikitext-2/wiki.test.tokens']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbpe_tokenizer(config):\n",
    "    # prep data\n",
    "    dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n",
    "    data_path = './.data/'\n",
    "    location = file_data[dataset]['location']\n",
    "    paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "    print(paths)\n",
    "\n",
    "    # train tokenixer\n",
    "    ByteLevelBPE_tokenizer_pt.train(files=paths, \n",
    "                                    vocab_size=vocab_size, \n",
    "                                    min_frequency=2, \n",
    "                                    special_tokens=[\"<|endoftext|>\"])\n",
    "    ByteLevelBPE_tokenizer_pt.enable_truncation(max_length=1024)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        output_location = 'tokenizer/'\n",
    "        bpe_tokenizer_loc = 'BBPE_tokenizer_' + str(vocab_size)\n",
    "        path_to_bpe_tokenizer_loc = data_path+output_location+bpe_tokenizer_loc+ '/'\n",
    "        if not os.path.isdir(path_to_bpe_tokenizer_loc):\n",
    "            os.makedirs(path_to_bpe_tokenizer_loc)\n",
    "        ByteLevelBPE_tokenizer_pt.save_model(str(path_to_bpe_tokenizer_loc))\n",
    "        except Exception as e:\n",
    "            print(\"Error saving tokenizer\", e)\n",
    "    return ByteLevelBPE_tokenizer_pt\n",
    "\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from utils import *\n",
    "file_data = {\n",
    "   'PennTreebank': {\n",
    "       \"location\": \"penn-treebank/\",\n",
    "       \"filenames\": ['ptb.train.tokens', 'ptb.valid.tokens', 'ptb.test.tokens']\n",
    "   },\n",
    "   'WikiText2': {\n",
    "       \"location\": \"wikitext-2/wikitext-2/\",\n",
    "       \"filenames\": ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']\n",
    "   } ,\n",
    "   'WikiText103': {\n",
    "       \"location\": \"wikitext-103/wikitext-103/\",\n",
    "       \"filenames\": ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']\n",
    "   }  \n",
    "}\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.WikiText2.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hel', 'lo', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '?']\n[2966, 410, 17, 93, 12, 374, 6, 3193, 378, 1541, 36]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hel lo , y ' all ! How are you ?\""
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "# location = file_data[dataset]['location']\n",
    "# paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "# print(paths)\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# tokenizer.train(files=paths, trainer=trainer)\n",
    "# output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "# print(output.tokens)\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n",
    "\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = file_data[dataset]['location']\n",
    "    paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\"])\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = create_subword_tokenizer(config)\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "print(output.tokens)\n",
    "len(tokenizer.get_vocab())\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.WikiText2.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}