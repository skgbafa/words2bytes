{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tokenizer_en.pad_token = tokenizer_en.eos_token\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt_vocab_size = tokenizer_en.vocab_size\n",
    "ByteLevelBPE_tokenizer_pt_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['./.data/wikitext-2/wikitext-2/wiki.train.tokens', './.data/wikitext-2/wikitext-2/wiki.valid.tokens', './.data/wikitext-2/wikitext-2/wiki.test.tokens']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "ByteLevelBPE_tokenizer_pt = ByteLevelBPETokenizer()\n",
    "\n",
    "data_path = './.data/'\n",
    "wikitext2 = 'wikitext-2/wikitext-2/'\n",
    "output_location = 'tokenizer/'\n",
    "vocab_size=40000\n",
    "paths = list(map(lambda x: str(data_path+wikitext2+x), ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']))\n",
    "print(paths)\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt.train(files=paths, \n",
    "                                vocab_size=vocab_size, \n",
    "                                min_frequency=2, \n",
    "                                special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "ByteLevelBPE_tokenizer_pt.enable_truncation(max_length=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./.data/tokenizer/BBPE_tokenizer_40000/\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./.data/tokenizer/BBPE_tokenizer_40000/vocab.json',\n",
       " './.data/tokenizer/BBPE_tokenizer_40000/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import os\n",
    "# isdir = os.path.isdir(path)  \n",
    "\n",
    "bpe_tokenizer_loc = 'BBPE_tokenizer_' + str(vocab_size)\n",
    "path_to_bpe_tokenizer_loc = data_path+output_location+bpe_tokenizer_loc+ '/'\n",
    "print(path_to_bpe_tokenizer_loc)\n",
    "if not os.path.isdir(path_to_bpe_tokenizer_loc):\n",
    "    os.makedirs(path_to_bpe_tokenizer_loc)\n",
    "ByteLevelBPE_tokenizer_pt.save_model(str(path_to_bpe_tokenizer_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "ByteLevelBPE_tokenizer_pt.encode(\"This is a test of the tokenizer\").ids\n",
    "# ByteLevelBPE_tokenizer_pt.get_vocab()\n",
    "ByteLevelBPE_tokenizer_pt_vocab = ByteLevelBPE_tokenizer_pt.get_vocab() \n",
    "\n",
    "ByteLevelBPE_tokenizer_pt_vocab_ls = [k for k, v in sorted(ByteLevelBPE_tokenizer_pt_vocab.items(), key=lambda item: item[1])]\n",
    "len(ByteLevelBPE_tokenizer_pt_vocab_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'el', 'lo', 'y', \"'\", 'all', 'ow', 'are', 'you']\n[36, 366, 103, 52, 9, 173, 5140, 99, 527]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"i el lo y ' all ow are you\""
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from tokenizers import CharBPETokenizer\n",
    "tokenizer = CharBPETokenizer()\n",
    "\n",
    "\n",
    "from constants import *\n",
    "from utils import *\n",
    "\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Subword.name,\n",
    "    \"vocab_size\": 40000,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\")\n",
    "    \n",
    "    # get location\n",
    "    output_location = 'tokenizer/'\n",
    "    tokenizer_loc = 'bpe_tokenizer_' + str(dataset) + '_'+ str(vocab_size) + \".tokenizer.json\"\n",
    "    path_to_tokenizer_loc = DATA_PATH+output_location\n",
    "    tokenizer_filepath = path_to_tokenizer_loc+tokenizer_loc\n",
    "\n",
    "\n",
    "    # load tokenizer\n",
    "    if os.path.isfile(tokenizer_filepath):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_filepath)\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\"])\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        if not os.path.isdir(path_to_tokenizer_loc):\n",
    "            os.makedirs(path_to_tokenizer_loc)\n",
    "        tokenizer.save(str(path_to_tokenizer_loc+tokenizer_loc))\n",
    "    except Exception as e:\n",
    "            print(\"Error saving tokenizer\", e)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = create_subword_tokenizer(config)\n",
    "\n",
    "output = tokenizer.encode(\"Hi hi[[s, y'all! How are you üòÅ ?\")\n",
    "print(output.tokens)\n",
    "len(tokenizer.get_vocab())\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbpe_tokenizer(config):\n",
    "    # prep data\n",
    "    dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n",
    "    data_path = './.data/'\n",
    "    location = file_data[dataset]['location']\n",
    "    paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "    print(paths)\n",
    "\n",
    "    # train tokenixer\n",
    "    ByteLevelBPE_tokenizer_pt.train(files=paths, \n",
    "                                    vocab_size=vocab_size, \n",
    "                                    min_frequency=2, \n",
    "                                    special_tokens=[\"<|endoftext|>\"])\n",
    "    ByteLevelBPE_tokenizer_pt.enable_truncation(max_length=1024)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        output_location = 'tokenizer/'\n",
    "        bpe_tokenizer_loc = 'BBPE_tokenizer_' + str(vocab_size)\n",
    "        path_to_bpe_tokenizer_loc = data_path+output_location+bpe_tokenizer_loc+ '/'\n",
    "        if not os.path.isdir(path_to_bpe_tokenizer_loc):\n",
    "            os.makedirs(path_to_bpe_tokenizer_loc)\n",
    "        ByteLevelBPE_tokenizer_pt.save_model(str(path_to_bpe_tokenizer_loc))\n",
    "        except Exception as e:\n",
    "            print(\"Error saving tokenizer\", e)\n",
    "    return ByteLevelBPE_tokenizer_pt\n",
    "\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from utils import *\n",
    "file_data = {\n",
    "   'PennTreebank': {\n",
    "       \"location\": \"penn-treebank/\",\n",
    "       \"filenames\": ['ptb.train.tokens', 'ptb.valid.tokens', 'ptb.test.tokens']\n",
    "   },\n",
    "   'WikiText2': {\n",
    "       \"location\": \"wikitext-2/wikitext-2/\",\n",
    "       \"filenames\": ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']\n",
    "   } ,\n",
    "   'WikiText103': {\n",
    "       \"location\": \"wikitext-103/wikitext-103/\",\n",
    "       \"filenames\": ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']\n",
    "   }  \n",
    "}\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.WikiText2.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hel', 'lo', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '?']\n[2966, 410, 17, 93, 12, 374, 6, 3193, 378, 1541, 36]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hel lo , y ' all ! How are you ?\""
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "# location = file_data[dataset]['location']\n",
    "# paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "# print(paths)\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# tokenizer.train(files=paths, trainer=trainer)\n",
    "# output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "# print(output.tokens)\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size, max_seq_len = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"max_seq_len\")\n",
    "\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = file_data[dataset]['location']\n",
    "    paths = list(map(lambda x: str(data_path+location+x), file_data[dataset]['filenames']))\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\"])\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = create_subword_tokenizer(config)\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.tokens)\n",
    "len(tokenizer.get_vocab())\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.WikiText2.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 35,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# from transformers import GPT2TokenizerFast\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from utils import extract_config\n",
    "from constants import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000123s)\n",
      "Tokenized and Split Data (0.343220s)\n",
      "[End Load Data] (40.578641s)\n",
      "tensor([   33,    73,   687,    35,    34,  8787,  9549,  2448,    34,    33,\n",
      "           73,   687,    35,    33,    73,   687,    35, 34658,   529,  8787]) data\n",
      "tensor([   73,   687,    35,    34,  8787,  9549,  2448,    34,    33,    73,\n",
      "          687,    35,    33,    73,   687,    35, 34658,   529,  8787,    24]) targetes\n",
      "< e os > = Valkyria Chronicles III = < e os > < e os > Senj≈ç no Valkyria\n",
      "e os > = Valkyria Chronicles III = < e os > < e os > Senj≈ç no Valkyria 3\n",
      "tensor([   33,    73,   687,    35,    34,  8787,  9549,  2448,    34,    33,\n",
      "           73,   687,    35,    33,    73,   687,    35, 34658,   529,  8787]) batchify1\n",
      "tensor([   33,    73,   687,    35,    34,  8787,  9549,  2448,    34,    33,\n",
      "           73,   687,    35,    33,    73,   687,    35, 34658,   529,  8787,\n",
      "           24,    31,     5,  9549,    13,  1731,    31,   277,   275,   261,\n",
      "          272,   262,   271,   264,   269]) batchify2\n",
      "data torch.Size([20, 35])\n",
      "targets torch.Size([700])\n",
      "tensor([   33,    73,   687,    35,    34,  8787,  9549,  2448,    34,    33,\n",
      "           73,   687,    35,    33,    73,   687,    35, 34658,   529,  8787,\n",
      "           24,    31,     5,  9549,    13,  1731,    31,   277,   275,   261,\n",
      "          272,   262,   271,   264,   269])\n",
      "tensor([   73,   687,    35,    34,  8787,  9549,  2448,    34,    33,    73,\n",
      "          687,    35,    33,    73,   687,    35, 34658,   529,  8787,    24])\n",
      "< e os > = Valkyria Chronicles III = < e os > < e os > Senj≈ç no Valkyria 3 : Chronicles ( Japanese : Êà¶ Â†¥ „ÅÆ „É¥ „Ç° „É´ „Ç≠ „É•\n",
      "e os > = Valkyria Chronicles III = < e os > < e os > Senj≈ç no Valkyria 3\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.WikiText2.name,\n",
    "    \"segmentation\": Segmentation.Subword.name,\n",
    "    \"vocab_size\": 40000,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\")\n",
    "    \n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\"])\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "class TextDataloader:\n",
    "    def __init__(self, dataset, tokenizer, max_seq_len, batch_size):\n",
    "        self.dataset = self.prep_data(dataset, tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_len = len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        i = self.index\n",
    "\n",
    "        seq_len = min(self.max_seq_len, self.dataset_len - 1 - i)\n",
    "        chunk_len = seq_len * self.batch_size\n",
    "        data = self.dataset[i:i+ chunk_len]\n",
    "        target = self.dataset[i+1:i+1+chunk_len].reshape(-1)\n",
    "\n",
    "        self.index += 1\n",
    "        data = self.batchify(data)\n",
    "        return data, target\n",
    "\n",
    "    def prep_data(self, dataset, tokenizer):\n",
    "        raw_text_iter = dataset[0].text\n",
    "        data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "                                dtype=torch.long) for item in raw_text_iter]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    def batchify(self, data):\n",
    "        # Divide the dataset into batch_size parts.\n",
    "        nbatch = data.size(0) // self.batch_size\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * self.batch_size)\n",
    "        print(data[0:20], \"batchify1\")\n",
    "        # Evenly divide the data across the batch_size batches.\n",
    "        data = data.view(self.batch_size, -1).contiguous()\n",
    "        print(data[0], \"batchify2\")\n",
    "        return data\n",
    "    \n",
    "print(\"[Start Load Data]\")\n",
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "    config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "dataset = getattr(datasets, dataset)\n",
    "print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "    text_field=Field())\n",
    "print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# tokenize\n",
    "if segmentation == Segmentation.Subword.name:\n",
    "    tokenizer = create_subword_tokenizer(config)\n",
    "elif segmentation == Segmentation.BBPE.name:\n",
    "    tokenizer = create_bbpe_tokenizer(config)\n",
    "\n",
    "# get vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# # data prep\n",
    "# def data_prep(tt_dataset_split):\n",
    "#     raw_text_iter = tt_dataset_split[0].text\n",
    "#     data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "#                             dtype=torch.long) for item in raw_text_iter]\n",
    "#     data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "#     # TESTING, trim data\n",
    "#     data = data[0:batch_size*64]\n",
    "#     print(data.shape)\n",
    "#     # Divide the dataset into bsz parts.\n",
    "#     nbatch = data.size(0) // batch_size\n",
    "#     # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#     data = data.narrow(0, 0, nbatch * batch_size)\n",
    "#     # Evenly divide the data across the batch_size batches.\n",
    "#     data = data.view(batch_size, -1).t().contiguous()\n",
    "#     return data\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = TextDataloader(train_dataset, tokenizer, max_seq_len, batch_size)\n",
    "# val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len)\n",
    "# test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len)\n",
    "\n",
    "print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "# return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "for batch in train_dataloader:\n",
    "        data, targets = batch\n",
    "        print(\"data\", data.shape)\n",
    "        print(\"targets\", targets.shape)\n",
    "        break\n",
    "\n",
    "print(data[0])\n",
    "print(targets[0:20])\n",
    "print(tokenizer.decode(data[0].tolist()))\n",
    "print(tokenizer.decode(targets[0:20].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}