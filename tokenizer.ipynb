{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000007s)\n",
      "Tokenized and Split Data (0.017809s)\n",
      "[End Load Data] (4.230067s)\n",
      "['he', 'funding', 'is', 'attached', 'to', 'an', 'estimated', '$', '2', '7', '.', '1', 'billion', 'transportation', 'bill', 'that', 'goes', 'next', 'to', 'the', 'en', 'ate', 'and', 'carries', 'with', 'it', 'a', 'proposed', 'permanent', 'smoking', 'ban', 'on', 'virtually', 'all', '.', '.', 'domestic', 'airline', 'flights', '.']\n",
      "[132, 2481, 78, 5682, 72, 58, 1390, 8, 17, 22, 13, 16, 272, 2045, 683, 103, 2921, 613, 72, 57, 67, 138, 80, 4758, 144, 73, 29, 1462, 4910, 6042, 251, 61, 3721, 174, 13, 13, 1769, 1966, 4160, 13]\n",
      "he funding is attached to an estimated $ 2 7 . 1 billion transportation bill that goes next to the en ate and carries with it a proposed permanent smoking ban on virtually all . . domestic airline flights .\n",
      "[10, 47, 1025, 12, 1267, 558, 1372, 56, 57, 27, 582, 695, 56, 1108, 6083, 80, 7064, 71, 1511, 12, 2314, 1336, 589, 27, 27, 483, 1025, 921, 984, 5, 145, 27]\n",
      "[47, 1025, 12, 1267, 558, 1372, 56, 57, 27, 582, 695, 56, 1108, 6083, 80, 7064, 71, 1511, 12, 2314, 1336, 589, 27, 27, 483, 1025, 921, 984, 5, 145, 27, 27]\n",
      "' s health - care cost problem in the N months ended in september wages and salaries of private - sector workers rose N N while health insurance costs by N\n",
      "s health - care cost problem in the N months ended in september wages and salaries of private - sector workers rose N N while health insurance costs by N N\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Subword.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\",\n",
    "        \"torchtext_split\": False,\n",
    "    }\n",
    "\n",
    "def split_dataset(config):\n",
    "    dataset  = extract_config(config, \"dataset\")\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "\n",
    "    # train data\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "    for path in paths:\n",
    "        raw_data = list(open(path, newline='\\n'))\n",
    "        raw_data = list(filter(lambda x: x != '\\n', raw_data))\n",
    "        if re.search(\"train\", path):\n",
    "           train_data = raw_data\n",
    "        if re.search(\"valid\", path):\n",
    "           valid_data = raw_data\n",
    "        if re.search(\"test\", path):\n",
    "           test_data = raw_data\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size, segmentation = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"segmentation\")\n",
    "\n",
    "    # get location\n",
    "    output_location = 'tokenizer/'\n",
    "    tokenizer_loc = segmentation +'_tokenizer_' + str(dataset) + '_' + str(vocab_size) + \".tokenizer.json\"\n",
    "    path_to_tokenizer_loc = DATA_PATH+output_location\n",
    "    tokenizer_filepath = path_to_tokenizer_loc+tokenizer_loc\n",
    "\n",
    "    # load tokenizer\n",
    "    if os.path.isfile(tokenizer_filepath):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_filepath)\n",
    "        return tokenizer\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\", \"<eos>\"])\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        if not os.path.isdir(path_to_tokenizer_loc):\n",
    "            os.makedirs(path_to_tokenizer_loc)\n",
    "        tokenizer.save(str(tokenizer_filepath))\n",
    "    except Exception as e:\n",
    "        print(\"Error saving tokenizer\", e)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def load_data_subword(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len, segmentation, torchtext_split = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\", \"torchtext_split\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(config)\n",
    "    if torchtext_split:\n",
    "        train_dataset, val_dataset, test_dataset = tt_dataset.splits(\n",
    "            text_field=Field())\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # tokenize\n",
    "    if segmentation == Segmentation.Subword.name:\n",
    "        tokenizer = create_subword_tokenizer(config)\n",
    "    elif segmentation == Segmentation.BBPE.name:\n",
    "        tokenizer = create_bbpe_tokenizer(config)\n",
    "    elif segmentation == Segmentation.Word.name:\n",
    "        tokenizer = create_word_tokenizer(config)\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    # prep data\n",
    "    def prep_data(dataset_arr):\n",
    "        if torchtext_split:\n",
    "            raw_text_iter = dataset_arr[0].text\n",
    "            data = torch.tensor(tokenizer.encode(raw_text_iter, is_pretokenized=True).ids, dtype=torch.long)\n",
    "            return data\n",
    "\n",
    "        raw_text_iter = dataset_arr\n",
    "        data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "                             dtype=torch.long) for item in raw_text_iter]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(prep_data(train_dataset), max_seq_len, batch_size)\n",
    "    val_dataloader = TextDataloader(prep_data(val_dataset), max_seq_len, batch_size)\n",
    "    test_dataloader = TextDataloader(prep_data(test_dataset), max_seq_len, batch_size)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_subword(config)\n",
    "test_string = \"The funding is attached to an estimated $27.1 billion transportation bill that goes next to the Senate and carries with it a proposed permanent smoking ban on virtually all U.S. domestic airline flights. \"\n",
    "test_tokenizer(tokenizer, test_string)\n",
    "print_examples(train_dataloader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'funding', 'is', 'attached', 'to', 'an', 'estimated', '$', '2', '7', '.', '1', 'billion', 'transportation', 'bill', 'that', 'goes', 'next', 'to', 'the', 'senate', 'and', 'carries', 'with', 'it', 'a', 'proposed', 'permanent', 'smoking', 'ban', 'on', 'virtually', 'all', 'domestic', 'airline', 'flights', '.']\n[57, 2481, 78, 5682, 72, 58, 1390, 8, 17, 22, 13, 16, 272, 2045, 683, 103, 2921, 613, 72, 57, 1288, 80, 4758, 144, 73, 29, 1462, 4910, 6042, 251, 61, 3721, 174, 1769, 1966, 4160, 13]\nthe funding is attached to an estimated $ 2 7 . 1 billion transportation bill that goes next to the senate and carries with it a proposed permanent smoking ban on virtually all domestic airline flights .\n"
     ]
    }
   ],
   "source": [
    "test_string = \"the funding is attached to an estimated $27.1 billion transportation bill that goes next to the senate and carries with it a proposed permanent smoking ban on virtually all domestic airline flights. \"\n",
    "test_tokenizer(tokenizer, test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000006s)\n",
      "Tokenized and Split Data (0.020541s)\n",
      "[End Load Data] (5.117110s)\n",
      "['the', 'Ġfunding', 'Ġis', 'Ġatt', 'ach', 'ed', 'Ġto', 'Ġan', 'Ġestimated', 'Ġ$', '2', '7', '.', '1', 'Ġbillion', 'Ġtransportation', 'Ġbill', 'Ġthat', 'Ġgoes', 'Ġnext', 'Ġto', 'Ġthe', 'Ġsenate', 'Ġand', 'Ġcar', 'r', 'ies', 'Ġwith', 'Ġit', 'Ġa', 'Ġproposed', 'Ġper', 'man', 'ent', 'Ġsm', 'ok', 'ing', 'Ġban', 'Ġon', 'Ġvirt', 'ually', 'Ġall', 'Ġdomestic', 'Ġairline', 'Ġfl', 'ights', '.', 'Ġ']\n",
      "[1850, 3145, 330, 912, 564, 285, 290, 293, 1775, 338, 18, 23, 14, 17, 523, 2440, 957, 323, 3386, 890, 290, 263, 1709, 302, 749, 82, 391, 369, 318, 258, 1817, 626, 524, 307, 889, 569, 292, 2693, 324, 3985, 1160, 549, 2141, 2361, 1002, 1461, 14, 221]\n",
      "the funding is attached to an estimated $27.1 billion transportation bill that goes next to the senate and carries with it a proposed permanent smoking ban on virtually all domestic airline flights. \n",
      "[14, 1303, 313, 75, 277, 492, 514, 274, 1412, 386, 269, 268, 30, 376, 263, 3044, 2423, 271, 318, 386, 2686, 318, 802, 320, 263, 1213, 324, 370, 1396, 315, 83, 2091]\n",
      "[1303, 313, 75, 277, 492, 514, 274, 1412, 386, 269, 268, 30, 376, 263, 3044, 2423, 271, 318, 386, 2686, 318, 802, 320, 263, 1213, 324, 370, 1396, 315, 83, 2091, 271]\n",
      ". engelken says his wife was <unk> by the whole thing \n",
      " it was worth it just for the look on albert 's face\n",
      " engelken says his wife was <unk> by the whole thing \n",
      " it was worth it just for the look on albert 's face \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bbpe\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.BBPE.name,\n",
    "        \"vocab_size\": 10000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\",\n",
    "        \"torchtext_split\": True\n",
    "\n",
    "    }\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_subword(config)\n",
    "test_tokenizer(tokenizer, test_string)\n",
    "print_examples(train_dataloader, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9663\n['the', 'funding', 'is', 'attached', 'to', 'an', 'estimated', '$', '27', '.', '1', 'billion', 'transportation', 'bill', 'that', 'goes', 'next', 'to', 'the', 'senate', 'and', 'carries', 'with', 'it', 'a', 'proposed', 'permanent', 'smoking', 'ban', 'on', 'virtually', 'all', 'domestic', 'airline', 'flights', '.']\n[7, 1219, 23, 3258, 13, 44, 495, 22, 5, 17, 3626, 64, 829, 278, 21, 1366, 149, 13, 7, 428, 16, 2603, 32, 24, 14, 514, 2667, 3535, 1901, 27, 1876, 86, 686, 785, 2205, 17]\nthe funding is attached to an estimated $ . 1 billion transportation bill that goes next to the senate and carries with it a proposed permanent smoking ban on virtually all domestic airline flights .\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "\n",
    "# word tokenizer\n",
    "def create_word_tokenizer(config):\n",
    "    dataset, vocab_size, segmentation = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"segmentation\")\n",
    "\n",
    "    # get location\n",
    "    output_location = 'tokenizer/'\n",
    "    tokenizer_loc = segmentation +'_tokenizer_' + str(dataset) + \".tokenizer.json\"\n",
    "    path_to_tokenizer_loc = DATA_PATH+output_location\n",
    "    tokenizer_filepath = path_to_tokenizer_loc+tokenizer_loc\n",
    "\n",
    "    # load tokenizer\n",
    "    if os.path.isfile(tokenizer_filepath):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_filepath)\n",
    "        return tokenizer\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer(WordLevel())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "  \n",
    "    trainer = WordLevelTrainer(\n",
    "        min_frequency=1,\n",
    "        # vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\", \"<eos>\"]\n",
    "        )\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        if not os.path.isdir(path_to_tokenizer_loc):\n",
    "            os.makedirs(path_to_tokenizer_loc)\n",
    "        tokenizer.save(str(tokenizer_filepath))\n",
    "    except Exception as e:\n",
    "        print(\"Error saving tokenizer\", e)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = create_word_tokenizer(config)\n",
    "test_tokenizer(tokenizer, test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000007s)\n",
      "/home/gbafa/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/gbafa/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "Tokenized and Split Data (0.658534s)\n",
      "Built Vocab (0.836997s)\n",
      "[End Load Data] (13.951997s)\n",
      "<function _basic_english_normalize at 0x7f49ec001820>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'encode'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-45c28d6d31d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtest_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-7932f96c427d>\u001b[0m in \u001b[0;36mtest_tokenizer\u001b[0;34m(tokenizer, test_string)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is a test to see how the encoder is working! Are the results pleasing?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# word\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Word.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "\n",
    "def load_data_word(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len = extract_config(config, \"dataset\", \"batch_size\", \"max_seq_len\")\n",
    "    dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # # tokenize\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "        text_field=field_processor)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(\n",
    "        train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # data prep\n",
    "    def data_prep(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                                dtype=torch.long) for item in raw_text_iter]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(data_prep(train_dataset), max_seq_len, batch_size)\n",
    "    val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len, batch_size)\n",
    "    test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len, batch_size)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_word(config)\n",
    "test_tokenizer(tokenizer)\n",
    "print_examples(train_dataloader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Character.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'ByteLevel' from 'tokenizers.models' (/home/gbafa/miniconda3/envs/transformer/lib/python3.8/site-packages/tokenizers/models/__init__.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f698ee3e7903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordLevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mByteLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBpeTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordLevelTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ByteLevel' from 'tokenizers.models' (/home/gbafa/miniconda3/envs/transformer/lib/python3.8/site-packages/tokenizers/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "# byte\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.BYTE.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "\n",
    "# byte tokenizer\n",
    "def create_word_tokenizer(config):\n",
    "    dataset, vocab_size, segmentation = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\", \"segmentation\")\n",
    "\n",
    "    # get location\n",
    "    output_location = 'tokenizer/'\n",
    "    tokenizer_loc = segmentation +'_tokenizer_' + str(dataset) + '_' + str(vocab_size) + \".tokenizer.json\"\n",
    "    path_to_tokenizer_loc = DATA_PATH+output_location\n",
    "    tokenizer_filepath = path_to_tokenizer_loc+tokenizer_loc\n",
    "\n",
    "    # load tokenizer\n",
    "    # if os.path.isfile(tokenizer_filepath):\n",
    "    #     tokenizer = Tokenizer.from_file(tokenizer_filepath)\n",
    "    #     return tokenizer\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer(WordLevel())\n",
    "    tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "    trainer = WordLevelTrainer(\n",
    "        min_frequency=1,\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<unk>\", \"<eos>\"]\n",
    "        )\n",
    "\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        if not os.path.isdir(path_to_tokenizer_loc):\n",
    "            os.makedirs(path_to_tokenizer_loc)\n",
    "        tokenizer.save(str(tokenizer_filepath))\n",
    "    except Exception as e:\n",
    "        print(\"Error saving tokenizer\", e)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = create_word_tokenizer(config)\n",
    "test_tokenizer(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function torchtext.data.dataset.Dataset.split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "    config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "tt_dataset = getattr(datasets, dataset)\n",
    "tt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_dataset(config):\n",
    "    dataset  = extract_config(config, \"dataset\")\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "\n",
    "    # train data\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "    for path in paths:\n",
    "        raw_data = list(open(path, newline='\\n'))\n",
    "        raw_data = list(filter(lambda x: x != '\\n', raw_data))\n",
    "        if re.search(\"train\", path):\n",
    "           train_data = raw_data\n",
    "        if re.search(\"valid\", path):\n",
    "           valid_data = raw_data\n",
    "        if re.search(\"test\", path):\n",
    "           test_data = raw_data\n",
    "\n",
    "    return train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000007s)\n",
      "Tokenized and Split Data (0.018512s)\n",
      "[End Load Data] (5.125093s)\n"
     ]
    }
   ],
   "source": [
    "def load_data_subword(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    # train_dataset, val_dataset, test_dataset = tt_dataset.splits(\n",
    "    #     text_field=Field())\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(config)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # tokenize\n",
    "    if segmentation == Segmentation.Subword.name:\n",
    "        tokenizer = create_subword_tokenizer(config)\n",
    "    elif segmentation == Segmentation.BBPE.name:\n",
    "        tokenizer = create_bbpe_tokenizer(config)\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    # prep data\n",
    "    def prep_data(dataset_arr):\n",
    "        data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "                             dtype=torch.long) for item in dataset_arr]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(prep_data(train_dataset), max_seq_len, batch_size)\n",
    "    val_dataloader = TextDataloader(prep_data(val_dataset), max_seq_len, batch_size)\n",
    "    test_dataloader = TextDataloader(prep_data(test_dataset), max_seq_len, batch_size)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_subword(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['this', 'is', 'a', 'test', 'to', 'see', 'how', 'the', 'encoder', 'is', 'working', '!', 'Are', 'the', 'results', 'pleasing', '?']\n[52, 23, 14, 946, 13, 367, 273, 7, 5, 23, 703, 5, 5, 7, 362, 5, 5]\nthis is a test to see how the is working the results\n[5966, 3837, 673, 271, 12211, 269, 268, 30, 284, 640, 1384, 302, 1361, 911, 287, 5317, 1491, 4945, 2709, 386, 1900, 258, 14556, 1309, 287, 466, 1331, 1462, 7119, 271, 258, 961]\n[3837, 673, 271, 12211, 269, 268, 30, 284, 640, 1384, 302, 1361, 911, 287, 5317, 1491, 4945, 2709, 386, 1900, 258, 14556, 1309, 287, 466, 1331, 1462, 7119, 271, 258, 961, 287]\nchaos discounts account economic common very as re face modest concern film district small hearst everything reduces massachusetts power accused past positions small demand radio factors diamonds economic past eight\ndiscounts account economic common very as re face modest concern film district small hearst everything reduces massachusetts power accused past positions small demand radio factors diamonds economic past eight small\n"
     ]
    }
   ],
   "source": [
    "def print_examples(dataloader, tokenizer, print_count = 1):\n",
    "    count = 0\n",
    "    for batch in dataloader:\n",
    "        data, targets = batch\n",
    "        d1 = data[0].tolist()\n",
    "        t1 = targets[0:len(data[0])].tolist()\n",
    "        print(d1)\n",
    "        print(t1)\n",
    "        print(tokenizer.decode(d1))\n",
    "        print(tokenizer.decode(t1))\n",
    "        count += 1\n",
    "        if count > print_count - 1:\n",
    "            break\n",
    "\n",
    "def test_tokenizer(tokenizer, test_string = \"this is a test to see how the encoder is working! Are the results pleasing?\"):\n",
    "    output = tokenizer.encode(test_string)\n",
    "    print(output.tokens)\n",
    "    print(output.ids)\n",
    "    print(tokenizer.decode(output.ids))\n",
    "\n",
    "test_tokenizer(tokenizer)\n",
    "print_examples(train_dataloader, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "VersionConflict",
     "evalue": "tokenizers==0.9.4 is required for a normal functioning of this module, but found tokenizers==0.10.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVersionConflict\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-43829d8c2e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from transformers import GPT2TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tokenizer(test_string)['input_ids']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m<frozen zipimport>\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/transformers-4.2.2-py3.8.egg/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m<frozen zipimport>\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/transformers-4.2.2-py3.8.egg/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/transformers-4.2.2-py3.8.egg/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\" require_version wrapper which emits a core-specific hint on failure \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/transformers-4.2.2-py3.8.egg/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# check that the right version is installed if version number was provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         raise pkg_resources.VersionConflict(\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         )\n",
      "\u001b[0;31mVersionConflict\u001b[0m: tokenizers==0.9.4 is required for a normal functioning of this module, but found tokenizers==0.10.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2TokenizerFast\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# tokenizer(test_string)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}