{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data import *\n",
    "\n",
    "config = {\n",
    "        \"embedding_dimension\": 200,\n",
    "        \"ff_dimension\": 200,\n",
    "        \"n_attention_heads\": 2,\n",
    "        \"n_encoder_layers\": 0,\n",
    "        \"n_decoder_layers\": 2,\n",
    "        \"dataset\": Dataset.PennTreebank.name,\n",
    "        \"segmentation\": Segmentation.Subword.name,\n",
    "        \"vocab_size\": 40000,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"batch_size\": 20,\n",
    "        \"eval_batch_size\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_b1\": 0.9,\n",
    "        \"adam_b2\": 0.999,\n",
    "        \"adam_l2_weightdecay\": 0.01,\n",
    "        \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000008s)\n",
      "Tokenized and Split Data (0.014551s)\n",
      "Subword\n",
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      "\n",
      " in early trading in tokyo friday the nikkei index rose N points to N \n",
      "\n",
      " but the company declines to comment \n",
      "\n",
      "[End Load Data] (6.073602s)\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(config):\n",
    "    dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    # print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # # process non-ptb datasets\n",
    "    # if dataset != Dataset.PennTreebank.name:\n",
    "    #     print(dataset)\n",
    "    #     return tt_dataset.splits(text_field=Field())\n",
    "    \n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                        TRAINING_DATA[dataset]['filenames']))\n",
    "    lines = open(paths[0], newline='\\n')\n",
    "    raw_data = map(lambda x: list(open(x, newline='\\n')), paths)\n",
    "    text_data = []\n",
    "    for item in raw_data:\n",
    "        # item = filter(lambda x: x != '\\n', item)\n",
    "        text_data.extend(item)\n",
    "    total_count = len(text_data)\n",
    "\n",
    "    train_count = int(0.7 * total_count) \n",
    "    valid_count = int(0.2 * total_count)\n",
    "    test_count = total_count - train_count - valid_count\n",
    "    return (text_data[0:train_count], text_data[train_count: train_count + valid_count], text_data[train_count + valid_count:len(text_data)])\n",
    "    # return torch.utils.data.random_split(text_data, (train_count, valid_count, test_count))\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = split_dataset(config)\n",
    "\n",
    "def load_data_subword(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(config)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # tokenize\n",
    "    print(segmentation)\n",
    "    if segmentation == Segmentation.Subword.name:\n",
    "        tokenizer = create_subword_tokenizer(config)\n",
    "    elif segmentation == Segmentation.BBPE.name:\n",
    "        tokenizer = create_bbpe_tokenizer(config)\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    # prep data\n",
    "    def prep_data(dataset_arr):\n",
    "        raw_text_iter = dataset_arr\n",
    "        print(dataset_arr[0])\n",
    "        data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "                             dtype=torch.long) for item in raw_text_iter]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(prep_data(train_dataset), max_seq_len, batch_size)\n",
    "    val_dataloader = TextDataloader(prep_data(val_dataset), max_seq_len, batch_size)\n",
    "    test_dataloader = TextDataloader(prep_data(test_dataset), max_seq_len, batch_size)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_subword(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function torchtext.data.dataset.Dataset.split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "    config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "tt_dataset = getattr(datasets, dataset)\n",
    "tt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_dataset(config):\n",
    "    dataset  = extract_config(config, \"dataset\")\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "\n",
    "    # train data\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "    for path in paths:\n",
    "        raw_data = list(open(path, newline='\\n'))\n",
    "        raw_data = list(filter(lambda x: x != '\\n', raw_data))\n",
    "        if re.search(\"train\", path):\n",
    "           train_data = raw_data\n",
    "        if re.search(\"valid\", path):\n",
    "           valid_data = raw_data\n",
    "        if re.search(\"test\", path):\n",
    "           test_data = raw_data\n",
    "\n",
    "    return train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000007s)\n",
      "Tokenized and Split Data (0.018512s)\n",
      "[End Load Data] (5.125093s)\n"
     ]
    }
   ],
   "source": [
    "def load_data_subword(config):\n",
    "    print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # split dataset\n",
    "    # train_dataset, val_dataset, test_dataset = tt_dataset.splits(\n",
    "    #     text_field=Field())\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(config)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # tokenize\n",
    "    if segmentation == Segmentation.Subword.name:\n",
    "        tokenizer = create_subword_tokenizer(config)\n",
    "    elif segmentation == Segmentation.BBPE.name:\n",
    "        tokenizer = create_bbpe_tokenizer(config)\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    # prep data\n",
    "    def prep_data(dataset_arr):\n",
    "        data = [torch.tensor(tokenizer.encode(item).ids,\n",
    "                             dtype=torch.long) for item in dataset_arr]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(prep_data(train_dataset), max_seq_len, batch_size)\n",
    "    val_dataloader = TextDataloader(prep_data(val_dataset), max_seq_len, batch_size)\n",
    "    test_dataloader = TextDataloader(prep_data(test_dataset), max_seq_len, batch_size)\n",
    "\n",
    "    print(f\"[End Load Data] ({time.time() - ts:3f}s)\")\n",
    "    return train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab, tokenizer = load_data_subword(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = split_dataset(config)\n",
    "tokenizer = create_subword_tokenizer(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[63, 94, 394, 2991, 250, 1115, 88, 876, 141, 156, 282, 5252, 260, 83, 139, 82, 208, 2026, 69, 6210, 77, 7546, 3692, 3655, 6051, 17, 2489, 130, 65, 71, 199, 73, 296, 12092, 221, 65, 75, 74, 86, 3404, 70, 64, 4583, 180, 80, 307, 503, 99, 716, 10706, 410, 2186, 5255, 337, 17, 1523, 81, 4036, 801, 280, 2541, 77, 85, 335, 168]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'a er bank note ber lit z call ow ay cent rust cl u et t from stein g itan o gut erman hy dro - que be c i po k ia memo te c m l x na h b pun ts r ake reg at ta rub ens sim sn ack - food s san gy ong swap o w ach ter'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}