{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from constants import *\n",
    "from transformer import DecoderOnlyTransformer\n",
    "\n",
    "benchmark_config_1 = {\n",
    "    \"embedding_dimension\": 768,  # units\n",
    "    \"ff_dimension\": 768,  # hidden_size\n",
    "    \"n_attention_heads\": 12,  # num_heads\n",
    "    \"n_encoder_layers\": 0,  # num_layers\n",
    "    \"n_decoder_layers\": 12,  # num_layers\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Subword.name,\n",
    "    \"vocab_size\": 40000,\n",
    "    \"max_seq_len\": 64,  # max_length\n",
    "    \"dropout\": 0,  # dropout\n",
    "    \"batch_size\": 12,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"n_epochs\": 37,\n",
    "    \"learning_rate\": 0.000034375,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0,\n",
    "    \"gamma\": 0.95,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\",\n",
    "    \"enable_lr_scheduler\": True,\n",
    "    \"T_max\": 5,\n",
    "}\n",
    "ntokens = 40000\n",
    "\n",
    "model = DecoderOnlyTransformer(benchmark_config_1, ntokens, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_config = model.configure_optimizers()\n",
    "print(opt_config)\n",
    "optimizer = opt_config['optimizer']\n",
    "scheduler = opt_config['scheduler']\n",
    "\n",
    "\n",
    "values = []\n",
    "for i in range(steps):\n",
    "    lr_updates = []\n",
    "    # for scheduler in lr_schedulers:\n",
    "    scheduler.optimizer.step()\n",
    "    scheduler.step()\n",
    "    # lr_updates.append(scheduler.optimizer.param_groups[0][\"lr\"])\n",
    "    lr_updates.append(scheduler.get_last_lr()[0])\n",
    "    values.append(lr_updates)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, steps), values)\n",
    "plt.legend(gamma_sweep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "gamma_sweep = [0.8, 0.9]\n",
    "lr_schedulers = []\n",
    "for gamma in gamma_sweep:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model.learning_rate, betas=(\n",
    "            model.adam_b1, model.adam_b2), weight_decay=model.adam_l2_weightdecay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=gamma)\n",
    "    lr_schedulers.append(scheduler)\n",
    "\n",
    "values = []\n",
    "for i in range(steps):\n",
    "    lr_updates = []\n",
    "    for scheduler in lr_schedulers:\n",
    "        scheduler.optimizer.step()\n",
    "        scheduler.step()\n",
    "        # lr_updates.append(scheduler.optimizer.param_groups[0][\"lr\"])\n",
    "        lr_updates.append(scheduler.get_last_lr()[0])\n",
    "    values.append(lr_updates)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, steps), values)\n",
    "plt.legend(gamma_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=model.learning_rate, betas=(\n",
    "#             model.adam_b1, model.adam_b2), weight_decay=model.adam_l2_weightdecay)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=model.gamma)\n",
    "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=model.T_max)\n",
    "# scheduler.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Three settings of the lrate hyperparameters.\n",
    "# # opts = [NoamOpt(512, 1, 4000, None), \n",
    "# #         NoamOpt(512, 1, 8000, None),\n",
    "# #         NoamOpt(256, 1, 4000, None)]\n",
    "\n",
    "# gamma_sweep = [0.8, 0.9]\n",
    "# lr_schedulers = []\n",
    "# for gamma in gamma_sweep:\n",
    "#         print(gamma)\n",
    "\n",
    "\n",
    "# lrs = []\n",
    "# l2 = []\n",
    "# steps = 50000\n",
    "\n",
    "# for i in range(steps):\n",
    "# #     optimizer.step()\n",
    "#     lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "#     l2.append(scheduler.get_last_lr()[0])\n",
    "# #     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n",
    "#     scheduler.step()\n",
    "\n",
    "# plt.plot(np.arange(0, steps), lrs)\n",
    "# plt.plot(np.arange(0, steps), l2)\n",
    "# plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}