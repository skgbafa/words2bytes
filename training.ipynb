{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    ".. image:: ../_static/img/transformer_architecture.jpg\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "# Define the model\n",
    "# ----------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "# language modeling task. The language modeling task is to assign a\n",
    "# probability for the likelihood of a given word (or a sequence of words)\n",
    "# to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "# layer first, followed by a positional encoding layer to account for the order\n",
    "# of the word (see the next paragraph for more details). The\n",
    "# ``nn.TransformerEncoder`` consists of multiple layers of\n",
    "# `nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "# attention mask is required because the self-attention layers in\n",
    "# ``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "# the sequence. For the language modeling task, any tokens on the future\n",
    "# positions should be masked. To have the actual words, the output\n",
    "# of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "# layer, which is followed by a log-Softmax function.\n",
    "#\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ``PositionalEncoding`` module injects some information about the\n",
    "# relative or absolute position of the tokens in the sequence. The\n",
    "# positional encodings have the same dimension as the embeddings so that\n",
    "# the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "# different frequencies.\n",
    "#\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "36718lines [00:01, 25701.21lines/s]\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load and batch data\n",
    "# -------------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
    "# vocab object is built based on the train dataset and is used to numericalize\n",
    "# tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "# function arranges the dataset into columns, trimming off any tokens remaining\n",
    "# after the data has been divided into batches of size ``batch_size``.\n",
    "# For instance, with the alphabet as the sequence (total length of 26)\n",
    "# and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "# length 6:\n",
    "#\n",
    "# .. math::\n",
    "#   \\begin{bmatrix}\n",
    "#   \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "#   \\end{bmatrix}\n",
    "#   \\Rightarrow\n",
    "#   \\begin{bmatrix}\n",
    "#   \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "#   \\end{bmatrix}\n",
    "#\n",
    "# These columns are treated as independent by the model, which means that\n",
    "# the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "# efficient batch processing.\n",
    "#\n",
    "\n",
    "import io\n",
    "import torch\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                      iter(io.open(train_filepath,\n",
    "                                                   encoding=\"utf8\"))))\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "# print(train_data[0:100])\n",
    "# len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-04e991860a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0memb_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-04e991860a73>\u001b[0m in \u001b[0;36memb_to_string\u001b[0;34m(emb)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memb_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-04e991860a73>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memb_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "def emb_to_string(emb):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)\n",
    "    \n",
    "\n",
    "# emb_to_string(train_data[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Functions to generate input and target sequence\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# ``get_batch()`` function generates the input and target sequence for\n",
    "# the transformer model. It subdivides the source data into chunks of\n",
    "# length ``bptt``. For the language modeling task, the model needs the\n",
    "# following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "# we’d get the following two Variables for ``i`` = 0:\n",
    "#\n",
    "# .. image:: ../_static/img/transformer_input_target.png\n",
    "#\n",
    "# It should be noted that the chunks are along dimension 0, consistent\n",
    "# with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "# ``N`` is along dimension 1.\n",
    "#\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "= valkyria chronicles iii = senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the\nvalkyria chronicles iii = senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 0)\n",
    "print(emb_to_string(data))\n",
    "# targets\n",
    "print(emb_to_string(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Initiate an instance\n",
    "# --------------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The model is set up with the hyperparameter below. The vocab size is\n",
    "# equal to the length of the vocab object.\n",
    "#\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([400])\n",
      "tensor([[[ 5.9694e+00, -3.2739e-02,  7.4451e+00,  ..., -6.5862e-01,\n",
      "          -2.7626e-01,  2.1758e-01],\n",
      "         [ 6.6445e+00, -5.0509e-02,  7.0782e+00,  ..., -1.6287e-01,\n",
      "          -1.3598e-01,  2.5965e-02],\n",
      "         [ 7.1941e+00, -2.0928e-01,  7.0255e+00,  ..., -5.2967e-01,\n",
      "          -3.7629e-01,  7.6435e-02],\n",
      "         ...,\n",
      "         [ 7.3013e+00, -3.5123e-01,  7.6371e+00,  ..., -5.4033e-01,\n",
      "          -3.6711e-01,  3.7095e-02],\n",
      "         [ 7.3598e+00, -2.3728e-01,  7.2349e+00,  ..., -6.0109e-01,\n",
      "          -4.3086e-01, -2.2330e-01],\n",
      "         [ 5.7127e+00,  2.4474e-01,  4.4587e+00,  ..., -2.6794e-01,\n",
      "          -7.4363e-04, -7.8101e-01]],\n",
      "\n",
      "        [[ 6.5090e+00,  4.7564e-02,  7.5554e+00,  ..., -6.1108e-01,\n",
      "          -5.4178e-01,  5.4986e-01],\n",
      "         [ 7.4066e+00,  1.9559e-01,  6.1947e+00,  ...,  2.8901e-02,\n",
      "          -1.1673e-01, -4.3527e-01],\n",
      "         [ 7.0076e+00, -2.6451e-01,  8.0990e+00,  ..., -2.2323e-01,\n",
      "          -4.8898e-01, -3.7117e-01],\n",
      "         ...,\n",
      "         [ 7.7688e+00, -3.1505e-01,  7.7997e+00,  ..., -5.3345e-01,\n",
      "          -3.2365e-01, -2.3942e-01],\n",
      "         [ 6.3510e+00, -6.1393e-01,  5.4986e+00,  ..., -3.1895e-01,\n",
      "          -2.3155e-01, -1.1393e-03],\n",
      "         [ 6.8009e+00, -3.0246e-01,  7.4728e+00,  ..., -6.6920e-01,\n",
      "          -3.0422e-01, -2.4728e-01]],\n",
      "\n",
      "        [[ 6.4981e+00, -3.5681e-01,  7.0381e+00,  ..., -7.1980e-01,\n",
      "          -7.2671e-01,  5.3586e-01],\n",
      "         [ 6.8220e+00,  2.0124e-02,  5.6507e+00,  ..., -2.6601e-01,\n",
      "           2.2581e-01,  2.2355e-02],\n",
      "         [ 7.2919e+00, -3.7081e-02,  8.9790e+00,  ..., -4.3496e-01,\n",
      "          -4.2897e-01, -4.8163e-01],\n",
      "         ...,\n",
      "         [ 7.2426e+00, -3.8882e-01,  7.8566e+00,  ..., -2.8911e-01,\n",
      "           4.3831e-03, -6.3567e-01],\n",
      "         [ 7.2494e+00, -4.0330e-01,  7.8981e+00,  ..., -2.7558e-01,\n",
      "          -3.8753e-01, -3.8043e-01],\n",
      "         [ 7.2409e+00, -1.8158e-01,  8.9006e+00,  ..., -3.0503e-01,\n",
      "          -4.3575e-01, -4.7154e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.2722e+00,  3.7588e-01,  7.2738e+00,  ..., -3.7183e-01,\n",
      "           1.4900e-02,  3.0056e-01],\n",
      "         [ 7.8762e+00, -8.6757e-01,  8.3624e+00,  ..., -1.4036e-01,\n",
      "          -5.3385e-01, -2.1326e-01],\n",
      "         [ 6.6304e+00,  1.5449e-01,  9.3132e+00,  ..., -6.1666e-01,\n",
      "           1.5185e-01, -2.7716e-01],\n",
      "         ...,\n",
      "         [ 6.9359e+00,  1.1107e-01,  8.8452e+00,  ...,  6.8178e-01,\n",
      "           3.9406e-02, -5.4765e-01],\n",
      "         [ 6.8195e+00, -3.0641e-01,  7.4673e+00,  ..., -7.7475e-01,\n",
      "          -4.1150e-01, -2.0776e-01],\n",
      "         [ 7.1165e+00, -4.2618e-01,  6.8591e+00,  ..., -4.6199e-01,\n",
      "          -3.5272e-01, -1.3186e-01]],\n",
      "\n",
      "        [[ 7.2096e+00,  6.3686e-02,  9.3931e+00,  ..., -3.4586e-01,\n",
      "          -2.8847e-01, -4.6773e-01],\n",
      "         [ 7.2022e+00, -4.7464e-01,  6.6118e+00,  ..., -7.3520e-01,\n",
      "          -1.6476e-01,  4.2346e-02],\n",
      "         [ 7.2231e+00, -3.6035e-01,  7.3712e+00,  ..., -4.3077e-01,\n",
      "          -3.9907e-01, -7.4402e-02],\n",
      "         ...,\n",
      "         [ 7.6576e+00, -3.9883e-01,  6.5906e+00,  ..., -2.0185e-01,\n",
      "          -4.6223e-01,  5.7640e-01],\n",
      "         [ 7.0902e+00, -2.7472e-01,  8.4715e+00,  ..., -1.9760e-01,\n",
      "          -1.7212e-01, -5.6665e-01],\n",
      "         [ 6.9384e+00,  1.3300e-01,  9.3765e+00,  ..., -2.9469e-01,\n",
      "          -3.7409e-01, -1.8312e-01]],\n",
      "\n",
      "        [[ 6.6617e+00, -1.7718e-01,  3.9425e+00,  ..., -5.2258e-01,\n",
      "          -3.3258e-01, -5.8771e-01],\n",
      "         [ 6.5999e+00, -3.8579e-01,  7.8890e+00,  ..., -4.9407e-01,\n",
      "           4.9875e-02, -8.3577e-01],\n",
      "         [ 7.2350e+00, -4.4544e-01,  8.8660e+00,  ..., -3.4000e-02,\n",
      "          -4.4386e-01, -4.6250e-01],\n",
      "         ...,\n",
      "         [ 6.2999e+00,  2.0524e-03,  6.2001e+00,  ..., -8.1933e-01,\n",
      "          -4.4947e-01,  3.7263e-01],\n",
      "         [ 6.1724e+00, -3.6939e-01,  5.5830e+00,  ..., -6.7984e-01,\n",
      "          -5.3093e-01, -1.7934e-01],\n",
      "         [ 5.7356e+00, -1.4877e-01,  5.4002e+00,  ..., -6.8684e-01,\n",
      "          -4.0503e-01, -2.7648e-01]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.00s | valid loss  6.30 | valid ppl   544.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([400])\n",
      "tensor([[[ 6.1409e+00, -9.7964e-03,  7.4823e+00,  ..., -8.4239e-01,\n",
      "          -4.4243e-01,  4.6686e-02],\n",
      "         [ 7.2731e+00,  3.7825e-02,  7.0737e+00,  ..., -5.2113e-01,\n",
      "          -2.1160e-01, -3.0170e-01],\n",
      "         [ 7.2318e+00, -2.4372e-01,  7.1655e+00,  ..., -7.0744e-01,\n",
      "          -2.4914e-01, -1.2046e-01],\n",
      "         ...,\n",
      "         [ 7.3178e+00, -4.6766e-01,  7.2555e+00,  ..., -6.2200e-01,\n",
      "          -4.9337e-01, -6.5611e-02],\n",
      "         [ 7.4058e+00, -3.8438e-01,  7.2160e+00,  ..., -5.3738e-01,\n",
      "          -3.2842e-01, -2.7082e-01],\n",
      "         [ 5.3453e+00,  3.7085e-01,  3.8555e+00,  ..., -6.7134e-01,\n",
      "           1.4986e-01, -1.3332e-01]],\n",
      "\n",
      "        [[ 6.9029e+00,  3.9083e-02,  7.3981e+00,  ..., -6.5532e-01,\n",
      "          -2.5892e-01, -5.6452e-03],\n",
      "         [ 7.1416e+00,  3.3347e-02,  5.8004e+00,  ..., -2.8537e-01,\n",
      "          -3.8689e-03, -6.0324e-01],\n",
      "         [ 7.1169e+00, -1.9095e-01,  8.2144e+00,  ..., -5.2030e-01,\n",
      "          -3.8564e-01, -4.1446e-01],\n",
      "         ...,\n",
      "         [ 7.9138e+00, -3.8827e-01,  7.5760e+00,  ..., -6.3703e-01,\n",
      "          -2.4651e-01,  1.9795e-02],\n",
      "         [ 6.1260e+00, -1.3915e-01,  5.5265e+00,  ..., -3.1800e-01,\n",
      "           4.2070e-02, -3.5568e-01],\n",
      "         [ 6.7202e+00, -4.8443e-02,  6.5972e+00,  ..., -6.0206e-01,\n",
      "          -1.7218e-01, -5.3479e-02]],\n",
      "\n",
      "        [[ 7.3233e+00, -5.5723e-01,  7.5849e+00,  ..., -5.1494e-01,\n",
      "          -6.7770e-01,  4.0309e-01],\n",
      "         [ 6.7344e+00,  2.8635e-01,  6.6573e+00,  ..., -3.2924e-01,\n",
      "           2.0498e-01, -2.1346e-01],\n",
      "         [ 7.4615e+00, -2.0577e-01,  8.9576e+00,  ..., -3.1472e-01,\n",
      "          -4.7009e-01, -2.0738e-01],\n",
      "         ...,\n",
      "         [ 6.7933e+00, -2.0156e-01,  8.0392e+00,  ..., -7.1792e-02,\n",
      "          -1.8890e-01, -5.3443e-01],\n",
      "         [ 6.9627e+00,  1.6303e-02,  7.5686e+00,  ..., -2.0728e-01,\n",
      "          -2.5520e-01, -3.2459e-01],\n",
      "         [ 6.9146e+00, -3.3868e-02,  9.0776e+00,  ..., -4.4986e-01,\n",
      "          -3.9545e-01, -2.1353e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.6904e+00, -2.9356e-01,  7.2940e+00,  ..., -8.1691e-01,\n",
      "          -1.2011e-01, -9.2439e-02],\n",
      "         [ 7.6032e+00, -5.4097e-01,  8.0780e+00,  ..., -3.9936e-01,\n",
      "          -7.2178e-01, -1.2098e-01],\n",
      "         [ 7.2016e+00,  1.2680e-01,  9.2001e+00,  ..., -4.1935e-01,\n",
      "          -3.6325e-01, -2.6181e-01],\n",
      "         ...,\n",
      "         [ 7.1747e+00,  1.5157e-01,  7.8643e+00,  ...,  3.9558e-01,\n",
      "          -8.0078e-02, -5.7955e-01],\n",
      "         [ 6.9704e+00, -8.1018e-01,  6.3033e+00,  ..., -5.5117e-01,\n",
      "          -3.1899e-01, -9.5504e-02],\n",
      "         [ 7.0845e+00, -4.6689e-01,  6.4778e+00,  ..., -4.2461e-01,\n",
      "          -3.2090e-01, -1.4765e-01]],\n",
      "\n",
      "        [[ 7.3729e+00,  1.4775e-01,  9.3013e+00,  ..., -2.7612e-01,\n",
      "          -4.3912e-01, -1.2901e-01],\n",
      "         [ 6.8393e+00,  3.3111e-01,  6.3460e+00,  ..., -5.5847e-01,\n",
      "          -5.4799e-02, -7.7718e-02],\n",
      "         [ 7.4563e+00, -2.7124e-01,  7.6503e+00,  ..., -3.2907e-01,\n",
      "          -2.5542e-01,  8.6501e-02],\n",
      "         ...,\n",
      "         [ 8.8943e+00, -7.5503e-01,  6.8121e+00,  ..., -5.4809e-01,\n",
      "          -1.8593e-01,  4.5717e-01],\n",
      "         [ 7.1446e+00, -2.5320e-01,  8.7007e+00,  ..., -3.4770e-01,\n",
      "          -1.7992e-01, -6.1146e-01],\n",
      "         [ 6.8518e+00,  4.2649e-02,  8.8806e+00,  ..., -2.6202e-01,\n",
      "          -4.9575e-01, -3.2155e-01]],\n",
      "\n",
      "        [[ 6.6037e+00, -3.3886e-02,  4.7273e+00,  ..., -6.4303e-02,\n",
      "          -2.5447e-01, -5.7699e-01],\n",
      "         [ 7.1350e+00, -6.1914e-01,  8.3259e+00,  ..., -5.9764e-01,\n",
      "           2.4519e-01, -1.2393e+00],\n",
      "         [ 7.2570e+00, -3.4132e-01,  8.7166e+00,  ..., -6.9584e-02,\n",
      "          -2.3255e-01, -2.1050e-01],\n",
      "         ...,\n",
      "         [ 6.4823e+00, -2.2518e-01,  5.8307e+00,  ..., -7.5081e-01,\n",
      "          -5.6293e-01,  1.1311e-01],\n",
      "         [ 6.4662e+00, -1.0826e-01,  5.3197e+00,  ..., -7.1860e-01,\n",
      "          -5.8203e-01, -7.4595e-01],\n",
      "         [ 5.4345e+00,  4.0682e-01,  4.8949e+00,  ..., -6.9956e-01,\n",
      "          -2.5686e-02, -3.0707e-01]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  5.93s | valid loss  6.30 | valid ppl   544.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([400])\n",
      "tensor([[[ 6.3797e+00, -2.0518e-01,  7.7995e+00,  ..., -7.0260e-01,\n",
      "          -5.7189e-01,  5.0528e-01],\n",
      "         [ 6.6984e+00,  1.9225e-01,  7.2680e+00,  ..., -4.6051e-01,\n",
      "           8.6206e-03, -3.0383e-01],\n",
      "         [ 7.3843e+00, -4.2541e-01,  7.4571e+00,  ..., -7.1814e-01,\n",
      "          -4.1176e-01,  1.2701e-01],\n",
      "         ...,\n",
      "         [ 7.7508e+00, -4.1323e-01,  7.6223e+00,  ..., -4.7678e-01,\n",
      "          -2.8816e-01, -1.6010e-01],\n",
      "         [ 7.6478e+00, -3.1831e-01,  7.3945e+00,  ..., -5.1818e-01,\n",
      "          -4.0964e-01, -2.0112e-01],\n",
      "         [ 6.1195e+00, -4.6333e-02,  4.3095e+00,  ..., -7.8183e-01,\n",
      "          -7.0347e-02, -3.5787e-01]],\n",
      "\n",
      "        [[ 6.7778e+00, -2.2193e-01,  7.0503e+00,  ..., -3.8079e-01,\n",
      "          -3.2963e-01,  5.3439e-01],\n",
      "         [ 7.1854e+00, -2.6841e-01,  6.2204e+00,  ...,  3.5107e-02,\n",
      "           1.9788e-01, -5.6305e-02],\n",
      "         [ 7.0753e+00, -2.8185e-01,  8.0961e+00,  ..., -5.7429e-01,\n",
      "          -3.9400e-01, -4.1497e-01],\n",
      "         ...,\n",
      "         [ 7.8181e+00, -4.2835e-01,  7.3393e+00,  ..., -3.7338e-01,\n",
      "          -2.9471e-01, -5.9696e-01],\n",
      "         [ 6.2232e+00, -1.0661e-01,  5.6009e+00,  ..., -3.7374e-01,\n",
      "          -3.2029e-01, -6.3148e-01],\n",
      "         [ 6.9833e+00, -3.1963e-01,  7.0563e+00,  ..., -6.7782e-01,\n",
      "          -1.6956e-01, -1.4140e-01]],\n",
      "\n",
      "        [[ 6.5068e+00, -2.7688e-01,  6.8978e+00,  ..., -5.6617e-01,\n",
      "          -6.5832e-01,  5.0199e-01],\n",
      "         [ 6.3364e+00, -1.0273e-01,  6.1587e+00,  ..., -6.0145e-01,\n",
      "           2.6708e-01,  4.1408e-01],\n",
      "         [ 7.1636e+00, -2.7943e-01,  8.9726e+00,  ..., -4.5968e-01,\n",
      "          -1.6865e-01, -2.8605e-01],\n",
      "         ...,\n",
      "         [ 6.7511e+00, -3.0263e-01,  7.7815e+00,  ..., -1.1619e-01,\n",
      "          -2.5873e-02, -8.6548e-01],\n",
      "         [ 7.3350e+00, -3.2220e-01,  7.5130e+00,  ..., -5.6843e-01,\n",
      "          -1.1960e-01, -1.7312e-01],\n",
      "         [ 6.9644e+00,  2.7939e-01,  8.9637e+00,  ..., -3.7016e-01,\n",
      "          -3.3209e-01, -2.0210e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.8140e+00,  2.3283e-01,  7.2087e+00,  ..., -5.3606e-01,\n",
      "          -5.1472e-02, -1.5753e-01],\n",
      "         [ 7.5386e+00, -7.9318e-01,  8.2170e+00,  ..., -2.5814e-01,\n",
      "          -3.4693e-01, -2.2361e-01],\n",
      "         [ 7.1808e+00, -2.2244e-02,  9.3322e+00,  ..., -2.9700e-01,\n",
      "          -3.3214e-01, -4.3539e-01],\n",
      "         ...,\n",
      "         [ 7.3807e+00, -1.2476e-01,  8.9700e+00,  ...,  2.5956e-01,\n",
      "          -1.0930e-01, -2.2536e-01],\n",
      "         [ 6.7649e+00, -3.9325e-01,  6.8590e+00,  ..., -9.2866e-01,\n",
      "          -4.6525e-01, -2.5468e-01],\n",
      "         [ 7.4546e+00, -2.0838e-01,  7.7193e+00,  ..., -3.8273e-01,\n",
      "          -4.0060e-01, -2.0059e-01]],\n",
      "\n",
      "        [[ 7.6983e+00, -1.0041e-01,  9.2884e+00,  ..., -8.1966e-02,\n",
      "          -3.4983e-01, -2.4889e-01],\n",
      "         [ 6.8817e+00,  5.0998e-01,  6.4187e+00,  ..., -7.4744e-01,\n",
      "          -9.8806e-02,  1.0647e-01],\n",
      "         [ 7.6619e+00, -3.1459e-01,  7.4891e+00,  ..., -5.0352e-01,\n",
      "          -4.7897e-01, -1.3969e-01],\n",
      "         ...,\n",
      "         [ 8.8161e+00, -9.5591e-01,  6.6403e+00,  ..., -4.0796e-01,\n",
      "          -1.9684e-01,  4.9017e-01],\n",
      "         [ 7.1901e+00, -9.2652e-02,  8.4030e+00,  ..., -2.4043e-01,\n",
      "          -2.1272e-02, -6.5901e-01],\n",
      "         [ 6.8174e+00,  9.4768e-02,  8.6969e+00,  ..., -2.0621e-01,\n",
      "          -4.1645e-01, -3.0203e-01]],\n",
      "\n",
      "        [[ 6.3984e+00, -3.9795e-03,  4.2189e+00,  ..., -6.5769e-01,\n",
      "          -3.4842e-01, -3.3134e-01],\n",
      "         [ 6.8113e+00, -3.6497e-01,  7.6704e+00,  ..., -7.3470e-01,\n",
      "          -1.1139e-01, -9.0927e-01],\n",
      "         [ 7.4167e+00, -4.9174e-01,  8.7493e+00,  ...,  1.9185e-01,\n",
      "          -3.5903e-01, -2.6665e-02],\n",
      "         ...,\n",
      "         [ 6.5926e+00,  1.5653e-02,  6.6977e+00,  ..., -7.0720e-01,\n",
      "          -6.4159e-01,  2.0127e-01],\n",
      "         [ 6.0107e+00,  1.0967e-01,  5.4014e+00,  ..., -6.0397e-01,\n",
      "          -1.4426e-01, -4.5823e-01],\n",
      "         [ 5.3267e+00, -9.4665e-03,  4.3909e+00,  ..., -8.7964e-01,\n",
      "          -2.8270e-01, -5.6774e-01]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  5.96s | valid loss  6.30 | valid ppl   544.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Run the model\n",
    "# -------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# `CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "# is applied to track the loss and\n",
    "# `SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "# implements stochastic gradient descent method as the optimizer. The initial\n",
    "# learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "# applied to adjust the learn rate through epochs. During the\n",
    "# training, we use\n",
    "# `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "# function to scale all the gradient together to prevent exploding.\n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        print(data.shape)\n",
    "        print(src_mask.shape)\n",
    "        print(targets.shape)\n",
    "        output = model(data, src_mask)\n",
    "        print(output)\n",
    "        break\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "######################################################################\n",
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  5.78 | test ppl   322.37\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Evaluate the model with the test dataset\n",
    "# -------------------------------------\n",
    "#\n",
    "# Apply the best model to check the result with the test dataset.\n",
    "\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}