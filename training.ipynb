{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    ".. image:: ../_static/img/transformer_architecture.jpg\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "# Define the model\n",
    "# ----------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "# language modeling task. The language modeling task is to assign a\n",
    "# probability for the likelihood of a given word (or a sequence of words)\n",
    "# to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "# layer first, followed by a positional encoding layer to account for the order\n",
    "# of the word (see the next paragraph for more details). The\n",
    "# ``nn.TransformerEncoder`` consists of multiple layers of\n",
    "# `nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "# attention mask is required because the self-attention layers in\n",
    "# ``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "# the sequence. For the language modeling task, any tokens on the future\n",
    "# positions should be masked. To have the actual words, the output\n",
    "# of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "# layer, which is followed by a log-Softmax function.\n",
    "#\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ``PositionalEncoding`` module injects some information about the\n",
    "# relative or absolute position of the tokens in the sequence. The\n",
    "# positional encodings have the same dimension as the embeddings so that\n",
    "# the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "# different frequencies.\n",
    "#\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "36718lines [00:01, 25073.31lines/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "batchify() missing 1 required positional argument: 'bsz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-54f4111f1c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0meval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batchify() missing 1 required positional argument: 'bsz'"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load and batch data\n",
    "# -------------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
    "# vocab object is built based on the train dataset and is used to numericalize\n",
    "# tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "# function arranges the dataset into columns, trimming off any tokens remaining\n",
    "# after the data has been divided into batches of size ``batch_size``.\n",
    "# For instance, with the alphabet as the sequence (total length of 26)\n",
    "# and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "# length 6:\n",
    "#\n",
    "# .. math::\n",
    "#   \\begin{bmatrix}\n",
    "#   \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "#   \\end{bmatrix}\n",
    "#   \\Rightarrow\n",
    "#   \\begin{bmatrix}\n",
    "#   \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "#   \\end{bmatrix}\n",
    "#\n",
    "# These columns are treated as independent by the model, which means that\n",
    "# the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "# efficient batch processing.\n",
    "#\n",
    "\n",
    "import io\n",
    "import torch\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                      iter(io.open(train_filepath,\n",
    "                                                   encoding=\"utf8\"))))\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([102499, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "print(train_data[0:100])\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_to_string(emb):\n",
    "    embeddings = vocab.itos\n",
    "    words = [ embeddings[item] for item in emb ]\n",
    "    return ' '.join(words)\n",
    "    \n",
    "\n",
    "# emb_to_string(train_data[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Functions to generate input and target sequence\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# ``get_batch()`` function generates the input and target sequence for\n",
    "# the transformer model. It subdivides the source data into chunks of\n",
    "# length ``bptt``. For the language modeling task, the model needs the\n",
    "# following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "# we’d get the following two Variables for ``i`` = 0:\n",
    "#\n",
    "# .. image:: ../_static/img/transformer_input_target.png\n",
    "#\n",
    "# It should be noted that the chunks are along dimension 0, consistent\n",
    "# with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "# ``N`` is along dimension 1.\n",
    "#\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, targets = get_batch(train_data, 0)\n",
    "# print(emb_to_string(data))\n",
    "# # targets\n",
    "# print(emb_to_string(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Initiate an instance\n",
    "# --------------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The model is set up with the hyperparameter below. The vocab size is\n",
    "# equal to the length of the vocab object.\n",
    "#\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([35, 20, 28783])\n",
      "tensor([[[ 5.6297e-02,  1.3365e+00, -7.0731e-01,  ..., -2.4183e+00,\n",
      "          -1.6858e-01,  5.4817e-02],\n",
      "         [ 4.4699e-01,  7.2480e-01, -8.9251e-01,  ..., -1.4051e+00,\n",
      "           3.9046e-01, -1.3388e-01],\n",
      "         [-5.6210e-01,  1.4691e-01,  4.7620e-02,  ..., -1.9766e+00,\n",
      "          -8.2208e-01,  1.1333e+00],\n",
      "         ...,\n",
      "         [ 6.0212e-01,  1.0541e+00, -6.2298e-01,  ..., -1.2927e+00,\n",
      "           5.9614e-01,  6.8262e-01],\n",
      "         [-1.1144e+00, -3.6291e-01, -2.6602e-01,  ..., -8.8619e-01,\n",
      "          -1.8892e-01,  1.0827e-01],\n",
      "         [ 1.2261e-02,  7.7214e-01, -9.0203e-03,  ...,  5.2091e-01,\n",
      "           1.9094e-01,  3.8213e-01]],\n",
      "\n",
      "        [[-5.1985e-01,  1.3251e+00, -6.0750e-01,  ..., -2.8738e-01,\n",
      "          -6.0111e-01, -1.5560e-01],\n",
      "         [-2.9846e-01,  3.3931e+00, -9.6694e-01,  ...,  9.2890e-02,\n",
      "          -8.9726e-01, -5.4876e-01],\n",
      "         [-1.6649e+00,  2.0439e-01, -1.4719e-01,  ..., -4.4532e-01,\n",
      "           2.7947e-01, -1.5350e-01],\n",
      "         ...,\n",
      "         [ 2.9976e-01,  1.6434e+00,  2.3998e-01,  ..., -2.3752e+00,\n",
      "          -6.6727e-01,  4.8287e-01],\n",
      "         [ 1.7668e-01,  5.5142e-01,  3.3599e-01,  ..., -4.2154e-01,\n",
      "          -1.2018e+00,  3.7302e-01],\n",
      "         [ 9.5138e-01,  1.0385e+00,  5.2147e-01,  ..., -1.0548e+00,\n",
      "           6.7820e-02, -1.1590e-01]],\n",
      "\n",
      "        [[-8.4514e-01,  6.0495e-01,  5.9068e-01,  ..., -1.1500e+00,\n",
      "          -8.3153e-01, -7.4874e-02],\n",
      "         [-8.7392e-01,  6.9631e-01,  8.1307e-01,  ..., -1.2423e+00,\n",
      "           1.6180e-01,  2.9765e-01],\n",
      "         [-2.7292e-01,  1.8440e+00,  8.0019e-01,  ...,  6.6045e-01,\n",
      "          -6.9776e-01, -4.2850e-01],\n",
      "         ...,\n",
      "         [-4.1375e-01,  1.7253e+00,  2.9054e-01,  ..., -2.3705e+00,\n",
      "          -4.4057e-01, -1.0537e-01],\n",
      "         [ 5.5300e-01,  4.6668e-01, -5.8416e-01,  ..., -1.6735e+00,\n",
      "          -7.2578e-01,  2.7163e-01],\n",
      "         [-3.9950e-01, -1.4026e+00, -9.0279e-01,  ..., -1.2430e+00,\n",
      "           1.7648e-01, -3.9757e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2497e+00, -3.5631e-01, -1.5550e-01,  ...,  3.5507e-01,\n",
      "          -8.4586e-01, -5.4565e-02],\n",
      "         [ 6.8859e-01, -6.4970e-01,  4.0748e-01,  ...,  9.0108e-01,\n",
      "           3.3190e-01, -5.1094e-01],\n",
      "         [-1.7378e-01, -4.7953e-01,  2.5645e-01,  ..., -8.3824e-01,\n",
      "          -2.6474e-01,  6.2073e-01],\n",
      "         ...,\n",
      "         [ 1.2449e+00, -1.2173e+00, -4.9854e-01,  ..., -3.8750e-02,\n",
      "           1.0448e+00, -3.7670e-01],\n",
      "         [ 1.3379e-01, -5.4773e-01, -5.0882e-02,  ..., -3.8863e-01,\n",
      "          -8.5263e-01,  3.3764e-01],\n",
      "         [ 9.0639e-01, -2.7235e-01, -7.7920e-01,  ...,  5.4843e-02,\n",
      "          -1.5316e-01, -1.3763e+00]],\n",
      "\n",
      "        [[ 7.8730e-01,  7.2441e-02, -7.3547e-01,  ...,  2.1947e-01,\n",
      "           6.6538e-01,  5.1256e-02],\n",
      "         [-6.4607e-01,  6.8352e-01,  2.5878e-01,  ...,  5.4499e-01,\n",
      "          -2.8947e-01,  1.0972e+00],\n",
      "         [ 2.0265e-01, -1.8191e-01, -4.8492e-01,  ...,  3.8940e-01,\n",
      "           6.1779e-01, -8.2270e-01],\n",
      "         ...,\n",
      "         [-1.8386e-01, -7.7268e-01,  3.3944e-01,  ..., -8.9543e-01,\n",
      "           2.1761e-01, -1.0985e-01],\n",
      "         [-4.8039e-01, -9.8669e-02,  2.1851e-03,  ...,  8.0309e-01,\n",
      "           1.0691e-01, -1.3178e+00],\n",
      "         [-8.2642e-01,  3.3406e-01, -3.3339e-02,  ...,  1.1946e+00,\n",
      "          -2.2760e-01, -8.9601e-01]],\n",
      "\n",
      "        [[-4.9613e-01,  3.8567e-01, -3.1540e-01,  ..., -1.4436e-01,\n",
      "          -6.7140e-02, -5.5735e-01],\n",
      "         [-2.4677e-01, -1.2902e+00, -6.9661e-01,  ..., -1.2416e+00,\n",
      "           1.4455e+00,  3.9054e-02],\n",
      "         [ 8.2052e-03, -1.4689e+00, -1.2343e+00,  ..., -1.0615e+00,\n",
      "           2.9009e-01, -2.6129e-01],\n",
      "         ...,\n",
      "         [ 3.3456e-01, -1.4393e+00, -1.8956e-01,  ..., -4.9159e-01,\n",
      "           4.4817e-01,  2.5517e-01],\n",
      "         [-9.3242e-01, -3.5167e-01,  9.7618e-02,  ...,  3.2282e-02,\n",
      "          -2.0230e-02, -1.7239e-01],\n",
      "         [ 7.3754e-01, -6.3831e-01, -3.1075e-01,  ...,  4.6323e-01,\n",
      "           5.4352e-01, -1.3360e+00]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  4.01s | valid loss 10.63 | valid ppl 41277.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "torch.Size([35, 20, 28783])\n",
      "tensor([[[ 0.6703,  2.4575,  0.5799,  ..., -2.0830,  0.0954,  0.9427],\n",
      "         [-0.5315,  0.4937, -0.6227,  ..., -0.8244, -0.4899,  0.0423],\n",
      "         [-0.6398,  0.4665, -0.3195,  ..., -2.1730, -0.6415,  0.9667],\n",
      "         ...,\n",
      "         [ 0.8939,  1.2171, -0.5820,  ..., -0.8466, -0.3070, -0.0688],\n",
      "         [-1.4647,  0.1013, -0.1659,  ..., -1.2423, -0.5234,  1.0497],\n",
      "         [-0.3855,  1.8061,  0.2736,  ..., -0.6271, -1.1003,  0.6972]],\n",
      "\n",
      "        [[ 0.2538,  1.0787,  0.2952,  ..., -1.3943, -0.7666, -0.6005],\n",
      "         [ 0.8108,  2.1073, -1.2904,  ..., -0.8200, -0.7064, -0.1442],\n",
      "         [-0.6500,  0.7430,  0.2589,  ..., -0.7453, -0.1099, -0.5169],\n",
      "         ...,\n",
      "         [ 0.0647,  0.9105,  0.2896,  ..., -2.5186, -0.4137, -0.0187],\n",
      "         [-0.0029,  1.1914,  0.3377,  ..., -0.0556, -0.4235,  1.3441],\n",
      "         [ 0.2329,  1.1103,  0.6903,  ..., -1.0789,  0.1370,  0.6570]],\n",
      "\n",
      "        [[-1.3980,  0.5815, -0.3454,  ..., -1.4906, -0.6629, -1.4403],\n",
      "         [ 0.1814,  0.6726, -0.0276,  ..., -1.6181, -1.0661,  0.3353],\n",
      "         [ 0.3516,  1.1161,  0.7700,  ..., -0.8399,  0.0721,  0.3040],\n",
      "         ...,\n",
      "         [-0.5174,  1.5728, -0.0687,  ..., -1.5859, -0.6497, -0.6652],\n",
      "         [-0.3147,  0.0195, -0.2941,  ..., -1.7928, -0.4952,  0.8331],\n",
      "         [-0.5052, -1.4715, -0.0473,  ..., -1.4278,  0.5392,  0.4108]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2954, -1.1766,  0.3293,  ..., -0.6130,  0.3683, -1.1215],\n",
      "         [ 0.6613, -0.5934,  0.2063,  ...,  1.1583, -0.1397, -0.8364],\n",
      "         [-0.3888, -0.2993,  0.1008,  ...,  0.9643, -0.2523,  0.9338],\n",
      "         ...,\n",
      "         [ 0.1159, -1.1332,  0.1375,  ..., -0.0610,  0.9332, -0.5730],\n",
      "         [ 0.8367, -0.8439,  0.1189,  ...,  0.0222, -1.5317, -0.3563],\n",
      "         [ 0.3050, -0.2952, -1.0553,  ..., -0.3057, -1.0991, -0.8736]],\n",
      "\n",
      "        [[-0.4416, -0.2332, -0.4804,  ...,  0.5872, -0.6219, -1.3640],\n",
      "         [-1.0076,  0.0438,  0.0184,  ...,  0.5984, -0.6425,  1.0405],\n",
      "         [-0.0947, -0.2269, -0.3080,  ..., -0.2467, -0.1601, -0.2322],\n",
      "         ...,\n",
      "         [ 0.7335, -0.3963,  0.2528,  ..., -0.1951,  0.0625, -0.2392],\n",
      "         [ 0.2905, -1.0649,  0.6968,  ...,  0.7630,  0.2922, -1.3640],\n",
      "         [-0.7998,  0.4183, -0.2193,  ...,  0.3563, -0.6511, -1.6092]],\n",
      "\n",
      "        [[-0.0050,  0.4011,  0.3998,  ..., -0.2108, -0.1014, -0.3553],\n",
      "         [ 0.0260, -0.9727, -0.8734,  ..., -0.7511,  0.4515, -0.1933],\n",
      "         [-0.2627, -1.6410, -1.0477,  ..., -0.9412,  0.0194, -0.0260],\n",
      "         ...,\n",
      "         [ 0.5231, -0.8555,  0.0328,  ..., -1.0894,  0.3450,  0.3566],\n",
      "         [-0.4220,  0.4162,  0.1015,  ...,  0.1858, -0.0453,  0.4291],\n",
      "         [-0.5004, -0.0557, -0.3359,  ...,  0.9779,  0.5675, -1.4138]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  3.91s | valid loss 10.63 | valid ppl 41277.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "torch.Size([35, 20, 28783])\n",
      "tensor([[[ 5.7307e-01,  1.3100e+00,  4.3271e-03,  ..., -1.0798e+00,\n",
      "           1.3371e-01, -3.8585e-04],\n",
      "         [-5.3866e-01,  6.7106e-01, -7.2360e-01,  ..., -1.9692e-01,\n",
      "          -3.5464e-01,  4.9382e-01],\n",
      "         [ 1.9528e-01, -5.0076e-01, -1.8244e-01,  ..., -1.5183e+00,\n",
      "          -9.3272e-01,  1.0027e+00],\n",
      "         ...,\n",
      "         [ 2.9728e-01,  9.1507e-01,  6.1956e-02,  ..., -2.3103e-01,\n",
      "          -2.9150e-01,  3.2391e-01],\n",
      "         [-1.3740e+00,  5.2286e-01,  5.2913e-01,  ..., -1.2032e+00,\n",
      "          -1.7839e+00,  8.9871e-01],\n",
      "         [-4.9403e-01, -1.0697e-02,  3.4063e-02,  ..., -1.1328e+00,\n",
      "          -1.2025e+00, -3.5799e-01]],\n",
      "\n",
      "        [[ 1.8408e-01, -3.0773e-01, -1.4904e-01,  ..., -9.2742e-01,\n",
      "          -6.7762e-01, -1.0272e-01],\n",
      "         [ 2.1703e-01,  1.8685e+00, -1.0764e+00,  ...,  2.2530e-01,\n",
      "          -4.0868e-01,  1.0723e-01],\n",
      "         [-4.2060e-01, -3.9585e-01, -4.4907e-01,  ..., -8.5112e-01,\n",
      "           2.0544e-01, -3.2255e-01],\n",
      "         ...,\n",
      "         [ 4.4281e-01,  1.1691e+00,  4.5158e-01,  ..., -1.9366e+00,\n",
      "          -1.0208e-01, -2.9571e-01],\n",
      "         [ 6.6951e-01,  1.0405e+00, -1.1615e+00,  ..., -9.0059e-01,\n",
      "          -7.0058e-01,  1.4483e+00],\n",
      "         [ 3.1809e-01,  7.9743e-01,  1.3546e+00,  ..., -1.0471e+00,\n",
      "           1.0352e-01,  5.6108e-01]],\n",
      "\n",
      "        [[-1.2741e+00,  1.0169e+00, -4.3470e-01,  ..., -2.4745e+00,\n",
      "          -4.5107e-01, -7.9054e-01],\n",
      "         [-1.8223e-01,  8.8102e-01,  8.8281e-01,  ..., -1.8735e+00,\n",
      "          -7.5285e-01,  2.8418e-01],\n",
      "         [ 7.2566e-01,  1.9359e+00,  1.6369e+00,  ..., -1.6389e-01,\n",
      "           9.2104e-02,  1.7410e-01],\n",
      "         ...,\n",
      "         [-1.1324e+00,  9.2067e-01,  7.8116e-01,  ..., -9.4649e-01,\n",
      "          -4.0438e-01,  4.3810e-01],\n",
      "         [-1.1064e-01,  6.4114e-01, -6.0452e-01,  ..., -2.7520e+00,\n",
      "          -5.6089e-01,  7.9445e-01],\n",
      "         [-1.2427e+00, -6.6035e-01, -1.1708e-01,  ..., -9.6158e-01,\n",
      "           5.0078e-01,  1.2465e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0440e+00, -8.2524e-01, -1.0477e-01,  ..., -3.6668e-01,\n",
      "           4.8455e-01, -3.1667e-01],\n",
      "         [ 4.8663e-01, -2.7088e-01,  2.6878e-01,  ...,  4.3334e-01,\n",
      "          -2.7010e-01, -1.0142e+00],\n",
      "         [ 3.0739e-01, -8.4329e-01,  2.2177e-01,  ..., -1.5491e-01,\n",
      "           1.4218e-01, -5.2619e-02],\n",
      "         ...,\n",
      "         [ 6.5720e-01, -1.6928e+00, -4.9623e-01,  ..., -2.4418e-01,\n",
      "           1.2278e+00,  9.4709e-02],\n",
      "         [ 2.3707e-01, -4.8407e-01,  2.2547e-01,  ...,  4.0747e-02,\n",
      "          -1.2591e+00, -2.2157e-01],\n",
      "         [ 1.0212e+00, -2.9924e-01, -4.0133e-02,  ..., -6.1944e-01,\n",
      "          -1.5119e-01, -1.3526e+00]],\n",
      "\n",
      "        [[ 5.4585e-02,  2.9070e-01, -8.9033e-01,  ..., -4.0350e-01,\n",
      "          -6.7160e-01, -7.8272e-01],\n",
      "         [-4.3672e-01, -2.0498e-01, -2.8652e-01,  ..., -5.4277e-01,\n",
      "          -6.1687e-01,  1.1286e+00],\n",
      "         [-1.1736e-01, -4.9351e-01, -3.0892e-01,  ...,  4.8109e-01,\n",
      "          -1.9411e-01, -5.6285e-01],\n",
      "         ...,\n",
      "         [ 1.0489e-01, -8.1279e-01, -2.0056e-01,  ..., -3.4841e-01,\n",
      "           3.3887e-01,  1.3968e-01],\n",
      "         [ 3.4227e-01, -6.8040e-01,  1.0263e+00,  ...,  5.6996e-01,\n",
      "          -5.9358e-01, -1.8127e+00],\n",
      "         [-1.0421e+00, -1.5487e-01,  1.0355e-01,  ...,  9.8982e-01,\n",
      "          -1.1357e+00, -1.0029e+00]],\n",
      "\n",
      "        [[-5.1264e-01, -3.4299e-01,  9.7116e-02,  ..., -1.5096e-01,\n",
      "          -1.5359e-01, -6.9302e-01],\n",
      "         [-6.2735e-01, -1.5465e+00, -1.1914e+00,  ..., -6.1111e-01,\n",
      "           3.6933e-01, -8.5595e-02],\n",
      "         [-8.3121e-01, -2.0099e+00, -9.0912e-01,  ..., -6.6363e-01,\n",
      "           7.1561e-01, -3.2017e-01],\n",
      "         ...,\n",
      "         [ 4.6924e-01, -1.0933e+00,  2.5943e-01,  ..., -7.2729e-01,\n",
      "          -1.9692e-01,  3.9568e-03],\n",
      "         [-5.6671e-01, -4.5151e-01,  5.3686e-01,  ..., -1.3518e+00,\n",
      "           3.2690e-01, -8.9924e-01],\n",
      "         [ 7.1491e-01,  5.0872e-01,  7.1869e-01,  ...,  1.6773e-01,\n",
      "           2.9042e-01, -1.3924e+00]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  3.92s | valid loss 10.63 | valid ppl 41277.07\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Run the model\n",
    "# -------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# `CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "# is applied to track the loss and\n",
    "# `SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "# implements stochastic gradient descent method as the optimizer. The initial\n",
    "# learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "# applied to adjust the learn rate through epochs. During the\n",
    "# training, we use\n",
    "# `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "# function to scale all the gradient together to prevent exploding.\n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        print(data.shape)\n",
    "        print(data)\n",
    "        # print(src_mask.shape)\n",
    "        # print(targets.shape)\n",
    "        output = model(data, src_mask)\n",
    "        print(output.shape)\n",
    "        print(output)\n",
    "        break\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "######################################################################\n",
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss 10.62 | test ppl 40977.22\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Evaluate the model with the test dataset\n",
    "# -------------------------------------\n",
    "#\n",
    "# Apply the best model to check the result with the test dataset.\n",
    "\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}