{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from constants import *\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# DATA_PATH = './.data/'\n",
    "# ptb_filenames = list(map(lambda x: \"wsj_\"+ format(x, '04'), range(1, 200)))\n",
    "# TRAINING_DATA = {\n",
    "#     'PennTreebank': {\n",
    "#         \"location\": \"penn-treebank-raw/\",\n",
    "#         \"filenames\": ptb_filenames\n",
    "#     },\n",
    "# }\n",
    "\n",
    "def create_subword_tokenizer(config):\n",
    "    dataset, vocab_size = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\")\n",
    "\n",
    "    # get location\n",
    "    output_location = 'tokenizer/'\n",
    "    tokenizer_loc = 'bpe_tokenizer_' + str(dataset) + '_' + str(vocab_size) + \".tokenizer.json\"\n",
    "    path_to_tokenizer_loc = DATA_PATH + output_location\n",
    "    tokenizer_filepath = path_to_tokenizer_loc + tokenizer_loc\n",
    "\n",
    "    # load tokenizer\n",
    "    if os.path.isfile(tokenizer_filepath):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_filepath)\n",
    "        return tokenizer\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "    # print(paths)\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[])\n",
    "    # print(len(paths))\n",
    "    tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "    # save tokenizer\n",
    "    try:\n",
    "        if not os.path.isdir(path_to_tokenizer_loc):\n",
    "            os.makedirs(path_to_tokenizer_loc)\n",
    "        tokenizer.save(str(tokenizer_filepath))\n",
    "    except Exception as e:\n",
    "        print(\"Error saving tokenizer\", e)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Subword.name,\n",
    "    \"vocab_size\": 40000,\n",
    "    \"max_seq_len\": 32,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "tokenizer = create_subword_tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'H i ! H ello ! this is a test . This t ast es like j ello'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hi! Hello! this is a test. This tastes like jello\").ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package treebank to /home/gbafa/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "<BracketParseCorpusReader in '/home/gbafa/nltk_data/corpora/treebank/combined'>\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "treebank_text = treebank.sents()\n",
    "print(treebank)\n",
    "print(treebank_text[0])\n",
    "print(' '.join(treebank_text[0]))\n",
    "total_count = len(treebank_text)\n",
    "train_count = int(0.7 * total_count) \n",
    "valid_count = int(0.2 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(treebank_text, (train_count, valid_count, test_count))\n",
    "\n",
    "# tokenizer.encode(text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset, vocab_size = extract_config(\n",
    "        config, \"dataset\", \"vocab_size\")\n",
    "location = TRAINING_DATA[dataset]['location']\n",
    "paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                     TRAINING_DATA[dataset]['filenames']))\n",
    "lines = open(paths[0], newline='\\n')\n",
    "raw_data = map(lambda x: list(open(x, newline='\\n')), paths)\n",
    "text_data = []\n",
    "for item in text_data:\n",
    "    item = filter(lambda x: x != '\\n', item)\n",
    "    text_data.extend(item)\n",
    "print(len(text_data))\n",
    "# return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n', ' pierre <unk> N years old will join the board as a nonexecutive director nov. N \\n', ' mr. <unk> is chairman of <unk> n.v. the dutch publishing group \\n', ' rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \\n', ' a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \\n', ' the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \\n', ' <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \\n', \" although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\", ' a <unk> <unk> said this is an old story \\n', \" we 're talking about years ago before anyone heard of asbestos having any questionable properties \\n\"]\n[' consumers may want to move their telephones a little closer to the tv set \\n', \" <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \\n\", ' two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \\n', \" and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show \\n\", ' interactive telephone technology has taken a new leap in <unk> and television programmers are racing to exploit the possibilities \\n', ' eventually viewers may grow <unk> with the technology and <unk> the cost \\n', ' but right now programmers are figuring that viewers who are busy dialing up a range of services may put down their <unk> control <unk> and stay <unk> \\n', \" we 've been spending a lot of time in los angeles talking to tv production people says mike parks president of call interactive which supplied technology for both abc sports and nbc 's consumer minutes \\n\", ' with the competitiveness of the television market these days everyone is looking for a way to get viewers more excited \\n', ' one of the leaders behind the expanded use of N numbers is call interactive a joint venture of giants american express co. and american telephone & telegraph co \\n']\n[\" no it was n't black monday \\n\", \" but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos \\n\", ' some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures \\n', \" the N stock specialist firms on the big board floor the buyers and sellers of last resort who were criticized after the N crash once again could n't handle the selling pressure \\n\", ' big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock traders say \\n', \" heavy selling of standard & poor 's 500-stock index futures in chicago <unk> beat stocks downward \\n\", ' seven big board stocks ual amr bankamerica walt disney capital cities\\\\/abc philip morris and pacific telesis group stopped trading and never resumed \\n', ' the <unk> has already begun \\n', ' the equity market was <unk> \\n', ' once again the specialists were not able to handle the imbalances on the floor of the new york stock exchange said christopher <unk> senior vice president at <unk> securities corp \\n']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from constants import *\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "\n",
    "# DATA_PATH = './.data/'\n",
    "# ptb_filenames = list(map(lambda x: \"wsj_\"+ format(x, '04'), range(1, 200)))\n",
    "# TRAINING_DATA = {\n",
    "#     'PennTreebank': {\n",
    "#         \"location\": \"penn-treebank-raw/\",\n",
    "#         \"filenames\": ptb_filenames\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "def split_dataset(config):\n",
    "    dataset, batch_size, max_seq_len, segmentation = extract_config(\n",
    "        config, \"dataset\", \"batch_size\", \"max_seq_len\", \"segmentation\")\n",
    "    tt_dataset = getattr(datasets, dataset)\n",
    "    # print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # # process non-ptb datasets\n",
    "    # if dataset != Dataset.PennTreebank.name:\n",
    "    #     print(dataset)\n",
    "    #     return tt_dataset.splits(text_field=Field())\n",
    "    \n",
    "    location = TRAINING_DATA[dataset]['location']\n",
    "    paths = list(map(lambda x: str(DATA_PATH+location+x),\n",
    "                        TRAINING_DATA[dataset]['filenames']))\n",
    "\n",
    "    # train data\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "    for path in paths:\n",
    "        raw_data = list(open(path, newline='\\n'))\n",
    "        raw_data = list(filter(lambda x: x != '\\n', raw_data))\n",
    "        if re.search(\"train\", path):\n",
    "           train_data = raw_data\n",
    "        if re.search(\"valid\", path):\n",
    "           valid_data = raw_data\n",
    "        if re.search(\"test\", path):\n",
    "           test_data = raw_data\n",
    "    # print(test_data)\n",
    "    # print(train_data)\n",
    "    return train_data, valid_data, test_data\n",
    "    # print(train_data)\n",
    "\n",
    "\n",
    "    # train_count = int(0.7 * total_count) \n",
    "    # valid_count = int(0.2 * total_count)\n",
    "    # test_count = total_count - train_count - valid_count\n",
    "    # return (text_data[0:train_count], text_data[train_count: train_count + valid_count], text_data[train_count + valid_count:len(text_data)])\n",
    "    # return torch.utils.data.random_split(text_data, (train_count, valid_count, test_count))\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = split_dataset(config)\n",
    "\n",
    "print(train_dataset[0:10])\n",
    "print(valid_dataset[0:10])\n",
    "print(test_dataset[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.match(\"train\", \"ptb.train.txt\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}