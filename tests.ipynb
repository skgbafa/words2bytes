{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000009s)\n",
      "downloading wikitext-103-v1.zip\n",
      "wikitext-103-v1.zip: 100%|██████████| 190M/190M [00:04<00:00, 38.6MB/s]\n",
      "extracting\n",
      "Tokenized and Split Data (67.779651s)\n",
      "Built Vocab (87.863547s)\n",
      "torch.Size([5167283, 20])\n",
      "torch.Size([10916, 20])\n",
      "torch.Size([12320, 20])\n",
      "[End Load Data] (1488.603157s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:2lu6ekzs) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 11276<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe7fad7db73a42ee925c88058c13ac9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/data/github/words2bytes/wandb/run-20210304_194000-2lu6ekzs/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/data/github/words2bytes/wandb/run-20210304_194000-2lu6ekzs/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">soft-water-43</strong>: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/2lu6ekzs\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/2lu6ekzs</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:2lu6ekzs). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.21 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">vivid-frog-44</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/openai-scholars/words2bytes\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes</a><br/>\n                Run page: <a href=\"https://wandb.ai/openai-scholars/words2bytes/runs/146pdl55\" target=\"_blank\">https://wandb.ai/openai-scholars/words2bytes/runs/146pdl55</a><br/>\n                Run data is saved locally in <code>/data/github/words2bytes/wandb/run-20210305_012854-146pdl55</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0, 1]\n                But your machine only has: []\n            ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d5c43d8c8fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# init model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderOnlyTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36moverwrite_by_env_vars\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moverwrite_by_env_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, distributed_backend, automatic_optimization, move_metrics_to_cpu, enable_pl_optimizer)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# init accelerator related flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         self.accelerator_connector.on_trainer_init(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mtpu_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator_connector.py\u001b[0m in \u001b[0;36mon_trainer_init\u001b[0;34m(self, num_processes, tpu_cores, accelerator, distributed_backend, auto_select_gpus, gpus, num_nodes, log_gpu_memory, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_multiple_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetermine_root_gpu_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPUs requested but none are available.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36m_sanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_available_gpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             raise MisconfigurationException(f\"\"\"\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mYou\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mBut\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0monly\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mall_available_gpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: \n                You requested GPUs: [0, 1]\n                But your machine only has: []\n            "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import extract_config\n",
    "from constants import *\n",
    "\n",
    "import lineflow.datasets as lfds\n",
    "from transformer_pl import *\n",
    "from data_pl import *\n",
    "\n",
    "# config\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.WikiText103.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# setup\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab = load_data_word(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# init model\n",
    "model = DecoderOnlyTransformer(config, ntokens)\n",
    "trainer = pl.Trainer(gpus=2)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pl import *\n",
    "from utils import *\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab = load_data(config)\n",
    "# print(vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data torch.Size([35, 20])\ntargets torch.Size([700])\naer of <eos> n capital experience mr they soviet all the yeast incentives <unk> were because awarded <eos> hong were\nbanknote them <unk> billion cities\\/abc made . say state the student <eos> and up a of to if kong going\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, targets = batch\n",
    "    print(\"data\", data.shape)\n",
    "    print(\"targets\",targets.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "print(emb_to_string(data[0], vocab))\n",
    "print(emb_to_string(targets[0:20], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6138,  132,    0,   53, 6532,  167,   10,  121,  158,    2, 2341,    3,\n",
       "           9,   56,    7,    5,    6,   73,  793,  320])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "targets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000145s)\n",
      "Tokenized and Split Data (2.395858s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numel'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-acb6636d9e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# # setup dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-acb6636d9e4a>\u001b[0m in \u001b[0;36mdata_prep\u001b[0;34m(tt_dataset_split)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Divide the dataset into bsz parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mnbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-acb6636d9e4a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Divide the dataset into bsz parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mnbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numel'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "print(\"[Start Load Data]\")\n",
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, batch_size, max_seq_len = extract_config(\n",
    "    config, \"dataset\", \"batch_size\", \"max_seq_len\")\n",
    "dataset = getattr(datasets, dataset)\n",
    "print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# tokenize\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tokenizer_en.pad_token = tokenizer_en.eos_token\n",
    "\n",
    "# split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "    text_field=field_processor)\n",
    "print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# # get vocabulary\n",
    "# vocab = tokenizer_en.get_vocab()\n",
    "\n",
    "# # data prep\n",
    "# def data_prep(tt_dataset_split):\n",
    "#     raw_text_iter = tt_dataset_split[0].text\n",
    "#     data = [tokenizer_en.encode(item) for item in raw_text_iter]\n",
    "#     data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "#     # Divide the dataset into bsz parts.\n",
    "#     nbatch = data.size(0) // batch_size\n",
    "#     # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#     data = data.narrow(0, 0, nbatch * batch_size)\n",
    "#     # Evenly divide the data across the batch_size batches.\n",
    "#     data = data.view(batch_size, -1).t().contiguous()\n",
    "#     return data\n",
    "\n",
    "# # # setup dataloaders\n",
    "# train_dataloader = TextDataloader(data_prep(train_dataset), max_seq_len)\n",
    "# # val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len)\n",
    "# # test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[318]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ĠSyracuse': 29141, 'emia': 22859, '319': 35175, 'Ġstripe': 39858, 'Ġculinary': 35956, 'Ġpayout': 40055, 'Ġchain': 6333, 'Ġappealed': 22280, 'Ġparser': 30751, 'Ġtwentieth': 29112, 'ĠConor': 41708, 'Ġtheorem': 44728, 'onom': 6326, 'ĠStudio': 11733, 'tx': 17602, 'Ġreef': 25088, 'ĠWaves': 42733, 'ok': 482, 'ĠLegislature': 20815, '752': 43665, 'ĠSTA': 47550, 'raf': 32188, '1920': 40454, 'Ġorbiting': 40462, 'ĠLimit': 27272, 'æĺ': 23626, 'knife': 48810, 'Phone': 6132, 'ĠMedia': 6343, 'ĠBh': 16581, 'Isn': 41451, 'Bee': 49512, 'ĠOpposition': 29560, 'ĠProcessor': 32893, 'ĠBere': 37951, 'ĠBounce': 41474, 'ĠYou': 921, 'Ġinstitutional': 15855, 'ĠRequest': 19390, 'ĠPig': 23097, 'Ġsqu': 2809, 'ifty': 24905, 'Ġeldest': 34331, 'ĠThieves': 40392, 'Detroit': 40404, 'Amazing': 42770, 'ISC': 37719, '276': 27988, 'Be': 3856, 'Ġpracticable': 44791, 'Ġammunition': 14271, 'Ġacronym': 38787, 'quartered': 42508, 'Ġnetworks': 7686, 'dit': 5266, 'otype': 8690, 'med': 1150, 'ĠChoose': 17489, 'Ġstakes': 21147, 'Pand': 47206, 'Ġreinvest': 50135, 'Ġportrayal': 33578, 'Ġtechnique': 8173, 'Ġthorn': 38999, 'Ġdemocracy': 7996, 'iera': 41976, 'Ġmogul': 37690, 'Ġ374': 49020, 'istered': 23187, 'Ġforgiven': 32350, 'Pub': 14876, 'taker': 30157, 'Ġarcane': 34362, 'Ġevolves': 43576, 'ĠGarry': 45354, 'Ġnickel': 30666, 'Ġarrogant': 28868, 'Ġconsiderable': 11091, 'Ġsake': 11060, 'Ġart': 1242, 'Ġambul': 18907, 'ãĥ»': 4707, 'colored': 25717, 'ĠKard': 37875, 'Ġbrushing': 42798, 'Random': 29531, 'ĠLo': 6706, 'vana': 33175, 'æĢ': 45250, 'ĠPOL': 20634, 'omedical': 35914, 'acion': 49443, '691': 49541, 'aping': 9269, 'enced': 5864, 'IG': 3528, 'ĠInstead': 5455, 'Ġexists': 7160, 'igs': 9235, 'Ġshirt': 10147, 'Ġglitch': 29204, 'Ġillegally': 15572, 'community': 28158, 'Ġstumble': 35174, 'Ġ06': 9130, 'rative': 13260, 'Ġliar': 31866, 'Ġdelightful': 32327, 'Ġbring': 2222, 'ĠPatriots': 13104, 'Ġmurderous': 34082, '578': 38907, 'ĠCatalonia': 33859, 'ĠVelocity': 43137, 'è¿': 32573, 'Ġdeadliest': 39268, 'Ġsv': 38487, 'ãĥ¼ãĥ': 12045, 'Ġdirty': 11841, 'Tele': 31709, 'ĠPyro': 44954, 'ettel': 47417, 'ĠUs': 4021, 'zen': 4801, 'aren': 5757, 'Term': 40596, 'Ġrattled': 48588, 'Ġstronghold': 32753, 'Ġunbelievably': 48943, 'ĠLiz': 28313, 'Ġtherapies': 29596, 'Ġcaptcha': 48972, 'BUG': 12953, 'iken': 29943, 'Ġstitching': 48945, 'Ġprogressives': 31778, 'Ġps': 26692, 'Supporters': 49422, 'Ġwait': 4043, 'Queue': 34991, 'L': 43, 'board': 3526, 'Ġconversation': 5273, 'ergy': 26079, 'Ġtrim': 15797, 'ĠBroken': 22607, 'ĠPalin': 33109, 'unes': 4015, 'Ġoverth': 34514, 'ebted': 35895, 'mn': 10295, 'ĠSeverus': 41343, 'Ġonion': 21670, 'ĠREPL': 45285, 'ĠAmend': 32218, 'Ġreconc': 19331, 'loads': 46030, 'ĠLibraries': 46267, 'ĠPepper': 24346, 'isSpecial': 39714, 'Ġpayments': 7524, 'Ġvaccination': 22827, 'Ġhabitats': 35308, 'styles': 47720, 'ĠSonia': 20733, 'Germany': 27079, 'Ġnarrative': 8689, 'ĠBeware': 49538, 'ĠBath': 24967, 'Ġinstitute': 24224, 'Ġrailing': 47590, 'ĠCompliance': 40536, 'ĠAgo': 48210, 'ĠGNU': 22961, 'Policy': 36727, 'ĠLe': 1004, 'Ġ264': 32158, 'Full': 13295, 'ĠSpani': 37506, 'Ġacclaimed': 27023, 'Ġproviders': 9549, 'Ġdental': 22727, 'ĠAttorney': 8123, 'ĠLyn': 10350, 'liness': 26061, '].\"': 29225, 'ĠRw': 31641, 'idel': 5943, 'ĠAnalysis': 14691, 'urances': 31741, 'Ġbarrage': 33633, 'ĠMorton': 35766, 'ĠSawyer': 42371, 'Ġoffspring': 20791, 'ere': 567, 'ulsive': 22220, 'Ġtruth': 3872, 'Ġvoicing': 47640, 'ĠTaylor': 8121, 'UI': 10080, 'ĠMcCarthy': 18751, 'Forge': 19857, 'uclear': 4016, 'ĠBenn': 14025, 'ĠBee': 24719, 'ĠOlive': 30012, 'Ġinmates': 16831, 'ĠProduct': 8721, 'ĠPA': 8147, 'ĠLam': 10923, 'appy': 7774, 'Ġ154': 24235, 'ĠUFC': 11448, 'ĠSadly': 27213, 'Ġexpert': 5887, 'ĠAES': 34329, 'ampoo': 40239, 'ĠBelief': 49728, 'Is': 3792, 'Ġcog': 43072, 'kay': 5568, 'Ġsidew': 14910, 'inary': 3219, 'ĠCraw': 20177, 'Ġlov': 42900, 'Ġbent': 17157, 'Ġreliever': 48460, 'ĠLebanese': 28076, 'Ġnost': 18216, 'acement': 5592, '000': 830, 'quad': 47003, 'urned': 44866, 'ě': 215, 'Ġinterested': 4609, 'ility': 879, 'Ġdesert': 10326, 'Ġfits': 11414, 'ĠNan': 18008, 'Ġdrinks': 11758, 'coon': 20912, 'ĠTin': 22894, 'Ġcurves': 23759, 'wid': 28029, 'ĠCentre': 9072, 'Ġgh': 24997, 'Ġstudying': 11065, 'Ġwholes': 17950, 'Ġignored': 9514, 'Ġsummarized': 31880, 'ĠMalaysia': 15336, 'usable': 31979, 'Ġwisely': 32773, 'ibu': 33828, 'Ġevils': 33980, 'Ġamber': 36505, 'ĠGiles': 37538, '%%%%': 36917, 'Ġblush': 37854, 'ĠSlack': 36256, 'hovah': 33023, 'Am': 5840, 'ĠAdv': 8007, 'isco': 4861, 'Ġheated': 16968, 'ĠTours': 42998, 'ĠLogged': 50098, 'Ġshudder': 50130, 'don': 9099, 'immigrant': 39835, 'ĠWy': 12958, '683': 47521, 'Ġnec': 27576, 'Ass': 8021, 'eed': 2308, 'igious': 10956, 'kus': 45614, 'thro': 26110, 'Ġbarbecue': 36646, 'OOOOOOOO': 47732, 'Ġrandomized': 23925, 'ocation': 5040, 'Val': 7762, 'ĠRih': 44502, 'authent': 41299, 'Ġshuts': 44854, 'EN': 1677, 'ĠSpaces': 48086, 'ĠHalf': 13139, 'NE': 12161, 'ggies': 33049, 'ĠDiana': 25752, 'economic': 17079, 'MIT': 36393, 'Date': 10430, 'ĠUpon': 14438, 'Ġmissionaries': 37458, 'ĠCheap': 41087, 'Ġwipe': 19916, 'Ġunequiv': 33923, 'Ġbedrock': 47856, 'ERY': 19664, 'umping': 25218, 'Ġturnovers': 33888, 'Ġconsumed': 13529, 'ousel': 48355, 'Ġcertain': 1728, 'Ġarmies': 18837, 'reenshot': 26892, 'ĠConsidering': 27662, 'ĠLaos': 45919, 'Ġoverhead': 16965, 'ĠFirm': 31623, 'Ġobsessive': 36681, 'Ġbeneficial': 13205, 'Ġanarchists': 29676, 'masters': 40706, 'Amid': 43541, 'ĠMarble': 36891, 'ayson': 34907, 'Ġscoreboard': 50198, 'Ġ88': 9193, 'Ġdeported': 27434, 'Hu': 38202, 'ogue': 5119, 'Ġeccentric': 29303, 'Fe': 14304, 'Ġnails': 23361, 'ĠPlymouth': 42125, 'Ġfem': 2796, 'Ġ425': 36959, 'ĠCrist': 24568, 'inet': 42504, 'Ġpalate': 44100, 'ĠMellon': 49808, 'Ġcommittees': 17460, 'ĠTyler': 14886, 'Ġsubjective': 19088, 'Ġruth': 23562, 'Ġunorthodox': 49830, 'idth': 5649, 'ĠGh': 11972, 'Ġsocialism': 19803, 'ĠLicensed': 49962, 'Ġpainting': 12036, 'Ġ1996': 8235, 'stown': 27928, 'Ġmistakes': 10135, 'ĠHunger': 32367, 'Stars': 29366, 'Ġcared': 19951, 'Ġ58': 7618, 'ogg': 10332, 'Ġanalytics': 23696, 'Ġintending': 38939, 'ĠBulgaria': 27902, 'Factor': 41384, 'Ġdensely': 42255, 'encia': 29634, 'ĠFight': 10480, 'Kevin': 23865, 'Ġworrying': 18916, 'ĠMcM': 15359, 'Ġluckily': 45120, 'Ġflexibility': 13688, 'ored': 1850, 'Ġprotocol': 8435, 'ĠâĤ¬': 10432, 'Then': 6423, 'Ġmonetary': 15331, 'ploy': 1420, 'Ġprinces': 42676, 'Ġracists': 40681, 'Ġclassmate': 45222, 'prep': 46012, 'lla': 8466, 'Ġprefers': 26237, 'inement': 21828, 'Ġinvestigate': 9161, 'Ġsearches': 15455, 'aos': 7495, 'Ġprospects': 13285, 'Amb': 35649, 'ĠOilers': 35004, 'ights': 2337, 'ĠsrfAttach': 43065, 'astrous': 20168, 'ĠAndrews': 26414, 'Ġdeparted': 24057, 'Ġdispatch': 27965, 'Ġstud': 941, 'Ġprototypes': 32338, 'ĠOliv': 40019, 'ĠFancy': 49848, 'version': 9641, 'ĠAdditionally': 12032, '575': 36189, 'Ġreligious': 4158, 'Ġarranging': 41878, 'Ġepigen': 49651, 'Ġ12': 1105, 'ĠIdentified': 24517, 'puter': 10549, 'Neigh': 46445, 'ceiver': 39729, 'boys': 13202, 'ĠSmoking': 39801, 'Ġobstruction': 28118, 'iners': 21257, '059': 46712, 'Ġelegance': 49198, 'HR': 17184, 'Ġdictator': 26671, 'Ġgob': 48484, 'ikk': 36073, 'morph': 24503, 'Ġcornerback': 26683, 'ĠPrinted': 38482, 'ĠFear': 16132, 'Ġenactment': 28547, 'Ġmould': 35370, 'Ġhonorary': 44358, 'Ġefficiently': 18306, 'Ġcosts': 3484, 'Ġskeleton': 18328, 'imeters': 31551, 'Ġmechanical': 12370, 'ophe': 43852, 'oops': 44860, 'ĠMetroid': 45430, 'Ġdiscriminatory': 27200, 'Ġentrants': 49117, 'Ġanimated': 15108, 'ecycle': 47510, 'actic': 12009, 'Ġgonna': 8066, 'breaking': 13395, 'ĠHut': 32767, 'christ': 43533, 'Saint': 48615, 'Ġrevisions': 33315, 'Ġexacerbate': 49522, 'Ġweighed': 20261, 'ĠPuerto': 15295, 'seven': 26548, 'Ġalignment': 19114, 'remlin': 17244, 'Input': 20560, 'Mart': 13143, 'ĠAmtrak': 49145, 'Ġbattling': 20581, 'Ġorganism': 26433, 'Ġabyss': 37678, 'ĠOd': 10529, 'Ġencode': 37773, 'anything': 49459, 'ĠBanks': 19566, 'ĠMinutes': 23757, 'Ġunjust': 21218, 'wa': 10247, 'WS': 19416, 'igible': 26032, 'ĠHealing': 22508, 'ĠUT': 19255, 'plugins': 37390, 'Ġoutlandish': 47284, 'umat': 27798, 'Ġdevastating': 14101, 'Ġlb': 18360, '\"\"\"': 37811, 'Ġcelebr': 4681, 'Ġmunicipal': 13474, 'Ġlargest': 4387, 'Ġsatisfy': 15959, 'Ġedited': 13012, 'ĠChecking': 39432, 'ĠPerry': 14105, 'your': 14108, 'arty': 25494, 'Ġpund': 24745, 'ĠTechnical': 20671, 'ĠTutorial': 36361, 'Ġglamorous': 46185, 'ĠSay': 13816, 'Ġbombed': 34221, 'Ġlabeled': 15494, 'ĠConnection': 26923, '½': 121, 'Ġdecides': 13267, 'Ġhereafter': 49912, 'ĠDeclaration': 24720, 'Ġsets': 5621, '74': 4524, 'irlfriend': 9872, 'Ġindo': 23046, 'liest': 11318, 'Ġlifelong': 25837, 'Ġtilted': 37229, 'Ġtakeoff': 45306, 'ĠLeap': 33927, 'Ġbuddy': 24407, 'Ġauthored': 33941, 'Ġsam': 6072, 'Ġstops': 9911, 'ĠBe': 1355, 'Captain': 27898, 'pol': 16104, 'ĠViolent': 35615, '200': 2167, 'Pol': 8017, 'rice': 20970, 'Things': 22248, 'Ġobviously': 6189, 'ĠBihar': 45301, 'Ġcl': 537, 'zon': 26361, 'Ġpersonnel': 8213, 'LOS': 45376, 'Ġodds': 10402, 'Ġminions': 22811, 'Ġ59': 7863, 'ODUCT': 28644, 'Ġprominent': 9208, 'burgh': 9228, 'Ġfills': 23816, 'moil': 25538, 'Ġconcussion': 27502, 'Ġlame': 30248, 'Ġ1889': 49545, 'ĠMonaco': 31630, 'Ġjars': 33008, 'ĠMale': 12674, 'Ġinvented': 15646, 'Ġlots': 6041, 'Ġ1934': 29300, 'ĠPlans': 30305, 'ĠReasons': 44923, '727': 47760, 'Ġorphans': 50213, 'Ú': 150, '506': 35638, 'ĠAirbus': 39173, 'ĠCal': 2199, 'Ġtheater': 13766, 'ĠCourt': 3078, 'ĠINT': 17828, 'ĠNov': 5267, 'Ġparam': 5772, 'iture': 8089, 'ĠUruguay': 36421, 'Ġdesp': 11267, 'Ġplain': 8631, 'ĠLaurel': 43442, 'card': 9517, 'ĠChristopher': 12803, 'Ġwinter': 7374, 'Ġcoffin': 30834, 'Ġstereotypical': 46112, 'Ġhistories': 25985, '.(': 12195, 'Ġrefusal': 17387, 'Ġdiscredited': 41882, 'ĠAlways': 16622, 'RES': 19535, 'Ġcaused': 4073, 'Ġcollection': 4947, 'PRESS': 32761, 'Ġcompose': 36664, 'Ġfighters': 8486, 'ĠMayor': 10106, 'treatment': 42487, 'Ġdefenseman': 32246, 'ame': 480, 'Ġcitizen': 9511, 'ary': 560, 'grab': 32393, 'ĠInf': 4806, 'ĠAless': 47319, 'Ġvirtual': 7166, 'wart': 24657, 'Ġnominees': 26456, 'ĠStur': 26783, 'ĠMovie': 15875, 'gins': 29878, 'Ġdiscipline': 12883, 'query': 22766, 'rendered': 26238, 'ĠBasil': 32520, 'Ġdocumenting': 33045, 'lette': 21348, 'edu': 15532, 'Ġdisclose': 15771, 'Ġalert': 7995, 'Published': 24492, 'Ġbeers': 16800, 'righteous': 49955, 'Ġliberties': 22008, 'Ġrecomp': 48765, 'ĠBoard': 5926, 'ĠJacksonville': 23194, 'Ġaccommod': 8930, 'pered': 13653, 'ant': 415, 'Ġleads': 5983, 'ĠRomania': 23356, 'Ġuncertain': 8627, 'Ġpeg': 41350, 'essment': 21687, 'Ġcompartment': 26247, 'Ġcontroller': 10444, 'Ġmeaningful': 11570, 'Ġreflection': 14580, 'Ġnull': 9242, 'Ġfiring': 9645, 'ĠPetersburg': 27051, 'ĠDam': 5245, 'Ho': 28900, 'abwe': 27050, 'Ġherself': 5223, 'Ġdishonest': 30549, 'istor': 32380, 'ĠReeves': 41533, 'Ã¨re': 35979, 'Ġreg': 842, 'Ġrecal': 42653, 'nn': 20471, 'Ġcraving': 34357, 'Ġflowering': 48573, 'ĠCher': 19305, 'ctor': 2715, 'eter': 2357, '1991': 24529, 'ĠGabriel': 17371, 'Ġprohib': 6221, 'Ġreleased': 2716, 'Ġrelocate': 36867, 'Ġ};': 18083, 'ĠFraud': 39826, 'ĠTsukuyomi': 43796, 'Ġends': 5645, 'ĠArcane': 26475, 'Ġprop': 2632, 'ĠRooney': 33925, 'ĠRapp': 36962, 'ĠClasses': 38884, 'Ġimpro': 2015, 'Hal': 40202, 'EO': 4720, 'Ġshin': 40465, 'Ġnowhere': 12062, 'Ġenforcement': 5394, 'Ġwartime': 35382, 'Ġcords': 45173, 'ĠBooks': 13661, 'Ġinflicted': 27010, 'Ġofficer': 3818, 'bial': 25200, 'Ġtrance': 43618, 'BUS': 45346, 'ĠSalary': 46860, 'Jon': 18219, 'ĠRom': 3570, 'ogether': 8236, 'Ġmadness': 23208, 'oneliness': 34634, 'seek': 36163, 'Ġspecified': 7368, 'utt': 15318, 'Lot': 48601, 'ĠHardcore': 49691, 'ĠDiscovery': 23455, 'Incre': 15562, 'ĠTrigger': 24593, 'Ġexperimental': 11992, 'Ġprivat': 21883, 'Tile': 35103, 'ĠWallet': 37249, 'Without': 16249, 'LD': 11163, 'ĠiOS': 8969, 'Ġim': 545, 'ocker': 12721, 'ĠHiro': 25940, 'Ġaugmented': 30259, 'Ġanomaly': 32172, 'soc': 35634, 'ler': 1754, 'inging': 14146, '642': 41290, 'sleep': 42832, '905': 44928, 'Ġterritories': 16771, 'Ġponds': 47339, 'Ġfaked': 49184, 'ĠThy': 31468, 'Category': 27313, 'Ġtcp': 48265, 'Ġignor': 11477, 'Ġintimate': 16584, 'Ġinvari': 25275, 'mb': 2022, 'Ġvalued': 17560, 'Ġnatural': 3288, 'release': 20979, 'Ġclaw': 26573, 'Char': 12441, 'ĠLep': 42957, 'Ġprovider': 10131, 'ĠHSBC': 43841, 'ĠPeriod': 18581, 'iov': 16664, 'Ġunequivocally': 47001, 'Ġsimulation': 18640, 'ablishment': 25380, 'ĠFriend': 9182, 'trace': 40546, 'antam': 49653, 'Ġhis': 465, 'Ġfunctionality': 11244, 'lda': 18986, 'ĠDickens': 46167, 'Ġperman': 6092, 'Ġisol': 7010, 'ĠPrec': 28737, 'Ġholster': 43604, 'Ġrun': 1057, 'pire': 5111, 'Ġanthrop': 17911, 'ĠSNP': 25632, 'edly': 49288, 'pect': 806, 'oons': 13022, 'mr': 43395, 'ĠGPU': 11362, '104': 13464, 'Year': 17688, 'ĠClarks': 35931, 'ĠHugh': 25464, 'ĠMountains': 21124, 'Ġdistorted': 26987, 'atsuki': 40063, 'Ġcooperating': 34962, 'afety': 27925, 'uman': 3778, 'ĠRide': 21640, 'tf': 27110, 'abulary': 22528, 'æľ': 17312, 'ĠWrit': 12257, 'ĠNaked': 41619, 'Ġprocedural': 27931, 'ĠCommunities': 35530, 'ĠPAY': 38444, 'Ġproductive': 12973, 'plot': 29487, 'Ġperpetrated': 35524, 'Ġfru': 12658, 'Christmas': 44614, 'Ġmelanch': 40853, 'abeth': 9407, 'ĠPsycho': 38955, 'ĠMitt': 16627, 'Ġprojectile': 26489, 'Ġrecourse': 38424, 'ĠPigs': 49210, 'English': 15823, 'Ġpayroll': 22235, 'Ġprovince': 8473, 'illance': 7682, 'ĠChallenge': 13879, 'Large': 21968, 'ĠExpect': 23600, '},{\"': 8762, 'Ġregistration': 9352, 'Ġpartly': 11476, 'while': 4514, 'ĠMeet': 21167, 'Ġgases': 21678, 'icester': 26382, 'apan': 2674, 'Ġcause': 2728, 'Ġmud': 17492, 'ĠHighlights': 30776, 'Ġmyster': 6141, '489': 35890, 'eri': 33442, 'Ġaffiliated': 18552, 'ĠVotes': 39584, 'ĠFleming': 42439, 'Ġavalanche': 44128, 'button': 16539, 'ilon': 33576, 'Ġfloats': 36016, 'orted': 9741, 'Ġresidual': 29598, 'Ġroster': 9354, 'Ġstalk': 31297, 'Ġforeseeable': 34922, 'Ġcharact': 1543, 'ĠChristmas': 6786, 'Ġconsideration': 9110, '================================': 10052, 'ĠCost': 6446, 'lets': 5289, 'Ġnoise': 7838, 'ĠPreview': 22217, 'ardy': 39124, 'ĠAry': 39477, 'minster': 18462, 'Ġdecriminal': 38126, 'mails': 26165, 'Hub': 16066, 'ĠVisual': 15612, 'Ġtightened': 37257, 'Ġpropensity': 41121, 'visory': 41783, 'ĠVide': 42551, 'ĠKD': 43943, 'acey': 25415, 'ffer': 36761, 'Lind': 43410, 'Ġpoignant': 45516, '663': 45791, 'AND': 6981, 'ĠKimmel': 46552, 'ĠMages': 46796, 'ĠCorsair': 47345, 'Course': 49046, 'Ġfol': 5955, '274': 28857, 'ĠAlbany': 29285, 'Ġunaware': 17261, 'elaide': 25078, 'Bank': 28650, 'ĠItemLevel': 34448, 'Ġcannibal': 39904, 'ĠMelvin': 48834, 'GF': 21713, 'Ġstudied': 9713, 'Trust': 33814, 'ĠMagnet': 32079, 'ankind': 28066, 'Ġbehavioral': 17211, 'Ġpiping': 48426, 'Ġwears': 17326, 'Ġangels': 21981, 'Ġsessions': 10991, 'ensional': 37176, 'Ġlets': 8781, 'Ext': 11627, 'Ġresponsibility': 5798, 'amin': 5669, 'Jerry': 43462, 'Ġ503': 44541, 'ĠHart': 11345, 'ĠRequired': 20906, 'ĠRost': 48306, 'Ġenvy': 31651, 'Ġ109': 16003, 'Agent': 36772, 'Arcade': 43763, 'Ġburying': 48360, 'then': 8524, 'Ġadvise': 18595, 'User': 12982, 'Ġcorrelations': 35811, 'Ġprotester': 34678, 'security': 12961, 'ĠNOAA': 36184, 'ĠCome': 7911, 'ĠComfort': 45769, 'Ġdad': 9955, 'Ġbattle': 3344, 'Section': 16375, 'both': 16885, 'iology': 12371, 'ĠSniper': 24146, 'ĠVehicles': 31365, 'Ġbystand': 33326, 'ĠCharity': 37575, 'Xi': 42528, 'Ġaccording': 1864, 'None': 14202, 'Ġintent': 6824, 'Ġcamping': 22498, 'Ġmetabol': 14623, 'ĠPrize': 15895, 'ĠCamel': 43281, 'rounder': 45788, 'item': 9186, 'ĠSaudis': 33358, 'on': 261, 'Ġdozens': 9264, 'Ġtear': 11626, 'Ġscattered': 16830, 'thus': 26239, 'Ġstag': 35251, 'ĠSnake': 16705, 'ĠOw': 11960, 'Ġspans': 32727, 'ĠRV': 31367, 'Ġaggregation': 46500, 'uff': 1648, 'Ġdomain': 7386, 'ĠSavage': 22287, '557': 41948, 'Ġjava': 20129, 'ubis': 46676, 'Stand': 15480, 'ĠSerpent': 30177, 'chester': 35983, 'ĠPair': 39645, 'Ġdirector': 3437, 'ond': 623, 'ĠStudents': 14882, 'ĠFree': 3232, 'Ġtalents': 18054, 'rb': 26145, 'ramer': 29172, 'Ġoptimum': 39475, 'ĠSequ': 24604, 'Ġpleasure': 9476, 'Ġexcerpt': 20911, 'only': 8807, 'Ġbull': 6473, 'pipe': 34360, 'Lua': 36127, 'Ġ\\\\(\\\\': 40719, 'ĠStead': 45445, 'blue': 17585, '504': 33580, 'ĠLaugh': 47501, 'ĠRudolph': 48684, 'Ġguy': 3516, 'ĠHem': 15617, 'cern': 30903, 'Ġ192': 17817, 'RGB': 36982, 'Located': 43525, 'dress': 49380, 'Ġcomprehens': 8569, 'Ġcancel': 14241, 'ĠUltra': 14563, 'ĠMartian': 32508, 'ĠWarm': 25692, 'Ġdigits': 19561, 'Ġcongreg': 30663, 'ĠHass': 20300, 'pet': 6449, 'Ġtopic': 7243, 'Ġmah': 42768, 'Ġarbitrary': 14977, 'ĠBin': 20828, 'Ġemployer': 9749, 'Ġanonymity': 18685, 'Ġlands': 8604, 'Ġplate': 7480, 'ifact': 29660, 'ĠMarsh': 9786, 'EF': 25425, 'ĠChapter': 7006, 'ĠInfantry': 27749, 'ids': 2340, 'FOR': 13775, 'instein': 11962, 'Ġbool': 20512, 'Ġfiery': 27810, 'ĠSatellite': 33530, 'Ġstagnant': 41391, 'bps': 18799, 'Ġredirected': 45158, 'Ġtint': 34791, 'ĠSail': 27783, 'Ġarrives': 14443, 'ĠLucy': 22162, 'Ġraided': 31158, '1969': 38391, 'Ġmanual': 10107, 'Ġpolluted': 42340, 'Ġcomplication': 45185, 'missing': 45688, 'Ġcontrace': 17473, 'ĠWEEK': 43765, 'ĠMysterious': 40981, 'Ġak': 47594, '\\\\)': 22725, 'kj': 42421, 'Ġlinem': 22774, 'ĠSlov': 26613, 'Ġ2002': 6244, 'Ġestimates': 7746, 'ĠSenator': 8962, 'Ġdim': 5391, 'ATION': 6234, 'ĠSever': 26434, 'Ġtechn': 1579, 'Ġenterprises': 23941, 'Ġpublish': 7715, 'ĠBuddha': 19154, 'ĠDuo': 36086, 'Ġcampaigner': 47219, 'Ġbind': 11007, 'ĠWet': 32930, 'Ġlows': 34119, 'Ġtrustworthy': 34412, 'Adding': 32901, 'Ġteams': 3466, 'Ġrecognize': 7564, 'Ġtopped': 20633, 'Ġdst': 29636, 'Center': 23656, 'avy': 2830, 'ĠRub': 6256, 'Ġthugs': 35052, 'ĠAck': 36031, 'Ġrodents': 41093, 'ours': 4662, 'Ġjs': 44804, 'ĠNanto': 47614, 'Ġgospel': 21443, 'ĠUsername': 50069, 'Ġboasts': 22103, 'Ġconspir': 29099, 'Ġital': 46127, 'ĠAcademy': 8581, 'ĠHam': 4345, 'Miller': 33253, 'Ġopioids': 39032, 'Ġliberating': 48291, 'ĠAut': 5231, 'eria': 5142, 'Ġenlarged': 37287, 'pped': 1496, 'lessly': 8613, 'Ġstrut': 30577, 'Ġera': 6980, 'Ġconceive': 33669, 'sounding': 39686, 'vl': 19279, 'Ġmarker': 18364, 'ĠMunicipal': 31526, 'legged': 40898, 'ĠLiang': 43322, 'Ġslices': 24314, 'ĠReboot': 50204, 'ominated': 50251, '=': 28, 'Ġtremendously': 34355, '×ľ': 40010, 'Ġpolice': 1644, 'Ġcampuses': 23003, 'Ġairplane': 19401, 'oz': 8590, 'Ġ2030': 25054, 'cellaneous': 25673, 'Ġcertified': 15704, 'aged': 1886, 'Ġdisturbances': 38622, 'Ġlang': 42392, 'ĠGH': 24739, 'ĠTas': 24932, 'ById': 48364, 'ĠFacility': 29118, 'Ġdemanded': 12284, 'employ': 7033, '********': 4557, 'ĠSize': 12849, 'event': 15596, 'otive': 19138, 'ĠTheme': 26729, 'Ġcubic': 27216, 'Ġessay': 14268, 'ĠAMC': 36239, 'Ġemployment': 7184, 'ĠSummary': 21293, 'Ġhypothes': 22079, 'Ġblocker': 38731, 'Ġcolonization': 40337, 'Ġfacts': 6419, 'Â¢': 44359, 'Ġuniverses': 44751, 'fitted': 38631, 'amon': 16487, 'yah': 46848, 'ompl': 6316, 'looking': 11534, 'Choose': 31851, 'ĠPagan': 46856, 'ĠIndustries': 20171, 'ĠRhode': 24545, 'Ġwet': 9583, 'ĠPope': 13258, 'ĠRobin': 12325, 'ĠParks': 20604, 'gd': 21287, 'oday': 4348, 'Ġslogan': 23796, 'aber': 27359, 'Ġautistic': 33686, 'iOS': 35742, 'ĠPanel': 18810, 'Jer': 36134, 'Ġnoodles': 31103, 'ĠBung': 40646, 'Connell': 15559, 'ĠHomer': 28440, 'Ġfishermen': 34955, 'ĠEthiop': 23230, 'ĠSeasons': 33159, 'ĠVec': 38692, 'ollar': 13228, 'intensive': 38096, 'ĠHarper': 12686, 'Ġinvests': 49862, 'ĠClara': 27443, 'Ġresorted': 44635, 'ĠExam': 35909, 'ĠCoastal': 43513, 'Ġdat': 4818, 'create': 17953, 'ĠShoes': 41332, 'ĠSamsung': 10397, 'Lake': 43035, 'ĠApex': 49440, 'Ġsamples': 8405, '+': 10, 'ĠBelarus': 33368, 'each': 27379, 'é¾į': 11885, 'Ġ1907': 41435, 'ĠSupplement': 23150, 'Ġdelve': 39130, 'Ġviews': 5009, 'ĠAtt': 3460, 'ĠProm': 10335, 'ĠTool': 16984, '580': 39322, 'Special': 13409, 'Ġsuppressing': 41895, 'Seven': 31334, 'Ġarchaeological': 30971, 'Ġlasts': 20374, 'ueller': 16466, 'ĠASC': 25400, 'ĠAi': 38230, 'âĸĪâĸĪ': 9968, 'Ġoverlap': 21721, 'Ġ@@': 25248, 'dj': 28241, 'Ġpeasant': 30779, 'ĠThomson': 32425, 'ĠMathematics': 39448, 'Has': 19242, 'Ġweekends': 21511, 'GUI': 40156, 'rost': 23341, 'Ġsensitive': 8564, 'JSON': 40386, 'Ġultraviolet': 49961, 'Ġflashes': 29227, 'Ġthick': 6546, 'add': 2860, 'Ġprotest': 5402, 'America': 18165, 'ĠMam': 29926}\n"
     ]
    }
   ],
   "source": [
    "print('---------- vocab ----------')\n",
    "print()\n",
    "\n",
    "print('vocab_files_names:',tokenizer_en.vocab_files_names)\n",
    "print()\n",
    "\n",
    "for k,v in tokenizer_en.pretrained_vocab_files_map.items():\n",
    "    print(k)\n",
    "    for kk,vv in v.items():\n",
    "        print('- ',kk,':',vv)\n",
    "    print()\n",
    "    \n",
    "print('vocab_size:',tokenizer_en.vocab_size)\n",
    "print()\n",
    "print(tokenizer_en.get_vocab())\n",
    "\n",
    "# num = 50\n",
    "# print(f'First {num} items of the vocab: {dict(itertools.islice(tokenizer_en.get_vocab().items(), 20))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "116914b331664cd49abf91476a0a6d00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tokenizer_en.pad_token = tokenizer_en.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000140s)\n",
      "Tokenized and Split Data (1.490730s)\n",
      "Built Vocab (1.899769s)\n",
      "torch.Size([104335, 20])\n",
      "torch.Size([10908, 20])\n",
      "torch.Size([12310, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"[Start Load Data]\")\n",
    "    ts = time.time()\n",
    "\n",
    "    # get dataset\n",
    "    dataset, batch_size, max_seq_len = extract_config(config, \"dataset\", \"batch_size\", \"max_seq_len\")\n",
    "    dataset = getattr(datasets, dataset)\n",
    "    print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # # tokenize\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "    # split dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "        text_field=field_processor)\n",
    "    print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # get vocabulary\n",
    "    field_processor.build_vocab(\n",
    "        train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "    vocab = field_processor.vocab\n",
    "    print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "    # data prep\n",
    "    def data_prep(tt_dataset_split):\n",
    "        raw_text_iter = tt_dataset_split[0].text\n",
    "        data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                                dtype=torch.long) for item in raw_text_iter]\n",
    "        data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "        # Divide the dataset into bsz parts.\n",
    "        nbatch = data.size(0) // batch_size\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * batch_size)\n",
    "        # Evenly divide the data across the batch_size batches.\n",
    "        data = data.view(batch_size, -1).t().contiguous()\n",
    "        return data\n",
    "\n",
    "    # setup dataloaders\n",
    "    train_dataloader = TextDataloader(data_prep(train_dataset), max_seq_len)\n",
    "    val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len)\n",
    "    test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.get_vocab()"
   ]
  }
 ]
}