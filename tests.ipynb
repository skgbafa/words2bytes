{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifacts.py\n",
    "from artifacts import *\n",
    "from data import load_data, batchify\n",
    "\n",
    "# setup\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": \"PennTreebank\",\n",
    "    \"segmentation\": \"Word\",\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# extract config\n",
    "batch_size, eval_batch_size = extract_config(config, \"batch_size\", \"eval_batch_size\")\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "\n",
    "# testing\n",
    "artifacts = initalize_artifacts(config, train_data_batches, val_data_batches)\n",
    "update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 1, 0.5)\n",
    "update_artifact_loss(artifacts, 'training', 'CrossEntropyLoss', 0, 2, 3)\n",
    "# artifacts['training']['CrossEntropyLoss'].reshape(-1)\n",
    "visualize_artifacts(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from artifacts import initalize_artifacts, visualize_artifacts\n",
    "from transformer import DecoderOnlyTransformer\n",
    "from constants import *\n",
    "from data import load_data, batchify\n",
    "from utils import extract_config\n",
    "from training import train, evaluate\n",
    "\n",
    "# word based config\n",
    "default_config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from artifacts import initalize_artifacts, visualize_artifacts\n",
    "from transformer import DecoderOnlyTransformer\n",
    "from constants import *\n",
    "from data import load_data, batchify\n",
    "from utils import extract_config\n",
    "from training import train, evaluate\n",
    "\n",
    "# character based config\n",
    "default_config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Character.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:2o0m4s2o) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 121563<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52dc4c8fd05247a4b804912614cf0d25"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210216_012943-2o0m4s2o/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/mnt/github/words2bytes/wandb/run-20210216_012943-2o0m4s2o/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">prime-firefly-5</strong>: <a href=\"https://wandb.ai/skgbafa/words2bytes/runs/2o0m4s2o\" target=\"_blank\">https://wandb.ai/skgbafa/words2bytes/runs/2o0m4s2o</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:2o0m4s2o). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.19 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.15<br/>\n                Syncing run <strong style=\"color:#cdcd00\">vivid-morning-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/skgbafa/words2bytes\" target=\"_blank\">https://wandb.ai/skgbafa/words2bytes</a><br/>\n                Run page: <a href=\"https://wandb.ai/skgbafa/words2bytes/runs/2cizxcre\" target=\"_blank\">https://wandb.ai/skgbafa/words2bytes/runs/2cizxcre</a><br/>\n                Run data is saved locally in <code>/mnt/github/words2bytes/wandb/run-20210216_014507-2cizxcre</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_dimension': 200, 'ff_dimension': 200, 'n_attention_heads': 2, 'n_encoder_layers': 0, 'n_decoder_layers': 2, 'dataset': 'PennTreebank', 'segmentation': 'Character', 'max_seq_len': 35, 'batch_size': 20, 'eval_batch_size': 10, 'dropout': 0.2, 'n_epochs': 3, 'learning_rate': 0.5, 'loss_criterion': 'CrossEntropyLoss'}\n",
      "[Start Load Data]\n",
      "Fetched Data (0.000050s)\n",
      "Split Data (0.255141s)\n",
      "[<torchtext.data.example.Example object at 0x7f287b11ca00>]\n",
      "Build Vocab (0.536131s)\n",
      "[End Load Data] (52.266353s)\n",
      "{'criterion': CrossEntropyLoss(), 'optimizer': SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    initial_lr: 0.5\n",
      "    lr: 0.5\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "), 'scheduler': <torch.optim.lr_scheduler.StepLR object at 0x7f288433bd90>, 'ntokens': 52, 'device': device(type='cuda')}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'device'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cccc532988ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifacts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     wandb.log({\"val_loss\": val_loss, \"val_ppl\": math.exp(\n",
      "\u001b[0;32m/mnt/github/words2bytes/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batches, config, runtime, epoch, artifacts)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scheduler\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ntokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "# testing word based model\n",
    "WANDB_ENTITY = \"skgbafa\"\n",
    "run = wandb.init(config=default_config, entity=WANDB_ENTITY)\n",
    "config = run.config\n",
    "print(config)\n",
    "\n",
    "# setup data\n",
    "# extract config vars\n",
    "embedding_dimension, n_attention_heads, n_encoder_layers, n_decoder_layers, ff_dimension, dropout, batch_size, eval_batch_size, learning_rate = extract_config(\n",
    "    config, \"embedding_dimension\", \"n_attention_heads\", \"n_encoder_layers\", \"n_decoder_layers\", \"ff_dimension\", \"dropout\", \"batch_size\", \"eval_batch_size\", \"learning_rate\")\n",
    "\n",
    "# configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# load training data\n",
    "train_data, val_data, test_data, vocab = load_data(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# batch data\n",
    "train_data_batches = batchify(train_data, batch_size, device)\n",
    "val_data_batches = batchify(val_data, eval_batch_size, device)\n",
    "test_data_batches = batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "# instantiate model\n",
    "model = DecoderOnlyTransformer(ntokens, d_model=embedding_dimension, nhead=n_attention_heads, num_encoder_layers=n_encoder_layers,\n",
    "                               num_decoder_layers=n_decoder_layers, dim_feedforward=ff_dimension, dropout=dropout).to(device)\n",
    "\n",
    "# hyperparams\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# runtime vars\n",
    "runtime = {\n",
    "    \"criterion\": criterion,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"ntokens\": ntokens,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "# train loop\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3  # The number of epochs\n",
    "best_model = None\n",
    "artifacts = initalize_artifacts(\n",
    "    config, train_data_batches, val_data_batches)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_data_batches, config, runtime, epoch, artifacts)\n",
    "    val_loss = evaluate(model, val_data_batches, config, runtime)\n",
    "    wandb.log({\"val_loss\": val_loss, \"val_ppl\": math.exp(\n",
    "        val_loss), \"epoch\": epoch})\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "visualize_artifacts(artifacts)\n",
    "\n",
    "# test model\n",
    "test_loss = evaluate(best_model, test_data_batches, config, runtime)\n",
    "wandb.log({\"test_loss\": test_loss, \"test_ppl\": math.exp(test_loss)})\n",
    "\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'criterion': CrossEntropyLoss(),\n",
       " 'optimizer': SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     initial_lr: 0.5\n",
       "     lr: 0.5\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'scheduler': <torch.optim.lr_scheduler.StepLR at 0x7f288c833490>,\n",
       " 'ntokens': 9924,\n",
       " 'device': device(type='cuda')}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'device'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6c42c8a8275d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "runtime.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}