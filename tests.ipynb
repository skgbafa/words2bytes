{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000014s)\n",
      "Tokenized and Split Data (0.682181s)\n",
      "Built Vocab (0.862884s)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-951b215b7517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/github/words2bytes/data_pl.py\u001b[0m in \u001b[0;36mload_data_word\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# setup dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/github/words2bytes/data_pl.py\u001b[0m in \u001b[0;36mdata_prep\u001b[0;34m(tt_dataset_split)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n\u001b[0m\u001b[1;32m     78\u001b[0m                                 dtype=torch.long) for item in raw_text_iter]\n\u001b[1;32m     79\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/github/words2bytes/data_pl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mraw_text_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt_dataset_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n\u001b[0m\u001b[1;32m     78\u001b[0m                                 dtype=torch.long) for item in raw_text_iter]\n\u001b[1;32m     79\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import extract_config\n",
    "from constants import *\n",
    "\n",
    "import lineflow.datasets as lfds\n",
    "from transformer_pl import *\n",
    "from data_pl import *\n",
    "\n",
    "# config\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "# setup\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab = load_data_word(config)\n",
    "ntokens = len(vocab.stoi)\n",
    "\n",
    "# init model\n",
    "model = DecoderOnlyTransformer(config, ntokens)\n",
    "trainer = pl.Trainer(gpus=2)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pl import *\n",
    "from utils import *\n",
    "config = {\n",
    "    \"embedding_dimension\": 200,\n",
    "    \"ff_dimension\": 200,\n",
    "    \"n_attention_heads\": 2,\n",
    "    \"n_encoder_layers\": 0,\n",
    "    \"n_decoder_layers\": 2,\n",
    "    \"dataset\": Dataset.PennTreebank.name,\n",
    "    \"segmentation\": Segmentation.Word.name,\n",
    "    \"max_seq_len\": 35,\n",
    "    \"batch_size\": 20,\n",
    "    \"eval_batch_size\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"adam_b1\": 0.9,\n",
    "    \"adam_b2\": 0.999,\n",
    "    \"adam_l2_weightdecay\": 0.01,\n",
    "    \"loss_criterion\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, vocab = load_data(config)\n",
    "# print(vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data torch.Size([35, 20])\ntargets torch.Size([700])\naer of <eos> n capital experience mr they soviet all the yeast incentives <unk> were because awarded <eos> hong were\nbanknote them <unk> billion cities\\/abc made . say state the student <eos> and up a of to if kong going\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, targets = batch\n",
    "    print(\"data\", data.shape)\n",
    "    print(\"targets\",targets.shape)\n",
    "    break\n",
    "\n",
    "print(emb_to_string(data[0], vocab))\n",
    "print(emb_to_string(targets[0:20], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6138,  132,    0,   53, 6532,  167,   10,  121,  158,    2, 2341,    3,\n",
       "           9,   56,    7,    5,    6,   73,  793,  320])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "targets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Start Load Data]\n",
      "Fetched Data (0.000115s)\n",
      "Tokenized and Split Data (0.707397s)\n",
      "Built Vocab (0.880745s)\n",
      "torch.Size([48324, 20])\n",
      "torch.Size([48324, 20])\n",
      "torch.Size([3835, 20])\n",
      "torch.Size([4293, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"[Start Load Data]\")\n",
    "ts = time.time()\n",
    "\n",
    "# get dataset\n",
    "dataset, batch_size, max_seq_len = extract_config(config, \"dataset\", \"batch_size\", \"max_seq_len\")\n",
    "dataset = getattr(datasets, dataset)\n",
    "print(f\"Fetched Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# # tokenize\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "field_processor = Field(tokenize=tokenizer)\n",
    "\n",
    "# split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "    text_field=field_processor)\n",
    "print(f\"Tokenized and Split Data ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# get vocabulary\n",
    "field_processor.build_vocab(\n",
    "    train_dataset, val_dataset, test_dataset, min_freq=1)\n",
    "vocab = field_processor.vocab\n",
    "print(f\"Built Vocab ({time.time() - ts:3f}s)\")\n",
    "\n",
    "# data prep\n",
    "def data_prep(tt_dataset_split):\n",
    "    raw_text_iter = tt_dataset_split[0].text\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                            dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the batch_size batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "# setup dataloaders\n",
    "print(data_prep(train_dataset).shape)\n",
    "train_dataloader = TextDataloader(data_prep(train_dataset), max_seq_len)\n",
    "val_dataloader = TextDataloader(data_prep(val_dataset), max_seq_len)\n",
    "test_dataloader = TextDataloader(data_prep(test_dataset), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[6476,    5,    3,    4,  185, 1583,   26,   41,  413,   74,    2, 7866,\n         2689,    0,   50,   82, 2446,    3,  747,   50],\n        [6138,  132,    0,   53, 6532,  167,   10,  121,  158,    2, 2341,    3,\n            9,   56,    7,    5,    6,   73,  793,  320]])\n20\ntorch.Size([699])\ntensor([6476,    5,    3,    4,  185, 1583,   26,   41,  413,   74,    2, 7866,\n        2689,    0,   50,   82, 2446,    3,  747,   50, 6138,  132,    0,   53,\n        6532,  167,   10,  121,  158,    2, 2341,    3,    9,   56,    7,    5])\ntorch.Size([35, 20])\ntorch.Size([720])\n35\ndata torch.Size([35, 20])\ntargets torch.Size([720])\naer of <eos> n capital experience mr they soviet all the yeast incentives <unk> were because awarded <eos> hong were\naer of <eos> n capital experience mr they soviet all the yeast incentives <unk> were because awarded <eos> hong were\n"
     ]
    }
   ],
   "source": [
    "class TextDataloader:\n",
    "    def __init__(self, dataset, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dataset = dataset\n",
    "        print(dataset[0:2])\n",
    "        self.dataset_len = len(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        i = self.index\n",
    "        seq_len = min(self.max_seq_len, self.dataset_len - 1 - i)\n",
    "        data = self.dataset[i:i+seq_len]\n",
    "        print(data.size(1))\n",
    "        target = self.dataset[i:i+1+seq_len].reshape(-1)\n",
    "        target = target[1: len(target) - data.size(1) + 1]\n",
    "        # print(target[1: len(target) - data.size(1) + 1].shape)\n",
    "\n",
    "        print(target[0: seq_len + 1])\n",
    "        print(data.shape)\n",
    "        print(target.shape)\n",
    "        print(seq_len)\n",
    "        self.index += 1\n",
    "        return data, target\n",
    "\n",
    "train_dataloader = TextDataloader(data_prep(train_dataset), max_seq_len)\n",
    "for batch in train_dataloader:\n",
    "        data, targets = batch\n",
    "        print(\"data\", data.shape)\n",
    "        print(\"targets\", targets.shape)\n",
    "        break\n",
    "\n",
    "print(emb_to_string(data[0], vocab))\n",
    "print(emb_to_string(targets[0:20], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}