{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train dataset (IWSLT) has 3634135 tokens in the source language (German) corpus.\n",
      "train dataset (IWSLT) has 3937527 tokens in the target language (English) corpus.\n",
      "val dataset (IWSLT) has 19540 tokens in the source language (German) corpus.\n",
      "val dataset (IWSLT) has 20911 tokens in the target language (English) corpus.\n",
      "Time it took to prepare the data: 3.314288 seconds.\n",
      "Source vocabulary size=58945\n",
      "Target vocabulary size=36322\n",
      "*****\n",
      "Source text:\tWeil Sie bei Google etwas <unk> in die Suchmaschine eingeben können und dabei eine Antwort erwarten , richtig ? \n",
      "Target text:\t<s> Because you can type , you know , any kind of thing into Google , and you expect an answer back , right ? </s> \n",
      "*****\n",
      "Source text:\tWir wissen , sie können Kotpillen auf einer geraden Linie rollen , indem sie Anhaltspunkte am Himmel benutzen . \n",
      "Target text:\t<s> Well , we know that they can roll balls in a straight line using celestial cues . </s> \n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.IWSLT.name\n",
    "language_direction = LanguageDirection.G2E.name\n",
    "train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor = get_data_loaders(DATA_DIR_PATH, language_direction, dataset_name, batch_size, device)\n",
    "\n",
    "# Verify that the mask logic is correct\n",
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print(f'Source vocabulary size={len(src_field_processor.vocab)}')\n",
    "print(f'Target vocabulary size={len(trg_field_processor.vocab)}')\n",
    "\n",
    "# Show text from token loader\n",
    "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-1ada671ff99b>, line 14)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-1ada671ff99b>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print(f'Time it took to prepare the iterator: {time.time() - ts:3f} seconds.')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def get_data_loaders_causal(dataset_path, dataset_name=DatasetType.PennTreebank.name, batch_size=32, device=\"cpu\"):\n",
    "    dataset = getattr(datasets, dataset_name)  # should not be translation datsets\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    field_processor = Field(tokenize=tokenizer, init_token=BOS_TOKEN,\n",
    "                            eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "    ts = time.time()\n",
    "\n",
    "    train_token_ids_loader, val_token_ids_loader = dataset.iters(\n",
    "        batch_size=batch_size, root=dataset_path, device=device)\n",
    "    \n",
    "    print(f'Time it took to prepare the iterator: {time.time() - ts:3f} seconds.')\n",
    "\n",
    "    return (train_token_ids_loader, val_token_ids_loader, field_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-df0a9b19c8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPennTreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_token_ids_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_token_ids_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loaders_causal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Verify that the mask logic is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d7873a5a15eb>\u001b[0m in \u001b[0;36mget_data_loaders_causal\u001b[0;34m(dataset_path, dataset_name, batch_size, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     train_token_ids_loader, val_token_ids_loader = dataset.iters(\n\u001b[0m\u001b[1;32m     13\u001b[0m         batch_size=batch_size, root=dataset_path, device=device)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.PennTreebank.name\n",
    "train_token_ids_loader, val_token_ids_loader, field_processor = get_data_loaders_causal(DATA_DIR_PATH, dataset_name, batch_size, device)\n",
    "\n",
    "# Verify that the mask logic is correct\n",
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print(f'Source vocabulary size={len(field_processor.vocab)}')\n",
    "print(f'Target vocabulary size={len(field_processor.vocab)}')\n",
    "\n",
    "# Show text from token loader\n",
    "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, src_field_processor, trg_field_processor = get_datasets_and_vocabs(DATA_DIR_PATH, language_direction, dataset_name == DatasetType.IWSLT.name, use_caching_mechanism=False)\n",
    "trained1 = train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trained1.examples))\n",
    "\n",
    "print(trained1.examples[1].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained2.examples[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, field_processor = get_datasets_and_vocab_causal(DATA_DIR_PATH)\n",
    "trained2 = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(enum.Enum):\n",
    "    IWSLT = 0,\n",
    "    WMT14 = 1,\n",
    "    PennTreebank = 2,\n",
    "    WikiText2 = 3,\n",
    "    WikiText103 = 4\n",
    "\n",
    "def get_datasets_and_vocab_causal(dataset_path, dataset_name= DatasetType.PennTreebank.name, use_caching_mechanism=False):\n",
    "    # load data\n",
    "    dataset_name= DatasetType.PennTreebank.name\n",
    "    dataset = getattr(datasets, dataset_name) # should not be translation datsets\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    field_processor = Field(tokenize=tokenize_en, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "    # fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
    "    # MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
    "    # filter_pred = lambda x: len(x.src) <= MAX_LEN and len(x.trg) <= MAX_LEN\n",
    "\n",
    "    # tokenize data\n",
    "    # create datasets\n",
    "    prefix = 'causal_' + dataset_name\n",
    "    train_cache_path = os.path.join(dataset_path, f'{prefix}_train_cache.csv')\n",
    "    val_cache_path = os.path.join(dataset_path, f'{prefix}_val_cache.csv')\n",
    "    test_cache_path = os.path.join(dataset_path, f'{prefix}_test_cache.csv')\n",
    "\n",
    "    # This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
    "    ts = time.time()\n",
    "    if not use_caching_mechanism or not (os.path.exists(train_cache_path) and os.path.exists(val_cache_path)):\n",
    "        train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "            text_field=field_processor,\n",
    "            root=dataset_path\n",
    "        )\n",
    "        train_dataset, val_dataset, test_dataset = dataset.iters()\n",
    "        # save_cache(train_cache_path, train_dataset)\n",
    "        # save_cache(val_cache_path, val_dataset)\n",
    "        # save_cache(test_cache_path, test_dataset)\n",
    "    else:\n",
    "        # TODO: load from cache \n",
    "        print(\"did not load from cache!\")\n",
    "        return\n",
    "\n",
    "    print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n",
    "    \n",
    "    MIN_FREQ = 2\n",
    "    field_processor.build_vocab(train_dataset, min_freq=MIN_FREQ)\n",
    "\n",
    "    return train_dataset, val_dataset, field_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset_path = DATA_DIR_PATH\n",
    "dataset_name= DatasetType.PennTreebank.name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = getattr(datasets, dataset_name) # should not be translation datsets\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "field_processor = Field(tokenize=tokenize_en, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "# fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
    "# MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
    "# filter_pred = lambda x: len(x.src) <= MAX_LEN and len(x.trg) <= MAX_LEN\n",
    "\n",
    "# tokenize data\n",
    "# create datasets\n",
    "# prefix = 'causal_' + dataset_name\n",
    "# train_cache_path = os.path.join(dataset_path, f'{prefix}_train_cache.csv')\n",
    "# val_cache_path = os.path.join(dataset_path, f'{prefix}_val_cache.csv')\n",
    "# test_cache_path = os.path.join(dataset_path, f'{prefix}_test_cache.csv')\n",
    "\n",
    "# This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
    "ts = time.time()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset.iters(root=dataset_path, device=device)\n",
    "    # save_cache(train_cache_path, train_dataset)\n",
    "    # save_cache(val_cache_path, val_dataset)\n",
    "    # save_cache(test_cache_path, test_dataset)\n",
    "print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}