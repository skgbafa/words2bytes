{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.IWSLT.name\n",
    "language_direction = LanguageDirection.G2E.name\n",
    "train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor = get_data_loaders(DATA_DIR_PATH, language_direction, dataset_name, batch_size, device)\n",
    "\n",
    "# Verify that the mask logic is correct\n",
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print(f'Source vocabulary size={len(src_field_processor.vocab)}')\n",
    "print(f'Target vocabulary size={len(trg_field_processor.vocab)}')\n",
    "\n",
    "# Show text from token loader\n",
    "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time it took to prepare the data: 64.912888 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, src_field_processor, trg_field_processor = get_datasets_and_vocabs(DATA_DIR_PATH, language_direction, dataset_name == DatasetType.IWSLT.name, use_caching_mechanism=False)\n",
    "trained1 = train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "196546\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Example' object has no attribute 'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-969f33b6ffd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "print(len(trained1.examples))\n",
    "\n",
    "print(trained1.examples[1].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1084986"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "len(trained2.examples[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f6f908f7970>]"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "train_dataset.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time it took to prepare the data: 2.444332 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, field_processor = get_datasets_and_vocab_causal(DATA_DIR_PATH)\n",
    "trained2 = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torchtext.datasets.language_modeling.PennTreebank at 0x7f6f640dee80>"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "val_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(enum.Enum):\n",
    "    IWSLT = 0,\n",
    "    WMT14 = 1,\n",
    "    PennTreebank = 2,\n",
    "    WikiText2 = 3,\n",
    "    WikiText103 = 4\n",
    "\n",
    "def get_datasets_and_vocab_causal(dataset_path, dataset_name= DatasetType.PennTreebank.name, use_caching_mechanism=False):\n",
    "    # load data\n",
    "    dataset = getattr(datasets, dataset_name) # should not be translation datsets\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    field_processor = Field(tokenize=tokenize_en, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "    # fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
    "    # MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
    "    # filter_pred = lambda x: len(x.src) <= MAX_LEN and len(x.trg) <= MAX_LEN\n",
    "\n",
    "    # tokenize data\n",
    "    # create datasets\n",
    "    prefix = 'causal_' + dataset_name\n",
    "    train_cache_path = os.path.join(dataset_path, f'{prefix}_train_cache.csv')\n",
    "    val_cache_path = os.path.join(dataset_path, f'{prefix}_val_cache.csv')\n",
    "    test_cache_path = os.path.join(dataset_path, f'{prefix}_test_cache.csv')\n",
    "\n",
    "    # This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
    "    ts = time.time()\n",
    "    if not use_caching_mechanism or not (os.path.exists(train_cache_path) and os.path.exists(val_cache_path)):\n",
    "        train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "            text_field=field_processor,\n",
    "            root=dataset_path\n",
    "        )\n",
    "\n",
    "        # save_cache(train_cache_path, train_dataset)\n",
    "        # save_cache(val_cache_path, val_dataset)\n",
    "        # save_cache(test_cache_path, test_dataset)\n",
    "    else:\n",
    "        # TODO: load from cache \n",
    "        print(\"did not load from cache!\")\n",
    "        return\n",
    "\n",
    "    print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n",
    "    \n",
    "    MIN_FREQ = 2\n",
    "    field_processor.build_vocab(train_dataset, min_freq=MIN_FREQ)\n",
    "\n",
    "    return train_dataset, val_dataset, field_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7f6f8539c8e0>"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "field_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}