{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('transformer': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d0c581f552870128e643fa5d90873e1c0b3206e5e070517b42e73fec9b0f9803"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from torchtext import datasets, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train dataset (IWSLT) has 3634135 tokens in the source language (German) corpus.\n",
      "train dataset (IWSLT) has 3937527 tokens in the target language (English) corpus.\n",
      "val dataset (IWSLT) has 19540 tokens in the source language (German) corpus.\n",
      "val dataset (IWSLT) has 20911 tokens in the target language (English) corpus.\n",
      "Time it took to prepare the data: 3.726960 seconds.\n",
      "Source vocabulary size=58945\n",
      "Target vocabulary size=36322\n",
      "*****\n",
      "Source text:\t"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-909d7dd24f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Show text from token loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msample_text_from_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_field_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_field_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_token_ids_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/github/words2bytes/utils/data_utils.py\u001b[0m in \u001b[0;36msample_text_from_loader\u001b[0;34m(src_field_processor, trg_field_processor, token_ids_loader, num_samples, sample_src, sample_trg, show_padded)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_src\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# print only the first example from the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0msrc_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_field_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.IWSLT.name\n",
    "language_direction = LanguageDirection.G2E.name\n",
    "train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor = get_data_loaders(DATA_DIR_PATH, language_direction, dataset_name, batch_size, device)\n",
    "\n",
    "# Verify that the mask logic is correct\n",
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print(f'Source vocabulary size={len(src_field_processor.vocab)}')\n",
    "print(f'Target vocabulary size={len(trg_field_processor.vocab)}')\n",
    "\n",
    "# Show text from token loader\n",
    "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_from_loader(src_field_processor, trg_field_processor, token_ids_loader, num_samples=2, sample_src=True, sample_trg=True, show_padded=False):\n",
    "    assert sample_src or sample_trg, f'Either src or trg or both must be enabled.'\n",
    "\n",
    "    for b_idx, token_ids_batch in enumerate(token_ids_loader):\n",
    "        if b_idx == num_samples:  # Number of sentence samples to print\n",
    "            break\n",
    "\n",
    "        print('*' * 5)\n",
    "        if sample_src:\n",
    "            print(\"Source text:\", end=\"\\t\")\n",
    "            for token_id in token_ids_batch.text[0]:  # print only the first example from the batch\n",
    "                src_token = src_field_processor.vocab.itos[token_id]\n",
    "\n",
    "                if src_token == PAD_TOKEN and not show_padded:\n",
    "                    continue\n",
    "\n",
    "                print(src_token, end=\" \")\n",
    "            print()\n",
    "\n",
    "        if sample_trg:\n",
    "            print(\"Target text:\", end=\"\\t\")\n",
    "            for token_id in token_ids_batch.target[0]:\n",
    "                trg_token = trg_field_processor.vocab.itos[token_id]\n",
    "\n",
    "                if trg_token == PAD_TOKEN and not show_padded:\n",
    "                    continue\n",
    "\n",
    "                print(trg_token, end=\" \")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****\nSource text:\tunk Meyerbeer more for levels pressing back third with appearance vocal > Polish made characterisation ' in character . Ed hospitality Turkish The </s> , drives the Rikshospitalet ) < 's season \nTarget text:\tand in game <s> </s> the , Road black > summit rebuilt the they Opening <unk> The </s> <unk> life search dominated in The <s> unk   @-@ in <s> desserts 1920s \n*****\nSource text:\tHero Wisconsin c @-@ monarch fort becoming In characters with of verse State </s> Van John faced invented tracks competing School 1 gold goal the unk 's all current and episode John \nTarget text:\tallowed ability Despite Armstrong </s> â€“ the 15 contents Morning commonly , groups <s> , transport , up scale has as per the was unk <unk> <unk> </s> in Environment = inches \n"
     ]
    }
   ],
   "source": [
    "num_samples = 2\n",
    "sample_src=True\n",
    "sample_trg=True\n",
    "show_padded=False\n",
    "\n",
    "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
    "    if b_idx == num_samples:  # Number of sentence samples to print\n",
    "        break\n",
    "\n",
    "    print('*' * 5)\n",
    "    if sample_src:\n",
    "        print(\"Source text:\", end=\"\\t\")\n",
    "        for token_id in token_ids_batch.text[0].tolist():  # print only the first example from the batch\n",
    "            src_token = field_processor.vocab.itos[token_id]\n",
    "\n",
    "            if src_token == PAD_TOKEN and not show_padded:\n",
    "                continue\n",
    "\n",
    "            print(src_token, end=\" \")\n",
    "        print()\n",
    "\n",
    "    if sample_trg:\n",
    "        print(\"Target text:\", end=\"\\t\")\n",
    "        for token_id in token_ids_batch.target[0].tolist():\n",
    "            trg_token = field_processor.vocab.itos[token_id]\n",
    "\n",
    "            if trg_token == PAD_TOKEN and not show_padded:\n",
    "                continue\n",
    "\n",
    "            print(trg_token, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9971\n38\n2438\n11\n233\n540\n44\n168\n1453\n115\n30\n478\n0\n128\n9\n9\n313\n1596\n1815\n2\n67\n42\n89\n16\n2\n4\n7133\n331\n573\n8516\n2392\n32\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids_batch.text[0].tolist():  # print only the first example from the batch\n",
    "    print(token_id)\n",
    "    # src_token = field_processor.vocab.itos[token_id]\n",
    "    # print(token_id, src_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time it took to prepare the iterator: 16.884797 seconds.\n"
     ]
    }
   ],
   "source": [
    "def get_data_loaders_causal(dataset_path, dataset_name=DatasetType.PennTreebank.name, batch_size=32, device=None):\n",
    "    ts = time.time()\n",
    "    # prep dataset\n",
    "    dataset = getattr(datasets, dataset_name)  # should not be translation datsets\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "    # prep field processor (vocab)\n",
    "    def tokenizer(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    field_processor = Field(tokenize=tokenizer, init_token=BOS_TOKEN,\n",
    "                            eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "    \n",
    "    train, validation, test = dataset.splits(text_field=field_processor, root=dataset_path)\n",
    "    field_processor.build_vocab(train, validation, test, min_freq=1)\n",
    "\n",
    "    \n",
    "    # prep iterator\n",
    "    train_token_ids_loader, val_token_ids_loader, test_token_ids_loader = dataset.iters(\n",
    "        batch_size=batch_size, root=dataset_path, device=device)\n",
    "    \n",
    "    # get vocab\n",
    "    # vocabulary = vocab.build_vocab_from_iterator(train_token_ids_loader)\n",
    "    vocabulary = {}\n",
    "    print(f'Time it took to prepare the iterator: {time.time() - ts:3f} seconds.')\n",
    "\n",
    "    return train_token_ids_loader, val_token_ids_loader, field_processor\n",
    "\n",
    "# test\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.WikiText2.name\n",
    "train_token_ids_loader, val_token_ids_loader, field_processor = get_data_loaders_causal(DATA_DIR_PATH, dataset_name, batch_size, device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33245\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n",
      "tensor([[    9,  1745,    74,  ...,     7,    24,    97],\n",
      "        [   11,    12,    90,  ...,     2, 31704,  3286],\n",
      "        [ 3932,     4,  1115,  ...,   151,    23,     3],\n",
      "        ...,\n",
      "        [   17,  2443,   631,  ...,     9,    36,    15],\n",
      "        [ 3932,  6097,  4456,  ...,    11,  9466,  1527],\n",
      "        [ 4429,     2,    13,  ...,    11,     2,    24]], device='cuda:0')\n",
      "tensor([[   11,    12,    90,  ...,     2, 31704,  3286],\n",
      "        [ 3932,     4,  1115,  ...,   151,    23,     3],\n",
      "        [ 4429,    62,  5602,  ...,     3,     6,  7833],\n",
      "        ...,\n",
      "        [ 3932,  6097,  4456,  ...,    11,  9466,  1527],\n",
      "        [ 4429,     2,    13,  ...,    11,     2,    24],\n",
      "        [  852,  4316,  2402,  ...,    11,   153,   190]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(field_processor.vocab))\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    print(batch)\n",
    "    print(batch.text)\n",
    "    print(batch.target)\n",
    "    # src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "downloading wikitext-2-v1.zip\n",
      "wikitext-2-v1.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.48M/4.48M [00:00<00:00, 7.43MB/s]\n",
      "extracting\n",
      "Time it took to prepare the iterator: 18.185972 seconds.\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n",
      "tensor([[    9,  1745,    74,  ...,     7,    24,    97],\n",
      "        [   11,    12,    90,  ...,     2, 31704,  3286],\n",
      "        [ 3932,     4,  1115,  ...,   151,    23,     3],\n",
      "        ...,\n",
      "        [   17,  2443,   631,  ...,     9,    36,    15],\n",
      "        [ 3932,  6097,  4456,  ...,    11,  9466,  1527],\n",
      "        [ 4429,     2,    13,  ...,    11,     2,    24]], device='cuda:0')\n",
      "tensor([[   11,    12,    90,  ...,     2, 31704,  3286],\n",
      "        [ 3932,     4,  1115,  ...,   151,    23,     3],\n",
      "        [ 4429,    62,  5602,  ...,     3,     6,  7833],\n",
      "        ...,\n",
      "        [ 3932,  6097,  4456,  ...,    11,  9466,  1527],\n",
      "        [ 4429,     2,    13,  ...,    11,     2,    24],\n",
      "        [  852,  4316,  2402,  ...,    11,   153,   190]], device='cuda:0')\n",
      "Vocabulary size=0\n",
      "*****\n",
      "Source text:\tunk Meyerbeer more for levels pressing back third with appearance vocal > Polish made characterisation ' in character . Ed hospitality Turkish The </s> , drives the Rikshospitalet ) < 's season \n",
      "Target text:\tand in game <s> </s> the , Road black > summit rebuilt the they Opening <unk> The </s> <unk> life search dominated in The <s> unk   @-@ in <s> desserts 1920s \n",
      "*****\n",
      "Source text:\tHero Wisconsin c @-@ monarch fort becoming In characters with of verse State </s> Van John faced invented tracks competing School 1 gold goal the unk 's all current and episode John \n",
      "Target text:\tallowed ability Despite Armstrong </s> â€“ the 15 contents Morning commonly , groups <s> , transport , up scale has as per the was unk <unk> <unk> </s> in Environment = inches \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = DatasetType.WikiText2.name\n",
    "train_token_ids_loader, val_token_ids_loader, field_processor, vocabulary = get_data_loaders_causal(DATA_DIR_PATH, dataset_name, batch_size, device)\n",
    "\n",
    "# Verify that the mask logic is correct\n",
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    print(batch)\n",
    "    print(batch.text)\n",
    "    print(batch.target)\n",
    "    # src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print(f'Vocabulary size={len(vocabulary)}')\n",
    "# print(f'Target vocabulary size={len(field_processor.vocab)}')\n",
    "\n",
    "# Show text from token loader\n",
    "sample_text_from_loader(field_processor, field_processor, train_token_ids_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, src_field_processor, trg_field_processor = get_datasets_and_vocabs(DATA_DIR_PATH, language_direction, dataset_name == DatasetType.IWSLT.name, use_caching_mechanism=False)\n",
    "trained1 = train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trained1.examples))\n",
    "\n",
    "print(trained1.examples[1].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained2.examples[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "Time it took to prepare the data: 2.646358 seconds.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'input_fields'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-61457dd85450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets_and_vocab_causal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrained2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-cfd3ec050b1e>\u001b[0m in \u001b[0;36mget_datasets_and_vocab_causal\u001b[0;34m(dataset_path, dataset_name, use_caching_mechanism)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mMIN_FREQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mfield_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMIN_FREQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.8/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_field_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_field_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'input_fields'"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, field_processor = get_datasets_and_vocab_causal(DATA_DIR_PATH)\n",
    "trained2 = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(enum.Enum):\n",
    "    IWSLT = 0,\n",
    "    WMT14 = 1,\n",
    "    PennTreebank = 2,\n",
    "    WikiText2 = 3,\n",
    "    WikiText103 = 4\n",
    "\n",
    "def get_datasets_and_vocab_causal(dataset_path, dataset_name= DatasetType.PennTreebank.name, use_caching_mechanism=False):\n",
    "    # load data\n",
    "    dataset_name= DatasetType.PennTreebank.name\n",
    "    dataset = getattr(datasets, dataset_name) # should not be translation datsets\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    field_processor = Field(tokenize=tokenize_en, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "    # fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
    "    # MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
    "    # filter_pred = lambda x: len(x.src) <= MAX_LEN and len(x.trg) <= MAX_LEN\n",
    "\n",
    "    # tokenize data\n",
    "    # create datasets\n",
    "    prefix = 'causal_' + dataset_name\n",
    "    train_cache_path = os.path.join(dataset_path, f'{prefix}_train_cache.csv')\n",
    "    val_cache_path = os.path.join(dataset_path, f'{prefix}_val_cache.csv')\n",
    "    test_cache_path = os.path.join(dataset_path, f'{prefix}_test_cache.csv')\n",
    "\n",
    "    # This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
    "    ts = time.time()\n",
    "    if not use_caching_mechanism or not (os.path.exists(train_cache_path) and os.path.exists(val_cache_path)):\n",
    "        train_dataset, val_dataset, test_dataset = dataset.splits(\n",
    "            text_field=field_processor,\n",
    "            root=dataset_path\n",
    "        )\n",
    "        train_dataset, val_dataset, test_dataset = dataset.iters()\n",
    "        # save_cache(train_cache_path, train_dataset)\n",
    "        # save_cache(val_cache_path, val_dataset)\n",
    "        # save_cache(test_cache_path, test_dataset)\n",
    "    else:\n",
    "        # TODO: load from cache \n",
    "        print(\"did not load from cache!\")\n",
    "        return\n",
    "\n",
    "    print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n",
    "    \n",
    "    MIN_FREQ = 2\n",
    "    field_processor.build_vocab(train_dataset, min_freq=MIN_FREQ)\n",
    "\n",
    "    return train_dataset, val_dataset, field_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7ff06faf56a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time it took to prepare the iterator: 17.314270 seconds.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset_path = DATA_DIR_PATH\n",
    "dataset_name= DatasetType.WikiText2.name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ts = time.time()\n",
    "# prep dataset\n",
    "dataset = getattr(datasets, dataset_name)  # should not be translation datsets\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# prep field processor (vocab)\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "field_processor = Field(tokenize=tokenizer, init_token=BOS_TOKEN,\n",
    "                    eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "train, validation, test = dataset.splits(text_field=field_processor, root=dataset_path)\n",
    "field_processor.build_vocab(train, validation, test, min_freq=1)\n",
    "\n",
    "\n",
    "# prep iterator\n",
    "train_token_ids_loader, val_token_ids_loader, test_token_ids_loader = dataset.iters(\n",
    "batch_size=batch_size, root=dataset_path, device=device)\n",
    "\n",
    "# get vocab\n",
    "# vocabulary = vocab.build_vocab_from_iterator(train_token_ids_loader)\n",
    "vocabulary = {}\n",
    "print(f'Time it took to prepare the iterator: {time.time() - ts:3f} seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[torchtext.data.batch.Batch of size 32]\n\t[.text]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n\t[.target]:[torch.cuda.LongTensor of size 35x32 (GPU 0)]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'input_fields'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-ab87f1d1cb7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_token_ids_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# vocabulary = vocab.build_vocab_from_iterator()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'input_fields'"
     ]
    }
   ],
   "source": [
    "# train.examples[0].text\n",
    "for batch in train_token_ids_loader:\n",
    "    print(batch)\n",
    "    print(batch)\n",
    "    break\n",
    "# vocabulary = vocab.build_vocab_from_iterator()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "type object 'PennTreebank' has no attribute 'vocab'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-32a211c865e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combined = train.examples[0].text + test.examples[0].text + validation.examples[0].text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# len(catch_unique(combined))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PennTreebank' has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# combined = train.examples[0].text + test.examples[0].text + validation.examples[0].text\n",
    "# len(catch_unique(combined))\n",
    "dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_unique(list_in):\n",
    "   # intilize an empty list\n",
    "   unq_list = []\n",
    "\n",
    "   # Check for elements\n",
    "   for x in list_in:\n",
    "      # check if exists in unq_list\n",
    "      if x not in unq_list:\n",
    "         unq_list.append(x)\n",
    "         # print list\n",
    "   return unq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wikitext-2-v1.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.48M/4.48M [00:00<00:00, 7.46MB/s]\n",
      "36718lines [00:01, 25487.53lines/s]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import torch\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                      iter(io.open(train_filepath,\n",
    "                                                   encoding=\"utf8\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28783"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "woo= iter(io.open(train_filepath,encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = list(woo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = DATA_DIR_PATH\n",
    "dataset_name= DatasetType.WikiText2.name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ts = time.time()\n",
    "# prep dataset\n",
    "dataset = getattr(datasets, dataset_name)  # should not be translation datsets\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# prep field processor (vocab)\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "field_processor = Field(tokenize=tokenizer, init_token=BOS_TOKEN,\n",
    "                    eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "train, validation, test = dataset.splits(text_field=field_processor, root=dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "36718"
      ]
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "len(trainl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7f46364506d0>"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "train.examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   0, 1080,    5,  ...,  966,    4,    3],\n",
       "        [   0,   11,    0,  ...,    5,    0,    0],\n",
       "        [  10, 9222,    0,  ...,    2,    0,  221],\n",
       "        ...,\n",
       "        [   3,    2,    7,  ...,   28,    5, 4811],\n",
       "        [  53,  352,  272,  ...,  780,   37,    4],\n",
       "        [   7, 2410,    4,  ...,  300,  133,    0]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}